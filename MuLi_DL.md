## 李沐深度学习视频


### GPT1/2/3  - 23

1. GPT1微调用最后一个token的embedding，进行信息抽取；GPT23和1的区别，不用微调下游任务，把下游任务的输入输出构造成和微调一样的形式 ; GPT2和1的区别：提前normalization,初始化方式不同
2. 现在的有用的生物大模型还处在GPT1和bert 的阶段，还需要fine-tuning ，GPT2/3的卖点是一个zero shot, few shot，将子任务构造成预训练的数据格式，进行promt engineering ，而不是基于梯度的模型更新，GPT2/3做few shot的原因是模型太大了，没办法更新
3. 但GPT3完全 few shot有自己的问题，1.当下游例子真的很多的时候，如英语翻译法语，不可能全部赛到一个example里面；2.模型无法把从一次forward中得到的信息永久的存储下来，每次都需要到一个example
4. 为什么需要多头注意力机制：GP T参数dmodel 增加1000倍，但每一个头的维度dread 大小不到2倍(如64->128)，只是会把头的个数nheads增加100倍(12->96)
5. 从模型效果来看，小模型适合小的批量，这样主动带来一些噪音防止过拟合，；而大模型则更适合大批量大小，研究发现大模型即使一个批量里没有什么噪音，好像也不容易过拟合




### Lamma 3.1  - 54,55

1. 大模型如何处理数据集？大公司很多人肉眼先看一些数据集，找到一些数据集的pattern，再对整个数据集进行清洗。这种方式称为启发式的数据处理方法
