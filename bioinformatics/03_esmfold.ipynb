{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the standard residue order when coding AA type as a number.\n",
    "# Reproduce it by taking 3-letter AA codes and sorting them alphabetically.\n",
    "restypes = [\n",
    "    \"A\",\n",
    "    \"R\",\n",
    "    \"N\",\n",
    "    \"D\",\n",
    "    \"C\",\n",
    "    \"Q\",\n",
    "    \"E\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"L\",\n",
    "    \"K\",\n",
    "    \"M\",\n",
    "    \"F\",\n",
    "    \"P\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "    \"V\",\n",
    "]\n",
    "restype_order = {restype: i for i, restype in enumerate(restypes)}\n",
    "restype_num = len(restypes)  # := 20.\n",
    "unk_restype_index = restype_num  # Catch-all index for unknown restypes.\n",
    "\n",
    "restypes_with_x = restypes + [\"X\"]\n",
    "restype_order_with_x = {restype: i for i, restype in enumerate(restypes_with_x)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mics file for basic component\n",
    "\n",
    "5.1\n",
    "\n",
    "1. Attention (Gated with bias), 只适用于single seqeunce的多头注意力机制，column-wise, row-wise的axial attention还是得用openfold\n",
    "2. Dropout along some specific dimension\n",
    "3. Residual MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_width, gated=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim == num_heads * head_width\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_width = head_width\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=True)\n",
    "        self.gated = gated\n",
    "        if gated:\n",
    "            self.g_proj = nn.Linear(embed_dim, embed_dim) #(B x L x embed_dim)\n",
    "            torch.nn.init.zeros_(self.g_proj.weight)\n",
    "            torch.nn.init.ones_(self.g_proj.bias)\n",
    "\n",
    "        self.rescale_factor = self.head_width**-0.5\n",
    "\n",
    "        torch.nn.init.zeros_(self.o_proj.bias)\n",
    "\n",
    "    def forward(self, x, mask=None, bias=None, indices=None):\n",
    "        \"\"\"\n",
    "        Basic self attention with optional mask and external pairwise bias.\n",
    "        To handle sequences of different lengths, use mask.\n",
    "\n",
    "        Inputs:\n",
    "          x: batch of input sequneces (.. x L x C)\n",
    "          mask: batch of boolean masks where 1=valid, 0=padding position (.. x L_k). optional.\n",
    "          bias: batch of scalar pairwise attention biases (.. x Lq x Lk x num_heads). optional.\n",
    "\n",
    "        Outputs:\n",
    "          sequence projection (B x L x embed_dim), attention maps (B x L x L x num_heads)\n",
    "        \"\"\"\n",
    "\n",
    "        t = rearrange(self.proj(x), \"... l (h c) -> ... h l c\", h=self.num_heads)\n",
    "        q, k, v = t.chunk(3, dim=-1) #(..., head, l, head_width)\n",
    "\n",
    "        q = self.rescale_factor * q # Q/sqrt(head_with)\n",
    "        a = torch.einsum(\"...qc,...kc->...qk\", q, k) #QK^T (..., head, lq, lk)\n",
    "\n",
    "        # Add external attention bias.\n",
    "        if bias is not None:\n",
    "            a = a + rearrange(bias, \"... lq lk h -> ... h lq lk\") #add bias\n",
    "\n",
    "        # Do not attend to padding tokens.\n",
    "        if mask is not None:\n",
    "            mask = repeat(\n",
    "                mask, \"... lk -> ... h lq lk\", h=self.num_heads, lq=q.shape[-2]\n",
    "            )\n",
    "            a = a.masked_fill(mask == False, -np.inf)\n",
    "\n",
    "        a = F.softmax(a, dim=-1)\n",
    "\n",
    "        y = torch.einsum(\"...hqk,...hkc->...qhc\", a, v)\n",
    "        y = rearrange(y, \"... h c -> ... (h c)\", h=self.num_heads)\n",
    "\n",
    "        if self.gated:\n",
    "            y = self.g_proj(x).sigmoid() * y\n",
    "        y = self.o_proj(y)\n",
    "\n",
    "        return y, rearrange(a, \"... lq lk h -> ... h lq lk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of dropout with the ability to share the dropout mask\n",
    "    along a particular dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r: float, batch_dim: T.Union[int, T.List[int]]):\n",
    "        super(Dropout, self).__init__()\n",
    "\n",
    "        self.r = r\n",
    "        if type(batch_dim) == int:\n",
    "            batch_dim = [batch_dim]\n",
    "        self.batch_dim = batch_dim\n",
    "        self.dropout = nn.Dropout(self.r)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shape = list(x.shape)\n",
    "        if self.batch_dim is not None:\n",
    "            for bd in self.batch_dim:\n",
    "                shape[bd] = 1\n",
    "        return x * self.dropout(x.new_ones(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceToPair(nn.Module):\n",
    "    def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(sequence_state_dim)\n",
    "        self.proj = nn.Linear(sequence_state_dim, inner_dim * 2, bias=True)\n",
    "        self.o_proj = nn.Linear(2 * inner_dim, pairwise_state_dim, bias=True)\n",
    "\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "        torch.nn.init.zeros_(self.o_proj.bias)\n",
    "\n",
    "    def forward(self, sequence_state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          sequence_state: B x L x sequence_state_dim\n",
    "\n",
    "        Output:\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "\n",
    "        Intermediate state:\n",
    "          B x L x L x 2*inner_dim\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(sequence_state.shape) == 3\n",
    "\n",
    "        s = self.layernorm(sequence_state)\n",
    "        s = self.proj(s)\n",
    "        q, k = s.chunk(2, dim=-1)\n",
    "\n",
    "        prod = q[:, None, :, :] * k[:, :, None, :]\n",
    "        diff = q[:, None, :, :] - k[:, :, None, :]\n",
    "\n",
    "        x = torch.cat([prod, diff], dim=-1)\n",
    "        x = self.o_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PairToSequence(nn.Module):\n",
    "    def __init__(self, pairwise_state_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(pairwise_state_dim)\n",
    "        self.linear = nn.Linear(pairwise_state_dim, num_heads, bias=False)\n",
    "\n",
    "    def forward(self, pairwise_state):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "\n",
    "        Output:\n",
    "          pairwise_bias: B x L x L x num_heads\n",
    "        \"\"\"\n",
    "        assert len(pairwise_state.shape) == 4\n",
    "        z = self.layernorm(pairwise_state)\n",
    "        pairwise_bias = self.linear(z)\n",
    "        return pairwise_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidueMLP(nn.Module):\n",
    "    def __init__(self, embed_dim, inner_dim, norm=nn.LayerNorm, dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            norm(embed_dim),\n",
    "            nn.Linear(embed_dim, inner_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inner_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.mlp(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triangular self attention block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openfold.model.triangular_attention import (\n",
    "    TriangleAttentionEndingNode,\n",
    "    TriangleAttentionStartingNode,\n",
    ")\n",
    "from openfold.model.triangular_multiplicative_update import (\n",
    "    TriangleMultiplicationIncoming,\n",
    "    TriangleMultiplicationOutgoing,\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "from esm.esmfold.v1.misc import (\n",
    "    Attention,\n",
    "    Dropout,\n",
    "    PairToSequence,\n",
    "    ResidueMLP,\n",
    "    SequenceToPair,\n",
    ")\n",
    "\n",
    "\n",
    "class TriangularSelfAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_state_dim,\n",
    "        pairwise_state_dim,\n",
    "        sequence_head_width,\n",
    "        pairwise_head_width,\n",
    "        dropout=0,\n",
    "        **__kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert sequence_state_dim % sequence_head_width == 0\n",
    "        assert pairwise_state_dim % pairwise_head_width == 0\n",
    "        sequence_num_heads = sequence_state_dim // sequence_head_width\n",
    "        pairwise_num_heads = pairwise_state_dim // pairwise_head_width\n",
    "        assert sequence_state_dim == sequence_num_heads * sequence_head_width\n",
    "        assert pairwise_state_dim == pairwise_num_heads * pairwise_head_width\n",
    "        assert pairwise_state_dim % 2 == 0\n",
    "\n",
    "        self.sequence_state_dim = sequence_state_dim\n",
    "        self.pairwise_state_dim = pairwise_state_dim\n",
    "\n",
    "        self.layernorm_1 = nn.LayerNorm(sequence_state_dim)\n",
    "\n",
    "        self.sequence_to_pair = SequenceToPair(\n",
    "            sequence_state_dim, pairwise_state_dim // 2, pairwise_state_dim\n",
    "        )\n",
    "        self.pair_to_sequence = PairToSequence(pairwise_state_dim, sequence_num_heads)\n",
    "\n",
    "        self.seq_attention = Attention(\n",
    "            sequence_state_dim, sequence_num_heads, sequence_head_width, gated=True\n",
    "        )\n",
    "        self.tri_mul_out = TriangleMultiplicationOutgoing(\n",
    "            pairwise_state_dim,\n",
    "            pairwise_state_dim,\n",
    "        )\n",
    "        self.tri_mul_in = TriangleMultiplicationIncoming(\n",
    "            pairwise_state_dim,\n",
    "            pairwise_state_dim,\n",
    "        )\n",
    "        self.tri_att_start = TriangleAttentionStartingNode(\n",
    "            pairwise_state_dim,\n",
    "            pairwise_head_width,\n",
    "            pairwise_num_heads,\n",
    "            inf=1e9,\n",
    "        )  # type: ignore\n",
    "        self.tri_att_end = TriangleAttentionEndingNode(\n",
    "            pairwise_state_dim,\n",
    "            pairwise_head_width,\n",
    "            pairwise_num_heads,\n",
    "            inf=1e9,\n",
    "        )  # type: ignore\n",
    "\n",
    "        self.mlp_seq = ResidueMLP(sequence_state_dim, 4 * sequence_state_dim, dropout=dropout)\n",
    "        self.mlp_pair = ResidueMLP(pairwise_state_dim, 4 * pairwise_state_dim, dropout=dropout)\n",
    "\n",
    "        assert dropout < 0.4\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.row_drop = Dropout(dropout * 2, 2)\n",
    "        self.col_drop = Dropout(dropout * 2, 1)\n",
    "\n",
    "        torch.nn.init.zeros_(self.tri_mul_in.linear_z.weight)\n",
    "        torch.nn.init.zeros_(self.tri_mul_in.linear_z.bias)\n",
    "        torch.nn.init.zeros_(self.tri_mul_out.linear_z.weight)\n",
    "        torch.nn.init.zeros_(self.tri_mul_out.linear_z.bias)\n",
    "        torch.nn.init.zeros_(self.tri_att_start.mha.linear_o.weight)\n",
    "        torch.nn.init.zeros_(self.tri_att_start.mha.linear_o.bias)\n",
    "        torch.nn.init.zeros_(self.tri_att_end.mha.linear_o.weight)\n",
    "        torch.nn.init.zeros_(self.tri_att_end.mha.linear_o.bias)\n",
    "\n",
    "        torch.nn.init.zeros_(self.sequence_to_pair.o_proj.weight)\n",
    "        torch.nn.init.zeros_(self.sequence_to_pair.o_proj.bias)\n",
    "        torch.nn.init.zeros_(self.pair_to_sequence.linear.weight)\n",
    "        torch.nn.init.zeros_(self.seq_attention.o_proj.weight)\n",
    "        torch.nn.init.zeros_(self.seq_attention.o_proj.bias)\n",
    "        torch.nn.init.zeros_(self.mlp_seq.mlp[-2].weight)\n",
    "        torch.nn.init.zeros_(self.mlp_seq.mlp[-2].bias)\n",
    "        torch.nn.init.zeros_(self.mlp_pair.mlp[-2].weight)\n",
    "        torch.nn.init.zeros_(self.mlp_pair.mlp[-2].bias)\n",
    "\n",
    "    def forward(self, sequence_state, pairwise_state, mask=None, chunk_size=None, **__kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          sequence_state: B x L x sequence_state_dim\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "          mask: B x L boolean tensor of valid positions\n",
    "\n",
    "        Output:\n",
    "          sequence_state: B x L x sequence_state_dim\n",
    "          pairwise_state: B x L x L x pairwise_state_dim\n",
    "        \"\"\"\n",
    "        assert len(sequence_state.shape) == 3\n",
    "        assert len(pairwise_state.shape) == 4\n",
    "        if mask is not None:\n",
    "            assert len(mask.shape) == 2\n",
    "\n",
    "        batch_dim, seq_dim, sequence_state_dim = sequence_state.shape\n",
    "        pairwise_state_dim = pairwise_state.shape[3]\n",
    "        assert sequence_state_dim == self.sequence_state_dim\n",
    "        assert pairwise_state_dim == self.pairwise_state_dim\n",
    "        assert batch_dim == pairwise_state.shape[0]\n",
    "        assert seq_dim == pairwise_state.shape[1]\n",
    "        assert seq_dim == pairwise_state.shape[2]\n",
    "\n",
    "        # Update sequence state\n",
    "        bias = self.pair_to_sequence(pairwise_state)\n",
    "\n",
    "        # Self attention with bias + mlp.\n",
    "        y = self.layernorm_1(sequence_state)\n",
    "        y, _ = self.seq_attention(y, mask=mask, bias=bias)\n",
    "        sequence_state = sequence_state + self.drop(y)\n",
    "        sequence_state = self.mlp_seq(sequence_state)\n",
    "\n",
    "        # Update pairwise state\n",
    "        pairwise_state = pairwise_state + self.sequence_to_pair(sequence_state)\n",
    "\n",
    "        # Axial attention with triangular bias.\n",
    "        tri_mask = mask.unsqueeze(2) * mask.unsqueeze(1) if mask is not None else None\n",
    "        pairwise_state = pairwise_state + self.row_drop(\n",
    "            self.tri_mul_out(pairwise_state, mask=tri_mask)\n",
    "        )\n",
    "        pairwise_state = pairwise_state + self.col_drop(\n",
    "            self.tri_mul_in(pairwise_state, mask=tri_mask)\n",
    "        )\n",
    "        pairwise_state = pairwise_state + self.row_drop(\n",
    "            self.tri_att_start(pairwise_state, mask=tri_mask, chunk_size=chunk_size)\n",
    "        )\n",
    "        pairwise_state = pairwise_state + self.col_drop(\n",
    "            self.tri_att_end(pairwise_state, mask=tri_mask, chunk_size=chunk_size)\n",
    "        )\n",
    "\n",
    "        # MLP over pairs.\n",
    "        pairwise_state = self.mlp_pair(pairwise_state)\n",
    "\n",
    "        return sequence_state, pairwise_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESMFold 中Folding trunk只自己写了:\n",
    "1. single sequence attetnion代替MSA representation attention\n",
    "2. Seq2Pair中的product ,difference 代替原有的outer product\n",
    "3. seq和pair的MLP\n",
    "Triangular update 包括：三角乘法更写(row-wise, column-wise)和三角注意力机制(row-wise, column-wise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention的标准操作,似乎Megatron第一次提出把layer norm的顺序放到后面?\n",
    "1. 多头注意力部分\n",
    "\n",
    "`x = x + Dropout(MHA(LayerNorm(x)))`\n",
    "\n",
    "2. MLP部分\n",
    "\n",
    "`x = x + Dropout(MLP(LayerNorm(x)))`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.15 multimer chain\n",
    "\n",
    "- 将序列字符串转化为tensor并检查序列中的非法token 转化为U\n",
    "- multimer 序列增加linker, chain break 和 相应的chain_idx , liker mask\n",
    "- make batch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"APVLIMWGFYSTNQCDEKRH:KRH:ANG\"\n",
    "chain_linker = \"G\" * 25\n",
    "chains = seq.split(\":\")\n",
    "seq = chain_linker.join(chains)\n",
    "\n",
    "unk_idx = restype_order_with_x[\"X\"]\n",
    "encoded = torch.tensor(\n",
    "    [restype_order_with_x.get(aa, unk_idx) for aa in seq]\n",
    ")\n",
    "residx = torch.arange(len(encoded))\n",
    "residue_index_offset = 512\n",
    "if residue_index_offset > 0:\n",
    "    start = 0\n",
    "    for i, chain in enumerate(chains):\n",
    "        residx[start : start + len(chain) + len(chain_linker)] += i * residue_index_offset\n",
    "        start += len(chain) + len(chain_linker)\n",
    "    linker_mask = torch.ones_like(encoded, dtype=torch.float32)\n",
    "chain_index = []\n",
    "offset = 0\n",
    "for i, chain in enumerate(chains):\n",
    "    if i > 0:\n",
    "        chain_index.extend([i - 1] * len(chain_linker)) # 第i-1条链对应的chain_linker 属于前一条链\n",
    "    chain_index.extend([i] * len(chain))\n",
    "    offset += len(chain)\n",
    "    linker_mask[offset : offset + len(chain_linker)] = 0\n",
    "    offset += len(chain_linker)\n",
    "\n",
    "chain_index = torch.tensor(chain_index, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chain_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(\n",
    "    seq: str,\n",
    "    residue_index_offset: T.Optional[int] = 512,\n",
    "    chain_linker: T.Optional[str] = \"G\" * 25,\n",
    ") -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    if chain_linker is None:\n",
    "        chain_linker = \"\"\n",
    "    if residue_index_offset is None:\n",
    "        residue_index_offset = 0\n",
    "\n",
    "    chains = seq.split(\":\")\n",
    "    seq = chain_linker.join(chains) # N条链中间加入chain_linker\n",
    "\n",
    "    unk_idx = residue_constants.restype_order_with_x[\"X\"]\n",
    "    encoded = torch.tensor(\n",
    "        [residue_constants.restype_order_with_x.get(aa, unk_idx) for aa in seq]\n",
    "    )\n",
    "    residx = torch.arange(len(encoded))\n",
    "\n",
    "    if residue_index_offset > 0: #每一条链加入offset\n",
    "        start = 0\n",
    "        for i, chain in enumerate(chains):\n",
    "            residx[start : start + len(chain) + len(chain_linker)] += (\n",
    "                i * residue_index_offset\n",
    "            )\n",
    "            start += len(chain) + len(chain_linker)\n",
    "\n",
    "    linker_mask = torch.ones_like(encoded, dtype=torch.float32)\n",
    "    chain_index = []\n",
    "    offset = 0\n",
    "    for i, chain in enumerate(chains):\n",
    "        if i > 0:\n",
    "            chain_index.extend([i - 1] * len(chain_linker))\n",
    "        chain_index.extend([i] * len(chain))\n",
    "        offset += len(chain)\n",
    "        linker_mask[offset : offset + len(chain_linker)] = 0\n",
    "        offset += len(chain_linker)\n",
    "\n",
    "    chain_index = torch.tensor(chain_index, dtype=torch.int64)\n",
    "\n",
    "    return encoded, residx, linker_mask, chain_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_encode_sequences(\n",
    "    sequences: T.Sequence[str],\n",
    "    residue_index_offset: T.Optional[int] = 512,\n",
    "    chain_linker: T.Optional[str] = \"G\" * 25,\n",
    ") -> T.Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    aatype_list = []\n",
    "    residx_list = []\n",
    "    linker_mask_list = []\n",
    "    chain_index_list = []\n",
    "    for seq in sequences:\n",
    "        aatype_seq, residx_seq, linker_mask_seq, chain_index_seq = encode_sequence(\n",
    "            seq,\n",
    "            residue_index_offset=residue_index_offset,\n",
    "            chain_linker=chain_linker,\n",
    "        )\n",
    "        aatype_list.append(aatype_seq)\n",
    "        residx_list.append(residx_seq)\n",
    "        linker_mask_list.append(linker_mask_seq)\n",
    "        chain_index_list.append(chain_index_seq)\n",
    "\n",
    "    aatype = collate_dense_tensors(aatype_list)\n",
    "    mask = collate_dense_tensors(\n",
    "        [aatype.new_ones(len(aatype_seq)) for aatype_seq in aatype_list]\n",
    "    )\n",
    "    residx = collate_dense_tensors(residx_list)\n",
    "    linker_mask = collate_dense_tensors(linker_mask_list)\n",
    "    chain_index_list = collate_dense_tensors(chain_index_list, -1)\n",
    "\n",
    "    return aatype, mask, residx, linker_mask, chain_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超级有用的函数，将一个list的张量padding到一个tensor里面\n",
    "# 前提是这个list里面的每一个张量ndim都相同\n",
    "# zip([iterable_1,iterable_2]) 会返回一个tuple的list\n",
    "def collate_dense_tensors(\n",
    "    samples: T.List[torch.Tensor], pad_v: float = 0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Takes a list of tensors with the following dimensions:\n",
    "        [(d_11,       ...,           d_1K),\n",
    "         (d_21,       ...,           d_2K),\n",
    "         ...,\n",
    "         (d_N1,       ...,           d_NK)]\n",
    "    and stack + pads them into a single tensor of:\n",
    "    (N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})\n",
    "    \"\"\"\n",
    "    if len(samples) == 0:\n",
    "        return torch.Tensor()\n",
    "    if len(set(x.dim() for x in samples)) != 1:\n",
    "        raise RuntimeError(\n",
    "            f\"Samples has varying dimensions: {[x.dim() for x in samples]}\"\n",
    "        )\n",
    "    (device,) = tuple(set(x.device for x in samples))  # assumes all on same device\n",
    "    max_shape = [max(lst) for lst in zip(*[x.shape for x in samples])]\n",
    "    result = torch.empty(\n",
    "        len(samples), *max_shape, dtype=samples[0].dtype, device=device\n",
    "    )\n",
    "    result.fill_(pad_v)\n",
    "    for i in range(len(samples)):\n",
    "        result_i = result[i]\n",
    "        t = samples[i]\n",
    "        result_i[tuple(slice(0, k) for k in t.shape)] = t\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fe6e554e4c0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(*(x.shape,x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
