{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习AlphaFold,OpenFold,ESMFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数1: 学习torch.split,在指定维度将tensor分成许多chunk tensor组成的tuple\n",
    "# 参数二可以写为一个single chunk的大小，要么是许多chunk size的大小加起来等于10\n",
    "x = torch.arange(10)\n",
    "x_chunk = torch.split(x,5,dim=0)\n",
    "# E.g.\n",
    "    #    # [*, N_res, H * P_q * 3]\n",
    "    #     q_pts = self.linear_q_points(s)\n",
    "\n",
    "    #     # This is kind of clunky, but it's how the original does it\n",
    "    #     # [*, N_res, H * P_q, 3]\n",
    "    #     q_pts = torch.split(q_pts, q_pts.shape[-1] // 3, dim=-1)\n",
    "    #     q_pts = torch.stack(q_pts, dim=-1)\n",
    "N_res = 100\n",
    "H = 10\n",
    "P_q = 4\n",
    "P_v = 8\n",
    "q_pts = torch.randn(5,N_res,H*P_q*3)\n",
    "q_pts = torch.split(q_pts, q_pts.shape[-1] // 3, dim=-1)\n",
    "# q_pts = torch.stack(q_pts, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数 1.5 : torch.chunk(input,chunks,dim=0)\n",
    "# chunks指的是将tensor变成几个chunk\n",
    "# chunks 是分成的几个块，chunk size是每一个块内部的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数2 : torch.unbind(input, dim=0)\n",
    "# Removes a tensor dimension.\n",
    "# Returns a tuple of all slices along a given dimension, already without it.\n",
    "# torch.unbind : 把某一个维度按1进行切割\n",
    "# torch.split  : 把某一个维度按照指定的chunk size进行切割\n",
    "torch.unbind(torch.arange(10),dim=0) == torch.split(torch.arange(10),1,dim=0)\n",
    "# E.g. *torch.unbind(o_pt, dim=-1) 其中*表示解压unbind产生的tuple 当作实参数\n",
    "o_pt = torch.randn((N_res,H*P_v,3))\n",
    "print(*torch.unbind(o_pt,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数3 : torch.nn.Softplut(beta=1,threshold=20)\n",
    "# SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.\n",
    "a = np.linspace(-10.0,10.0,num=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=16\n",
    "head_weights = torch.randn((H,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6482, -0.9007, -0.7718, -0.1868, -0.4537,  0.7549, -0.1214, -0.6609,\n",
       "         2.3571, -0.3103, -2.0868,  0.8933, -1.5520, -0.1901,  0.5534, -1.4012])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_weights.view(*())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dog is a class method,given <class '__main__.MyDog'>\n"
     ]
    }
   ],
   "source": [
    "# class 内部的装饰器 1.\n",
    "# 一共有三种class method的类型: instance method; statistic method; class method\n",
    "# instance method绑定的对象是instance，必须先创建一个对象之后才能调用该方法(函数定义时形参是self)\n",
    "# static method创建的时候不需要添加self 形参,可以在不创建对象的时候就调用该函数，避免了创建对象所带来的内存消耗;另一个角度理解是static method本身就是一个普通的函数，只是挂靠在class的这个命名空间底下而言，不需要实例化对象\n",
    "# classmethod  类方法主要是为了优雅的创建多个对象\n",
    "class MyDog:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def instance_method(self): # self 形参接受的是实例化后对象的IP地址\n",
    "        print(\"My dog is an instance method\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def static_method():\n",
    "        print(\"My dog is a static method\")\n",
    "    \n",
    "    @classmethod\n",
    "    def class_method(cls): # cls参数表示这个类本身\n",
    "        print(f\"My dog is a class method,given {cls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 内部的装饰器 2.\n",
    "# @property 将一个method的调用方法变成属性的调用方法\n",
    "# @property 将某个方法变成只读属性\n",
    "# @xxx.setter 把该属性进行赋值\n",
    "# 注意属性的方法名不要和属性名重名,否则self.birth调用时会递归的找self.birth这个函数造成栈溢出\n",
    "class Student:\n",
    "    def __init__(self) -> None:\n",
    "        self._birth = 10\n",
    "    @property\n",
    "    def birth(self):\n",
    "        return self._birth\n",
    "\n",
    "    @birth.setter\n",
    "    def birth(self,value):\n",
    "        self._birth = value\n",
    "a = Student()\n",
    "a.birth = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 0. Rigid Transformation\n",
    "\n",
    "如果对坐标做变换，如何存储transformation\n",
    "\n",
    "1. Rotation 是一个类似张量的 Rotation 对象，每一个point代表一个Rotation\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional,Tuple\n",
    "# Q1 : 3*3 旋转矩阵到旋转四元数的变换？3*3旋转正交矩阵自由度不是只有3?\n",
    "# identity rotation matrix eye(3)\n",
    "def identity_rot_mats(\n",
    "    batch_dims: Tuple[int], \n",
    "    dtype: Optional[torch.dtype] = None, \n",
    "    device: Optional[torch.device] = None, \n",
    "    requires_grad: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    rots = torch.eye(\n",
    "        3, dtype=dtype, device=device, requires_grad=requires_grad\n",
    "    )\n",
    "    rots = rots.view(*((1,) * len(batch_dims)), 3, 3)\n",
    "    rots = rots.expand(*batch_dims, -1, -1)\n",
    "    rots = rots.contiguous()\n",
    "\n",
    "    return rots\n",
    "\n",
    "# identity translation vector [0,0,0]\n",
    "def identity_trans(\n",
    "    batch_dims: Tuple[int], \n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "    device: Optional[torch.device] = None, \n",
    "    requires_grad: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    trans = torch.zeros(\n",
    "        (*batch_dims, 3), \n",
    "        dtype=dtype, \n",
    "        device=device, \n",
    "        requires_grad=requires_grad\n",
    "    )\n",
    "    return trans\n",
    "\n",
    "# identity quaternion [1, 0, 0, 0]\n",
    "def identity_quats(\n",
    "    batch_dims: Tuple[int], \n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "    device: Optional[torch.device] = None, \n",
    "    requires_grad: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    quat = torch.zeros(\n",
    "        (*batch_dims, 4), \n",
    "        dtype=dtype, \n",
    "        device=device, \n",
    "        requires_grad=requires_grad\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # quat [* , 4] \n",
    "        quat[..., 0] = 1\n",
    "\n",
    "    return quat\n",
    "_quat_elements = [\"a\", \"b\", \"c\", \"d\"]\n",
    "_qtr_keys = [l1 + l2 for l1 in _quat_elements for l2 in _quat_elements]\n",
    "_qtr_ind_dict = {key: ind for ind, key in enumerate(_qtr_keys)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evoformer .Triangular multiplicative update\n",
    "\n",
    "Algorithm 11 : Triangular multiplicative update using \"outgoing\" edges, for ij : ik , jk\n",
    "\n",
    "Algorithm 112 : Triangular multiplicative update using \"incoming\" edges, for ij : ki,kj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evoformer .Triangular self attention\n",
    "\n",
    "Algorithm 13 : Triangular gated self-attention around starting node. (row-wise)\n",
    "\n",
    "Algorithm 112 : Triangular gated self-attention around ending node. (column-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, c_in, c_hidden, no_heads, starting=True, inf=1e9\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in:\n",
    "                Input channel dimension\n",
    "            c_hidden:\n",
    "                Overall hidden channel dimension (not per-head)\n",
    "            no_heads:\n",
    "                Number of attention heads\n",
    "        \"\"\"\n",
    "        super(TriangleAttention, self).__init__()\n",
    "\n",
    "        self.c_in = c_in\n",
    "        self.c_hidden = c_hidden\n",
    "        self.no_heads = no_heads\n",
    "        self.starting = starting\n",
    "        self.inf = inf\n",
    "\n",
    "        self.layer_norm = LayerNorm(self.c_in)\n",
    "\n",
    "        self.linear = Linear(c_in, self.no_heads, bias=False, init=\"normal\")\n",
    "\n",
    "        self.mha = Attention(\n",
    "            self.c_in, self.c_in, self.c_in, self.c_hidden, self.no_heads\n",
    "        )\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _chunk(self,\n",
    "        x: torch.Tensor,\n",
    "        biases: List[torch.Tensor],\n",
    "        chunk_size: int,\n",
    "        use_memory_efficient_kernel: bool = False,\n",
    "        use_lma: bool = False,\n",
    "        inplace_safe: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"triangle! triangle!\"\n",
    "        mha_inputs = {\n",
    "            \"q_x\": x,\n",
    "            \"kv_x\": x,\n",
    "            \"biases\": biases,\n",
    "        }\n",
    "\n",
    "        return chunk_layer(\n",
    "            partial(\n",
    "                self.mha, \n",
    "                use_memory_efficient_kernel=use_memory_efficient_kernel,\n",
    "                use_lma=use_lma\n",
    "            ),\n",
    "            mha_inputs,\n",
    "            chunk_size=chunk_size,\n",
    "            no_batch_dims=len(x.shape[:-2]),\n",
    "            _out=x if inplace_safe else None,\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "        x: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        chunk_size: Optional[int] = None,\n",
    "        use_memory_efficient_kernel: bool = False,\n",
    "        use_lma: bool = False,\n",
    "        inplace_safe: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:\n",
    "                [*, I, J, C_in] input tensor (e.g. the pair representation)\n",
    "        Returns:\n",
    "            [*, I, J, C_in] output tensor\n",
    "        \"\"\" \n",
    "        if mask is None:\n",
    "            # [*, I, J]\n",
    "            mask = x.new_ones(\n",
    "                x.shape[:-1],\n",
    "            )\n",
    "\n",
    "        if(not self.starting):\n",
    "            x = x.transpose(-2, -3)\n",
    "            mask = mask.transpose(-1, -2)\n",
    "\n",
    "        # [*, I, J, C_in]\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # [*, I, 1, 1, J]\n",
    "        mask_bias = (self.inf * (mask - 1))[..., :, None, None, :]\n",
    "\n",
    "        # [*, H, I, J]\n",
    "        triangle_bias = permute_final_dims(self.linear(x), (2, 0, 1))\n",
    "\n",
    "        # [*, 1, H, I, J]\n",
    "        triangle_bias = triangle_bias.unsqueeze(-4)\n",
    "\n",
    "        biases = [mask_bias, triangle_bias]\n",
    "\n",
    "        if chunk_size is not None:\n",
    "            x = self._chunk(\n",
    "                x, \n",
    "                biases, \n",
    "                chunk_size, \n",
    "                use_memory_efficient_kernel=use_memory_efficient_kernel,\n",
    "                use_lma=use_lma,\n",
    "                inplace_safe=inplace_safe,\n",
    "            )\n",
    "        else:\n",
    "            x = self.mha(\n",
    "                q_x=x, \n",
    "                kv_x=x, \n",
    "                biases=biases, \n",
    "                use_memory_efficient_kernel=use_memory_efficient_kernel,\n",
    "                use_lma=use_lma\n",
    "            )\n",
    "\n",
    "        if(not self.starting):\n",
    "            x = x.transpose(-2, -3)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Implements Algorithm 13\n",
    "TriangleAttentionStartingNode = TriangleAttention\n",
    "\n",
    "\n",
    "class TriangleAttentionEndingNode(TriangleAttention):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 14.\n",
    "    \"\"\"\n",
    "    __init__ = partialmethod(TriangleAttention.__init__, starting=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Module 1. IPA\n",
    "PS : \n",
    "1. H最开始肯定放在N_res后面,但是在attention score中一定是放在N_res前面，也就是H,N_res,N_res\n",
    "2. key,value是二维的情况下怎么画图\n",
    "\n",
    "\n",
    "总结：\n",
    "- 两套qkv,全部从single chain representation中得到\n",
    "  - qkv 为标准的多头注意力, 将c_s 投影为H * C_hidden\n",
    "  - q_pts, k_pts, v_pts 显示的构建了坐标的注意力机制,将c_s 投影为H*P_qk/P_v * 3,之后直接用Rigid transforamtion变换这些点点坐标\n",
    "- 三个attention score (affinities) 和一个bias\n",
    "  - pair-bias 由pair-representation直接将c_z投影到H的维度进行加和\n",
    "  - dot-product 由标准的多头注意力机制qkv计算得来\n",
    "  - squared distance affinities 由点注意力机制构建 $o_{pt} = -\\frac{\\gamma ^h W_c}{2}\\sum\\limits_{p}||q^{hp}_{i}-k^{hp}_{j} ||^2$\n",
    "    - 具体计算步骤就是输入一个[*, N_res, H, P_q, 3]的query q_pts和key k_pts\n",
    "    - pt_att = q_pts.unsqueeze(-4) - k_pts.unsqueeze(-5) query key氨基酸每个原子中的xyz坐标之差，利用brodcast来进行对称\n",
    "    - pt_att = pt_att** 2得到xyz距离平方\n",
    "    - pt_att = torch.sum(pt_att,dim=-1) 求和xyz坐标，得到每个residue的每个原子的坐标距离\n",
    "      - 源代码里是这样写的 sum(*torch.unbind(pt_att,dim=-1)),似乎没什么差别\n",
    "    - 加入weight\n",
    "    - 加入squared mask,常规操作:\n",
    "      - mask [*, N_res]\n",
    "      - -inf * (mask.unsqueeze(-1) * mask.unsqueeze(-2)).unsqueeze(-3) -> [*, 1, N_res, N_res]\n",
    "- 四个output value, 最后concat h,q\n",
    "  - pair-representation value\n",
    "    - [ *, H, N_res, N_res].transpose(-2,-3) @ [ *, N_res, N_res, c_z] -> [ *, N_res, H, c_z] -> [ *, N_res, H * c_z]\n",
    "  - dot-product value\n",
    "    - [ *, H, N_res, N_res] matmaul [ \\*, H, N_res, C_hidden] -> [ \\*, H, N_res ,C_hidden] -> [ *, N_res, H * C_hidden]\n",
    "  - o_pt (point attention value)\n",
    "    - input\n",
    "      - v_pts : [ *, N_res, H, P_v, 3] \n",
    "      - a     : [ *, H, N_res, N_res]\n",
    "    - output : o_pt : [ *, N_res, H, P_v, 3], split into 3 points : [ *, N_res, H * P_v] * 3\n",
    "  - o_pt_norm\n",
    "    - v_pts : $\\sqrt{v_{pts}^2}$\n",
    "- 1个输出\n",
    "  - 更新之后的s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From OpenFold\n",
    "class InvariantPointAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 22.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_s: int,\n",
    "        c_z: int,\n",
    "        c_hidden: int,\n",
    "        no_heads: int,\n",
    "        no_qk_points: int,\n",
    "        no_v_points: int,\n",
    "        inf: float = 1e5,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_s:\n",
    "                Single representation channel dimension\n",
    "            c_z:\n",
    "                Pair representation channel dimension\n",
    "            c_hidden:\n",
    "                Hidden channel dimension\n",
    "            no_heads:\n",
    "                Number of attention heads\n",
    "            no_qk_points:\n",
    "                Number of query/key points to generate\n",
    "            no_v_points:\n",
    "                Number of value points to generate\n",
    "        \"\"\"\n",
    "        super(InvariantPointAttention, self).__init__()\n",
    "\n",
    "        self.c_s = c_s\n",
    "        self.c_z = c_z\n",
    "        self.c_hidden = c_hidden\n",
    "        self.no_heads = no_heads\n",
    "        self.no_qk_points = no_qk_points\n",
    "        self.no_v_points = no_v_points\n",
    "        self.inf = inf\n",
    "        self.eps = eps\n",
    "\n",
    "        # These linear layers differ from their specifications in the\n",
    "        # supplement. There, they lack bias and use Glorot initialization.\n",
    "        # Here as in the official source, they have bias and use the default\n",
    "        # Lecun initialization.\n",
    "        hc = self.c_hidden * self.no_heads\n",
    "        self.linear_q = Linear(self.c_s, hc)\n",
    "        self.linear_kv = Linear(self.c_s, 2 * hc) # 2*hc 表示一个是key,一个是value\n",
    "\n",
    "        hpq = self.no_heads * self.no_qk_points * 3 # 表示point attention 中 query 需要投影到的维度\n",
    "        self.linear_q_points = Linear(self.c_s, hpq)\n",
    "\n",
    "        hpkv = self.no_heads * (self.no_qk_points + self.no_v_points) * 3 # 表示point attention 中 key + value 需要投影到的维度\n",
    "        self.linear_kv_points = Linear(self.c_s, hpkv)\n",
    "\n",
    "        hpv = self.no_heads * self.no_v_points * 3\n",
    "\n",
    "        self.linear_b = Linear(self.c_z, self.no_heads)\n",
    "\n",
    "        self.head_weights = nn.Parameter(torch.zeros((no_heads))) # head weights 表示一个可学习的gamma h参数用来控制point attention affinity中的不同head\n",
    "        ipa_point_weights_init_(self.head_weights)\n",
    "\n",
    "        concat_out_dim = self.no_heads * (\n",
    "            self.c_z + self.c_hidden + self.no_v_points * 4\n",
    "        ) # 最终拼接时的dimension, c_z表示pair representation的output,  c_hidden 表示从single representaion计算的output, 4个no_v_points表示3个坐标(此处concat p)+1个norm；外侧乘H表示concat (H)\n",
    "        self.linear_out = Linear(concat_out_dim, self.c_s, init=\"final\")\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        s: torch.Tensor,\n",
    "        z: Optional[torch.Tensor],\n",
    "        r: Rigid,\n",
    "        mask: torch.Tensor,\n",
    "        inplace_safe: bool = False,\n",
    "        _offload_inference: bool = False,\n",
    "        _z_reference_list: Optional[Sequence[torch.Tensor]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s:\n",
    "                [*, N_res, C_s] single representation\n",
    "            z:\n",
    "                [*, N_res, N_res, C_z] pair representation\n",
    "            r:\n",
    "                [*, N_res] transformation object\n",
    "            mask:\n",
    "                [*, N_res] mask\n",
    "        Returns:\n",
    "            [*, N_res, C_s] single representation update\n",
    "        \"\"\"\n",
    "        if(_offload_inference and inplace_safe):\n",
    "            z = _z_reference_list\n",
    "        else:\n",
    "            z = [z]\n",
    "       \n",
    "        #######################################\n",
    "        # Generate scalar and point activations\n",
    "        #######################################\n",
    "        # [*, N_res, H * C_hidden]\n",
    "        q = self.linear_q(s)\n",
    "        kv = self.linear_kv(s)\n",
    "\n",
    "        # [*, N_res, H, C_hidden]\n",
    "        q = q.view(q.shape[:-1] + (self.no_heads, -1))\n",
    "\n",
    "        # [*, N_res, H, 2 * C_hidden]\n",
    "        kv = kv.view(kv.shape[:-1] + (self.no_heads, -1))\n",
    "\n",
    "        # [*, N_res, H, C_hidden]\n",
    "        k, v = torch.split(kv, self.c_hidden, dim=-1)\n",
    "\n",
    "        # [*, N_res, H * P_q * 3]\n",
    "        q_pts = self.linear_q_points(s)\n",
    "\n",
    "        # This is kind of clunky, but it's how the original does it\n",
    "        # 注意Rigid instance 做slicing的时候是对*做的(batch dimension)，而不用考虑最后的3*3\n",
    "        q_pts = torch.split(q_pts, q_pts.shape[-1] // 3, dim=-1)\n",
    "        # [*, N_res, H * P_q, 3]\n",
    "        q_pts = torch.stack(q_pts, dim=-1)\n",
    "        q_pts = r[..., None].apply(q_pts)\n",
    "\n",
    "        # [*, N_res, H, P_q, 3]\n",
    "        q_pts = q_pts.view(\n",
    "            q_pts.shape[:-2] + (self.no_heads, self.no_qk_points, 3)\n",
    "        )\n",
    "\n",
    "        # [*, N_res, H * (P_q + P_v) * 3]\n",
    "        kv_pts = self.linear_kv_points(s)\n",
    "\n",
    "        # [*, N_res, H * (P_q + P_v), 3]\n",
    "        kv_pts = torch.split(kv_pts, kv_pts.shape[-1] // 3, dim=-1)\n",
    "        kv_pts = torch.stack(kv_pts, dim=-1)\n",
    "        # r[...,None] -> \n",
    "        # Rigid : _rot       = [ *, N_res, 1, 3, 3]  \n",
    "        #         _trans     = [ *, N_res, 1, 3]\n",
    "        kv_pts = r[..., None].apply(kv_pts)\n",
    "\n",
    "\n",
    "        # [*, N_res, H, (P_q + P_v), 3]\n",
    "        kv_pts = kv_pts.view(kv_pts.shape[:-2] + (self.no_heads, -1, 3))\n",
    "\n",
    "        # [*, N_res, H, P_q/P_v, 3]\n",
    "        k_pts, v_pts = torch.split(\n",
    "            kv_pts, [self.no_qk_points, self.no_v_points], dim=-2\n",
    "        )\n",
    "\n",
    "        ##########################\n",
    "        # Compute attention scores\n",
    "        ##########################\n",
    "        # [*, N_res, N_res, H]\n",
    "        b = self.linear_b(z[0])\n",
    "        \n",
    "        if(_offload_inference):\n",
    "            assert(sys.getrefcount(z[0]) == 2)\n",
    "            z[0] = z[0].cpu()\n",
    "\n",
    "        # [*, H, N_res, N_res]\n",
    "        if(is_fp16_enabled()):\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                a = torch.matmul(\n",
    "                    permute_final_dims(q.float(), (1, 0, 2)),  # [*, H, N_res, C_hidden]\n",
    "                    permute_final_dims(k.float(), (1, 2, 0)),  # [*, H, C_hidden, N_res]\n",
    "                )\n",
    "        else:\n",
    "            # q,k,v original [*, N_res, H, C_hidden]\n",
    "            a = torch.matmul(\n",
    "                permute_final_dims(q, (1, 0, 2)),  # [*, H, N_res, C_hidden]\n",
    "                permute_final_dims(k, (1, 2, 0)),  # [*, H, C_hidden, N_res]\n",
    "            ) # attention affinity [*, H, N_res, N_res] \n",
    "        \n",
    "        a *= math.sqrt(1.0 / (3 * self.c_hidden))\n",
    "        a += (math.sqrt(1.0 / 3) * permute_final_dims(b, (2, 0, 1)))\n",
    "        # bias from  [*, N_res, N_res, H] -> [*, H, N_res, N_res]\n",
    "\n",
    "        # [*, N_res, N_res, H, P_q, 3]\n",
    "        # Original q_pts,k_pts,v_pts after global transformation  -> [*, N_res, H, P_qk, 3]\n",
    "        # broadcast from [*, N_res, 1, H, P_q, 3] - [*, N_res, 1, H, P_q, 3]\n",
    "        pt_att = q_pts.unsqueeze(-4) - k_pts.unsqueeze(-5)\n",
    "        if(inplace_safe):\n",
    "            pt_att *= pt_att\n",
    "        else:\n",
    "            pt_att = pt_att ** 2\n",
    "\n",
    "        # [*, N_res, N_res, H, P_q]\n",
    "        pt_att = sum(torch.unbind(pt_att, dim=-1)) # 等价于 torch.sum(pt_att,dim=-1), 求两两点之间的膜长\n",
    "        head_weights = self.softplus(self.head_weights).view(\n",
    "            *((1,) * len(pt_att.shape[:-2]) + (-1, 1))\n",
    "        ) # [*, 1, 1, H, 1]\n",
    "        head_weights = head_weights * math.sqrt(\n",
    "            1.0 / (3 * (self.no_qk_points * 9.0 / 2))\n",
    "        )\n",
    "        if(inplace_safe):\n",
    "            pt_att *= head_weights\n",
    "        else:\n",
    "            pt_att = pt_att * head_weights\n",
    "\n",
    "        # [*, N_res, N_res, H]\n",
    "        pt_att = torch.sum(pt_att, dim=-1) * (-0.5)\n",
    "        # [*, N_res, N_res]\n",
    "        # Broadcast from [*, N_res, 1] * [*, 1, N_res]\n",
    "        square_mask = mask.unsqueeze(-1) * mask.unsqueeze(-2)\n",
    "        square_mask = self.inf * (square_mask - 1)\n",
    "\n",
    "        # [*, H, N_res, N_res]\n",
    "        pt_att = permute_final_dims(pt_att, (2, 0, 1))\n",
    "        \n",
    "        if(inplace_safe):\n",
    "            a += pt_att\n",
    "            del pt_att\n",
    "            a += square_mask.unsqueeze(-3)\n",
    "            # in-place softmax\n",
    "            attn_core_inplace_cuda.forward_(\n",
    "                a,\n",
    "                reduce(mul, a.shape[:-1]),\n",
    "                a.shape[-1],\n",
    "            )\n",
    "        else:\n",
    "            # a from dot-product affinities and bias , will be added point attention \n",
    "            a = a + pt_att \n",
    "            a = a + square_mask.unsqueeze(-3)\n",
    "            a = self.softmax(a)\n",
    "\n",
    "        ################\n",
    "        # Compute output\n",
    "        ################\n",
    "        # [*, N_res, H, C_hidden]\n",
    "        o = torch.matmul(\n",
    "            a, v.transpose(-2, -3).to(dtype=a.dtype)\n",
    "        ).transpose(-2, -3)\n",
    "\n",
    "        # [*, N_res, H * C_hidden]\n",
    "        o = flatten_final_dims(o, 2)\n",
    "\n",
    "        # [*, H, 3, N_res, P_v] \n",
    "        if(inplace_safe):\n",
    "            v_pts = permute_final_dims(v_pts, (1, 3, 0, 2))\n",
    "            o_pt = [\n",
    "                torch.matmul(a, v.to(a.dtype)) \n",
    "                for v in torch.unbind(v_pts, dim=-3)\n",
    "            ]\n",
    "            o_pt = torch.stack(o_pt, dim=-3)\n",
    "        else:\n",
    "            o_pt = torch.sum(\n",
    "                (\n",
    "                    a[..., None, :, :, None]\n",
    "                    * permute_final_dims(v_pts, (1, 3, 0, 2))[..., None, :, :]\n",
    "                ),\n",
    "                dim=-2,\n",
    "            )\n",
    "            # step by step\n",
    "            # a[...,None,:,:,None] from [*, H, N_res, N_res]                -> [*, H, 1, N_res, N_res,    1]\n",
    "            # v_pts from [*, N_res, H, P_v, 3] ->  [*, H, 3, N_res, P_v]    -> [*, H, 3, 1,     N_res,  P_v] \n",
    "            # 与 [*, H, N_res, N_res] @ [*, H, N_res, 3*P_v] 然后再把最后一个维度展开有啥区别?\n",
    "\n",
    "        # [*, N_res, H, P_v, 3]\n",
    "        o_pt = permute_final_dims(o_pt, (2, 0, 3, 1))\n",
    "        # _rot : [*, N_res, 1, 1, 3, 3], _trans : [*, N_res, 1, 1, 3]\n",
    "        o_pt = r[..., None, None].invert_apply(o_pt)\n",
    "\n",
    "        # [*, N_res, H * P_v]\n",
    "        o_pt_norm = flatten_final_dims(\n",
    "            torch.sqrt(torch.sum(o_pt ** 2, dim=-1) + self.eps), 2\n",
    "        )\n",
    "\n",
    "        # [*, N_res, H * P_v, 3]\n",
    "        o_pt = o_pt.reshape(*o_pt.shape[:-3], -1, 3)\n",
    "\n",
    "        if(_offload_inference):\n",
    "            z[0] = z[0].to(o_pt.device)\n",
    "\n",
    "        # [*, N_res, H, C_z]\n",
    "        # [*, N_res, H, N_res] @ [*, N_res, N_res, c_z] -> [*, N_res, H, c_z]\n",
    "        o_pair = torch.matmul(a.transpose(-2, -3), z[0].to(dtype=a.dtype))\n",
    "\n",
    "        # [*, N_res, H * C_z]\n",
    "        o_pair = flatten_final_dims(o_pair, 2)\n",
    "\n",
    "        # [*, N_res, C_s]\n",
    "        s = self.linear_out(\n",
    "            torch.cat(\n",
    "                (o, *torch.unbind(o_pt, dim=-1), o_pt_norm, o_pair), dim=-1\n",
    "            ).to(dtype=z[0].dtype)\n",
    "            # concat list : H * (4*P_v + C_z + c_hidden)\n",
    "            # o : [*, N_res, H*c_hidden]\n",
    "            # * torch.unbind(o_pt, dim=-1),解压每一个坐标的维度 \n",
    "            # - o_pt_p1 [*, N_res, H*P_v]\n",
    "            # - o_pt_p2 [*, N_res, H*P_v]\n",
    "            # - o_pt_p3 [*, N_res, H*P_v]\n",
    "            # o_pt_norm : [*, N_res, H*P_v]\n",
    "            # o_pair    : [*, N_res, H*C_z]\n",
    "        )\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023.6.26 学习openfold chunk size相关函数\n",
    "\n",
    "chunk size是每一小块里面的氨基酸数目，具体的for 循环次数是$\\frac{N_{res}}{N_{chunk\\ size}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数1：递归得到一个tensor dict的所有维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_dims(tree):\n",
    "    shapes = []\n",
    "    tree_type = type(tree)\n",
    "    if tree_type is dict:\n",
    "        for v in tree.values():\n",
    "            shapes.extend(_fetch_dims(v))\n",
    "    elif tree_type is list or tree_type is tuple:\n",
    "        for t in tree:\n",
    "            shapes.extend(_fetch_dims(t))\n",
    "    elif tree_type is torch.Tensor:\n",
    "        shapes.append(tree.shape)\n",
    "    else:\n",
    "        print(tree_type)\n",
    "        raise ValueError(\"Not supported\")\n",
    "\n",
    "    return shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "x = {\"p1\":torch.randn(1, 2, 3), \"p2\":torch.randn(1, 2, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 2, 3]), torch.Size([1, 2, 3])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_fetch_dims(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将一个平铺之后的索引flat_idx转为在dims中的多维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def _flat_idx_to_idx(\n",
    "    flat_idx: int,\n",
    "    dims: Tuple[int],\n",
    ") -> Tuple[int]:\n",
    "    idx = []\n",
    "    for d in reversed(dims):\n",
    "        idx.append(flat_idx % d)\n",
    "        flat_idx = flat_idx // d\n",
    "\n",
    "    return tuple(reversed(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "array = [[0, 1, 2],\n",
    "         [3, 4, 5]]\n",
    "flat_idx = 4\n",
    "dims = (2, 3)\n",
    "\n",
    "result = _flat_idx_to_idx(flat_idx, dims)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy/torch einsum function\n",
    "\n",
    "Take openfold code as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. covariance matrix and outer product\n",
    "N_seq = 77\n",
    "N_res = 256\n",
    "c = 128\n",
    "msa_repre = torch.randn(N_seq,N_res,c)\n",
    "\n",
    "i = 10\n",
    "j = 20\n",
    "\n",
    "ai = msa_repre[:,i,:] # [N_seq,c]\n",
    "bj = msa_repre[:,j,:] # [N_seq,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covaraince = 1/N_seq * ai.T @ bj # covariance = 1/N \\sum_{i=1}^{N} ais @ bis\n",
    "outer_product = ai.unsqueeze(-1) @ bj.unsqueeze(1) # save all the rank1 matr\n",
    "(covaraince - outer_product.mean(dim=0)) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 77, 128]) torch.Size([256, 77, 128])\n"
     ]
    }
   ],
   "source": [
    "# 2. torch.einsum is much faster than for loop to calculate the N^2 covariance matrix\n",
    "a = nn.Linear(c,c)(msa_repre)\n",
    "b = nn.Linear(c,c)(msa_repre)\n",
    "a = a.transpose(-2,-3)\n",
    "b = b.transpose(-2,-3)\n",
    "print(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_einsum_zb = torch.einsum(\"...abc,dbf->adcf\",a,b)\n",
    "outer_einsum_zb /= N_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a shape torch.Size([256, 77, 128])\n",
      "outer shape torch.Size([256, 256, 128, 128])\n",
      "zb outer einsum is equal to outer product : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"a shape {a.shape}\") # N_res sets of input faetures with N_seq,c\n",
    "outer_esinsum = torch.einsum(\"...bac,...dae->...bdce\", a, b)\n",
    "outer_esinsum = outer_esinsum / N_seq\n",
    "print(f\"outer shape {outer_esinsum.shape}\") # N_res sets of outer product with N_seq,c,c\n",
    "\n",
    "print(f\"zb outer einsum is equal to outer product : {torch.allclose(outer_einsum_zb,outer_esinsum)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer shape torch.Size([256, 256, 128, 128])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "outer = torch.zeros(N_res,N_res,c,c)\n",
    "for i in range(N_res):\n",
    "    for j in range(N_res):\n",
    "        outer[i,j] = 1/N_seq * a[i,...].T @ b[j,...]\n",
    "print(f\"outer shape {outer.shape}\") # N_res sets of outer product with N_seq,c,c\n",
    "print(torch.all((outer_esinsum - outer).abs() < 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. how to understand einsum : Einstenin summation convention \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1 Implicit summation: In implicit mode, the chosen subscripts are important since the axes of the output are reordered alphabetically\n",
    "# Whenever a label is repeated it is summed,\n",
    "x = np.array([1,2,3,4])\n",
    "np.einsum(\"i,i\",x,x)\n",
    "\n",
    "#  If a label appears only once, it is not summed\n",
    "np.einsum(\"i\",x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner product torch.Size([]) outer product torch.Size([2, 2])\n",
      "matrix multiplication torch.Size([3, 3]) elementwise multiplication torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Explicit mode : In explicit mode the output can be directly controlled by specifying output subscript labels. This requires the identifier ‘->’ as well as the list of output subscript labels. This feature increases the flexibility of the function since summing can be disabled or forced when required. \n",
    "# same label will be summed multiplication, different label will be multiplication once\n",
    "\n",
    "A = torch.randn(2)\n",
    "B = torch.randn(2)\n",
    "C_inner_product = torch.einsum(\"i,i->\",A,B) # same label will be \\sum A_i * B_i\n",
    "C_outer_product = torch.einsum(\"i,j->ij\",A,B) # different label will be A_i * B_j \n",
    "C_elementwise_product = torch.einsum(\"i,i->i\",A,B) # different label will be A_i * B_i no sum , the axis will be preserved\n",
    "print(f\"inner product {C_inner_product.shape} outer product {C_outer_product.shape}\")\n",
    "\n",
    "A = torch.randn(3,3)\n",
    "B = torch.randn(3,3)\n",
    "C_matrix_multiplication = torch.einsum(\"ij,jk->ik\",A,B) # same label will be \\sum_{j=1}^N A_ij * B_jk\n",
    "C_elementwise_multiplication = torch.einsum(\"ij,ij->ij\",A,B) # different label will be A_ij * B_ij no sum , the axis will be preserved\n",
    "print(f\"matrix multiplication {C_matrix_multiplication.shape} elementwise multiplication {C_elementwise_multiplication.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.8008) tensor(-1.8008)\n"
     ]
    }
   ],
   "source": [
    "# element-wise multiplication and summation \n",
    "A = torch.randn(3,3)\n",
    "B = torch.randn(3,3)\n",
    "C = torch.einsum(\"ij,ij->\",A,B) \n",
    "print(C,torch.sum(A*B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3, 3])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# kroneck product\n",
    "A = torch.randn(3,3)\n",
    "B = torch.randn(3,3)\n",
    "C = torch.einsum(\"ij,mn->ijmn\",A,B) \n",
    "print(C.shape)\n",
    "# what is C\n",
    "# C[i,j,m,n] = A[i,j] * B[m,n]\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print(C[i,j] == A[i,j] * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.9452,  3.6882, -1.3189],\n",
      "        [-0.4733, -0.9803,  0.8564],\n",
      "        [ 0.2147, -0.7913,  0.2626]]) \n",
      " tensor([[ 4.9452,  3.6882, -1.3189],\n",
      "        [-0.4733, -0.9803,  0.8564],\n",
      "        [ 0.2147, -0.7913,  0.2626]])\n"
     ]
    }
   ],
   "source": [
    "# transpose product 1: A^T @ B\n",
    "A = torch.randn(3,3)\n",
    "B = torch.randn(3,3)\n",
    "C = torch.einsum(\"ij,im->jm\",A,B) \n",
    "print(C,\"\\n\",A.T@B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6810,  1.1732,  0.0499],\n",
      "        [ 0.8210, -0.6290, -0.1657],\n",
      "        [-0.2496,  1.0134, -0.4312]]) \n",
      " tensor([[-0.6810,  1.1732,  0.0499],\n",
      "        [ 0.8210, -0.6290, -0.1657],\n",
      "        [-0.2496,  1.0134, -0.4312]])\n"
     ]
    }
   ],
   "source": [
    "# transpose product 2: A @ B^T\n",
    "A = torch.randn(3,3)\n",
    "B = torch.randn(3,3)\n",
    "C = torch.einsum(\"ij,bj->ib\",A,B) \n",
    "print(C,\"\\n\",A@B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice\n",
    "a = torch.randn(3)\n",
    "b = torch.randn(3)\n",
    "c = torch.einsum(\"i,i->\",a,b)\n",
    "d = torch.einsum(\"i,i->i\",a,b)\n",
    "e = torch.einsum(\"i,j->ij\",a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace and diagonal\n",
    "A = torch.randn(3,3)\n",
    "trace_A = torch.einsum(\"ii->\",A) # summation Aii \n",
    "diag_A = torch.einsum(\"ii->i\",A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# sum on an axis\n",
    "i,j,m,n = 3,4,5,6\n",
    "A = torch.randn(i,j,m,n)\n",
    "sum_A = torch.einsum(\"ijmn->ijm\",A) # sum all the elements\n",
    "sum_A_n = torch.sum(A,dim=-1)\n",
    "print(torch.all(sum_A == sum_A_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4400., 4730.],\n",
       "       [4532., 4874.],\n",
       "       [4664., 5018.],\n",
       "       [4796., 5162.],\n",
       "       [4928., 5306.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(60.).reshape(3,4,5) # i j k\n",
    "b = np.arange(24.).reshape(4,3,2) # j i l\n",
    "np.einsum('ijk,jil->kl', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(60.).reshape(3,4,5) # i j k\n",
    "b = np.arange(24.).reshape(4,3,2) # j i l\n",
    "np.einsum('ijk,jil->kl', a, b) # kl is output subscript, ij is summation subscript, Okl = \\sum_i \\sum_j a_{ijk} * b_{jil}\n",
    "\n",
    "tensor_contraction = np.zeros((5,2))\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        tensor_contraction[i,j] = np.trace(a[...,i] @ b[...,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4400., 4730.],\n",
       "       [4532., 4874.],\n",
       "       [4664., 5018.],\n",
       "       [4796., 5162.],\n",
       "       [4928., 5306.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice alphafold outer product mean\n",
    "outer_zb = torch.einsum(\"...abc,...dbe->...adce\",a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_zb = outer_zb / N_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class OuterProductMean(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 10.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_m, c_z, c_hidden, eps=1e-3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_m:\n",
    "                MSA embedding channel dimension\n",
    "            c_z:\n",
    "                Pair embedding channel dimension\n",
    "            c_hidden:\n",
    "                Hidden channel dimension\n",
    "        \"\"\"\n",
    "        super(OuterProductMean, self).__init__()\n",
    "\n",
    "        self.c_m = c_m\n",
    "        self.c_z = c_z\n",
    "        self.c_hidden = c_hidden\n",
    "        self.eps = eps\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(c_m)\n",
    "        self.linear_1 = nn.Linear(c_m, c_hidden)\n",
    "        self.linear_2 = nn.Linear(c_m, c_hidden)\n",
    "        self.linear_out = nn.Linear(c_hidden ** 2, c_z)\n",
    "\n",
    "    def _opm(self, a, b):\n",
    "        # [*, N_res, N_res, C, C]\n",
    "        # a : [*, N_res, S, C]\n",
    "        # b : [*, N_res, S, C]\n",
    "        outer = torch.einsum(\"...bac,...dae->...bdce\", a, b)\n",
    "\n",
    "        # [*, N_res, N_res, C * C]\n",
    "        outer = outer.reshape(outer.shape[:-2] + (-1,))\n",
    "\n",
    "        # [*, N_res, N_res, C_z]\n",
    "        outer = self.linear_out(outer)\n",
    "\n",
    "        return outer\n",
    "    \n",
    "    def forward_by_zb(self,m):\n",
    "        ## by zb to understand einsum, outer product mean to extract co-evolution information, chunk size \n",
    "        ln = self.layer_norm(m)\n",
    "        a = self.linear_1(ln)\n",
    "        b = self.linear_2(ln)\n",
    "        return self._opm(a,b)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _chunk(self, \n",
    "        a: torch.Tensor, \n",
    "        b: torch.Tensor, \n",
    "        chunk_size: int\n",
    "    ) -> torch.Tensor:\n",
    "        # Since the \"batch dim\" in this case is not a true batch dimension\n",
    "        # (in that the shape of the output depends on it), we need to\n",
    "        # iterate over it ourselves\n",
    "        # a.shape [N_seq, N_res, C] -> (-1,N_seq,N_res,C)\n",
    "        a_reshape = a.reshape((-1,) + a.shape[-3:])\n",
    "        b_reshape = b.reshape((-1,) + b.shape[-3:])\n",
    "        out = []\n",
    "        for a_prime, b_prime in zip(a_reshape, b_reshape):\n",
    "            outer = chunk_layer(\n",
    "                partial(self._opm, b=b_prime),\n",
    "                {\"a\": a_prime},\n",
    "                chunk_size=chunk_size,\n",
    "                no_batch_dims=1,\n",
    "            )\n",
    "            out.append(outer)\n",
    "\n",
    "        # For some cursed reason making this distinction saves memory\n",
    "        if(len(out) == 1):\n",
    "            outer = out[0].unsqueeze(0)\n",
    "        else:\n",
    "            outer = torch.stack(out, dim=0)\n",
    "\n",
    "        outer = outer.reshape(a.shape[:-3] + outer.shape[1:])\n",
    "\n",
    "        return outer\n",
    "\n",
    "    def _forward(self, \n",
    "        m: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        chunk_size: Optional[int] = None,\n",
    "        inplace_safe: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            m:\n",
    "                [*, N_seq, N_res, C_m] MSA embedding\n",
    "            mask:\n",
    "                [*, N_seq, N_res] MSA mask\n",
    "        Returns:\n",
    "            [*, N_res, N_res, C_z] pair embedding update\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            mask = m.new_ones(m.shape[:-1])\n",
    "\n",
    "        # [*, N_seq, N_res, C_m]\n",
    "        ln = self.layer_norm(m)\n",
    "\n",
    "        # [*, N_seq, N_res, C]\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        a = self.linear_1(ln) \n",
    "        a = a * mask\n",
    "        \n",
    "        b = self.linear_2(ln) \n",
    "        b = b * mask\n",
    "\n",
    "        del ln\n",
    "\n",
    "        a = a.transpose(-2, -3)\n",
    "        b = b.transpose(-2, -3)\n",
    "\n",
    "        if chunk_size is not None:\n",
    "            outer = self._chunk(a, b, chunk_size)\n",
    "        else:\n",
    "            outer = self._opm(a, b)\n",
    "\n",
    "        # [*, N_res, N_res, 1]\n",
    "        norm = torch.einsum(\"...abc,...adc->...bdc\", mask, mask)\n",
    "        norm = norm + self.eps\n",
    "\n",
    "        # [*, N_res, N_res, C_z]\n",
    "        if(inplace_safe):\n",
    "            outer /= norm\n",
    "        else:\n",
    "            outer = outer / norm\n",
    "\n",
    "        return outer\n",
    "\n",
    "    def forward(self,\n",
    "                m: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None,\n",
    "                chunk_size: Optional[int] = None,\n",
    "                inplace_safe: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        if(is_fp16_enabled()):\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                return self._forward(m.float(), mask, chunk_size, inplace_safe)\n",
    "        else:\n",
    "            return self._forward(m, mask, chunk_size, inplace_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm 10:msa pair weighted averaging with gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input projections\n",
    "\n",
    "# pseudo msa and residue\n",
    "N_seq = 50\n",
    "N_res = 256\n",
    "\n",
    "# channel information\n",
    "cm = 64\n",
    "cz = 128\n",
    "c = 32\n",
    "head = 8\n",
    "\n",
    "# pseudo data\n",
    "msa_input = torch.randn(N_seq,N_res,cm)\n",
    "pair_representation = torch.rand(N_res,N_res,cz)\n",
    "\n",
    "# model\n",
    "layernorm1 = nn.LayerNorm(cm)\n",
    "layernorm2 = nn.LayerNorm(cz)\n",
    "linear1 = nn.Linear(cm,c*head,bias=False)\n",
    "linear2 = nn.Linear(cz,head,bias=False)\n",
    "linear3 = nn.Linear(cm,c*head,bias=False)\n",
    "linear4 = nn.Linear(c*head,cm,bias=False)\n",
    "\n",
    "# Input projection\n",
    "msa = layernorm1(msa_input)\n",
    "V = linear1(msa)\n",
    "V = V.reshape(V.shape[:-1] + (head,-1)) # multiple head projection\n",
    "bias = linear2(layernorm2(pair_representation))\n",
    "gating = nn.Sigmoid()(linear3(msa))\n",
    "gating = gating.reshape(gating.shape[:-1] + (head,-1)) # multiple head projection\n",
    "\n",
    "# Weighted average with gating\n",
    "weight = nn.Softmax(dim=-2)(bias) # N_res, N_res, head\n",
    "output = torch.einsum(\"sih...,ijh,sjh...->sih...\",gating,weight,V)\n",
    "\n",
    "# Output projection\n",
    "output = linear4(output.reshape(output.shape[:-2]+(-1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output : N_res,N_seq,c \n",
    "output_einsum = torch.einsum(\"sih...,ijh,sjh...->sih...\",gating,weight,V)\n",
    "output = torch.zeros(N_seq,N_res,head,c)\n",
    "\n",
    "\n",
    "for s in range(output.shape[0]):\n",
    "    for i in range(output.shape[1]):\n",
    "        for h in range(output.shape[2]):\n",
    "            output[s,i,h] = gating[s,i,h] * torch.sum(V[s,:,h,:]*weight[i,:,h].unsqueeze(1),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(torch.all((output_einsum - output) < 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_einsum[0,0] - output[0,0]) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm 12/13 Triangle updates of the pair representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(N_res,N_res,cz)\n",
    "z = nn.LayerNorm(cz)(z)\n",
    "a = nn.Sigmoid()(nn.Linear(cz,cz,bias=False)(z)) * nn.Linear(cz,cz,bias=False)(z)\n",
    "b = nn.Sigmoid()(nn.Linear(cz,cz,bias=False)(z)) * nn.Linear(cz,cz,bias=False)(z)\n",
    "g = nn.Sigmoid()(nn.Linear(cz,cz,bias=False)(z))\n",
    "\n",
    "\n",
    "direction = \"outging\"\n",
    "\n",
    "if direction == \"outging\":\n",
    "    intermediate = nn.Linear(cz,cz,bias=False)(nn.LayerNorm(cz)(torch.einsum(\"ikc,jkc->ijc\",a,b)))\n",
    "elif direction == \"incmoing\":\n",
    "    intermediate = nn.Linear(cz,cz,bias=False)(nn.LayerNorm(cz)(torch.einsum(\"kic,kjc->ijc\",a,b)))\n",
    "z_output = g * intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = \"incoming\"\n",
    "\n",
    "\n",
    "intermediate_incoming = nn.Linear(cz,cz,bias=False)(nn.LayerNorm(cz)(torch.einsum(\"kic,kjc->ijc\",a,b)))\n",
    "z_output_incoming = g * intermediate_incoming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "test_outging = torch.einsum(\"ik...,jk...->ij...\",a,b)\n",
    "print(torch.all(test_outging[0,0] - torch.sum(a[0,] * b[0,],dim=0) < 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "test_incoming = torch.einsum(\"ki...,kj...->ij...\",a,b)\n",
    "print(torch.all(test_incoming[0,0] - torch.sum(a[:,0] * b[:,0],dim=0) < 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaFold Multihead Attenntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0558, -0.2889,  0.3755],\n",
       "        [ 0.2219,  0.1194,  0.5650],\n",
       "        [-0.2853, -1.7966, -0.1812]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,3)\n",
    "y = torch.randn(3,3)\n",
    "torch.einsum(\"ij,jk->ik\",x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6112,  1.1692, -1.8062],\n",
       "        [-0.0200, -0.2070,  0.0071],\n",
       "        [ 1.0505,  3.8545,  1.8491]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij,ik->ik\",x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6112,  1.1692, -1.8062],\n",
       "        [-0.0200, -0.2070,  0.0071],\n",
       "        [ 1.0505,  3.8545,  1.8491]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.reshape(3,1,3)*y.reshape(3,3,1)).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6112,  1.1692, -1.8062],\n",
       "        [-0.0200, -0.2070,  0.0071],\n",
       "        [ 1.0505,  3.8545,  1.8491]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.zeros((3,3))\n",
    "for i in range(output.shape[0]):\n",
    "    for k in range(output.shape[1]):\n",
    "        output[i,k] = torch.sum(x[i,] * y[i,k])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256, 8, 16])\n",
      "torch.Size([10, 256, 8, 16])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "batch = 10 \n",
    "N_res = 256\n",
    "channel = 128\n",
    "head = 8\n",
    "\n",
    "pseudo_input = torch.randn(batch,N_res,channel)\n",
    "\n",
    "linear1 = nn.Linear(channel,3*channel)\n",
    "linear2 = nn.Linear(channel,channel)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "q,k,v = linear1(pseudo_input).chunk(3,dim=-1)\n",
    "q = q.reshape(batch,N_res,head,int(channel/head)) * (channel/head) ** -0.5\n",
    "k = k.reshape(batch,N_res,head,int(channel/head))\n",
    "v = v.reshape(batch,N_res,head,int(channel/head))\n",
    "attention1 = softmax(q.permute(0,2,1,3) @ k.permute(0,2,3,1))\n",
    "attention2 = softmax(torch.einsum(\"bqhc,bkhc->bhqk\",q,k))\n",
    "\n",
    "output1 = (attention1 @ v.permute(0,2,1,3)).transpose(-2,-3)\n",
    "output2 = torch.einsum(\"bhqk,bkhc->bqhc\",attention2,v) # 之前的错误是写为了bhqk, bqhc；真正应该是bkhc\n",
    "\n",
    "output1 = linear2(output1.reshape(output1.shape[:-2]+(-1,)))\n",
    "output2 = linear2(output2.reshape(output2.shape[:-2]+(-1,)))\n",
    "\n",
    "print(torch.allclose(attention1,attention2))\n",
    "print(torch.allclose(output1,output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(24).reshape(2,3,4)\n",
    "print(x)\n",
    "print(x.reshape(4,3,2))\n",
    "print(x.permute(2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]],\n",
       "\n",
       "        [[18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, 12],\n",
       "         [ 4, 16],\n",
       "         [ 8, 20]],\n",
       "\n",
       "        [[ 1, 13],\n",
       "         [ 5, 17],\n",
       "         [ 9, 21]],\n",
       "\n",
       "        [[ 2, 14],\n",
       "         [ 6, 18],\n",
       "         [10, 22]],\n",
       "\n",
       "        [[ 3, 15],\n",
       "         [ 7, 19],\n",
       "         [11, 23]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.permute(2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_einsum = torch.einsum(\"bqhc,bkhc->bhqk\", q, k)\n",
    "activation_matrix = q.permute(0, 2, 1, 3) @ k.permute(0, 2, 1, 3).transpose(-1, -2)\n",
    "\n",
    "# 确保两者相同\n",
    "torch.allclose(activation_einsum, activation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation\n",
    "activation = torch.einsum(\"bqhc,bkhc->bhqk\",q,k)\n",
    "# output and concat \n",
    "output = torch.einsum(\"bhqk,bqhc->bqhc\",activation,v)\n",
    "output = output.reshape(batch,N_res,channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "activation_weight = q.reshape(batch,head,N_res,-1) @ k.reshape(batch,head,N_res,-1).transpose(-1,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6665,  0.6064, -0.4296,  ...,  0.3532, -0.1957,  0.5937],\n",
       "         [ 0.2747, -0.4977, -0.1304,  ...,  0.0076, -0.3048, -0.9296],\n",
       "         [ 0.0308, -0.1339,  0.3036,  ..., -0.4195,  0.3613, -0.0514],\n",
       "         ...,\n",
       "         [ 0.5664,  0.2680, -0.1534,  ...,  0.2680, -0.0338,  0.2619],\n",
       "         [-0.5188, -0.1951,  1.1662,  ..., -0.3544, -0.3555,  0.2509],\n",
       "         [-0.3540, -0.2852,  0.1981,  ...,  0.3805,  0.0887,  0.2447]],\n",
       "\n",
       "        [[-0.3834,  0.1719, -0.1108,  ..., -0.4971, -0.1468, -0.3366],\n",
       "         [ 0.1695,  0.7629,  0.0778,  ..., -0.3644,  0.4890, -0.3516],\n",
       "         [ 0.1993,  0.2595,  0.7844,  ..., -0.6483, -0.2891,  0.4231],\n",
       "         ...,\n",
       "         [ 0.2987,  0.8410, -0.1231,  ...,  0.0992, -0.2299,  0.3941],\n",
       "         [ 0.0879,  0.3421,  0.3264,  ..., -0.8061,  0.0335,  0.5958],\n",
       "         [ 0.4242, -0.2689,  0.4354,  ..., -0.4684,  1.0080,  0.7794]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6665, -0.6025,  0.6064,  ...,  0.5723, -0.0917,  0.3571],\n",
       "         [ 0.0574, -0.3834,  0.1375,  ..., -0.1645, -0.2602, -0.4735],\n",
       "         [ 0.2747,  0.5407, -0.4977,  ..., -0.2870, -0.1773,  0.4674],\n",
       "         ...,\n",
       "         [-0.6115, -0.7624,  0.1468,  ...,  0.0752, -0.1521, -0.5720],\n",
       "         [-0.0367, -0.2636, -0.3998,  ...,  0.7545, -0.3549, -0.4083],\n",
       "         [-0.0781, -0.2383,  0.0311,  ..., -0.6561, -0.5898,  0.2084]],\n",
       "\n",
       "        [[ 0.1004, -0.4839,  0.1916,  ...,  0.4732, -1.1515, -0.3070],\n",
       "         [ 0.6639, -0.1203, -0.0753,  ..., -0.1722,  0.1032,  0.2047],\n",
       "         [ 0.0483,  0.6451, -0.0687,  ..., -0.2258, -0.3715,  0.0644],\n",
       "         ...,\n",
       "         [-0.6793, -0.2143,  0.1027,  ...,  0.0335,  0.0279,  0.5958],\n",
       "         [-0.3293,  0.1868, -0.2412,  ..., -0.5827,  0.2447, -0.1429],\n",
       "         [-0.6114,  0.3001, -0.2557,  ...,  1.0080, -0.1627,  0.7794]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(hk.Module):\n",
    "  \"\"\"Multihead attention.\"\"\"\n",
    "\n",
    "  def __init__(self, config, global_config, output_dim, name='attention'):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self.config = config\n",
    "    self.global_config = global_config\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "  def __call__(self, q_data, m_data, mask, nonbatched_bias=None):\n",
    "    \"\"\"Builds Attention module.\n",
    "\n",
    "    Arguments:\n",
    "      q_data: A tensor of queries, shape [batch_size, N_queries, q_channels].\n",
    "      m_data: A tensor of memories from which the keys and values are\n",
    "        projected, shape [batch_size, N_keys, m_channels].\n",
    "      mask: A mask for the attention, shape [batch_size, N_queries, N_keys].\n",
    "      nonbatched_bias: Shared bias, shape [N_queries, N_keys].\n",
    "\n",
    "    Returns:\n",
    "      A float32 tensor of shape [batch_size, N_queries, output_dim].\n",
    "    \"\"\"\n",
    "    # Sensible default for when the config keys are missing\n",
    "    key_dim = self.config.get('key_dim', int(q_data.shape[-1]))\n",
    "    value_dim = self.config.get('value_dim', int(m_data.shape[-1]))\n",
    "    num_head = self.config.num_head\n",
    "    assert key_dim % num_head == 0\n",
    "    assert value_dim % num_head == 0\n",
    "    key_dim = key_dim // num_head\n",
    "    value_dim = value_dim // num_head\n",
    "\n",
    "    q_weights = hk.get_parameter(\n",
    "        'query_w', shape=(q_data.shape[-1], num_head, key_dim),\n",
    "        dtype=q_data.dtype,\n",
    "        init=glorot_uniform())\n",
    "    k_weights = hk.get_parameter(\n",
    "        'key_w', shape=(m_data.shape[-1], num_head, key_dim),\n",
    "        dtype=q_data.dtype,\n",
    "        init=glorot_uniform())\n",
    "    v_weights = hk.get_parameter(\n",
    "        'value_w', shape=(m_data.shape[-1], num_head, value_dim),\n",
    "        dtype=q_data.dtype,\n",
    "        init=glorot_uniform())\n",
    "\n",
    "    q = jnp.einsum('bqa,ahc->bqhc', q_data, q_weights) * key_dim**(-0.5)\n",
    "    k = jnp.einsum('bka,ahc->bkhc', m_data, k_weights)\n",
    "    v = jnp.einsum('bka,ahc->bkhc', m_data, v_weights)\n",
    "    logits = jnp.einsum('bqhc,bkhc->bhqk', q, k)\n",
    "    if nonbatched_bias is not None:\n",
    "      logits += jnp.expand_dims(nonbatched_bias, axis=0)\n",
    "    logits = jnp.where(mask, logits, _SOFTMAX_MASK)\n",
    "    weights = utils.stable_softmax(logits)\n",
    "    weighted_avg = jnp.einsum('bhqk,bkhc->bqhc', weights, v)\n",
    "\n",
    "    if self.global_config.zero_init:\n",
    "      init = hk.initializers.Constant(0.0)\n",
    "    else:\n",
    "      init = glorot_uniform()\n",
    "\n",
    "    if self.config.gating:\n",
    "      gating_weights = hk.get_parameter(\n",
    "          'gating_w',\n",
    "          shape=(q_data.shape[-1], num_head, value_dim),\n",
    "          dtype=q_data.dtype,\n",
    "          init=hk.initializers.Constant(0.0))\n",
    "      gating_bias = hk.get_parameter(\n",
    "          'gating_b',\n",
    "          shape=(num_head, value_dim),\n",
    "          dtype=q_data.dtype,\n",
    "          init=hk.initializers.Constant(1.0))\n",
    "\n",
    "      gate_values = jnp.einsum('bqc, chv->bqhv', q_data,\n",
    "                               gating_weights) + gating_bias\n",
    "\n",
    "      gate_values = jax.nn.sigmoid(gate_values)\n",
    "\n",
    "      weighted_avg *= gate_values\n",
    "\n",
    "    o_weights = hk.get_parameter(\n",
    "        'output_w', shape=(num_head, value_dim, self.output_dim),\n",
    "        dtype=q_data.dtype,\n",
    "        init=init)\n",
    "    o_bias = hk.get_parameter(\n",
    "        'output_b', shape=(self.output_dim,),\n",
    "        dtype=q_data.dtype,\n",
    "        init=hk.initializers.Constant(0.0))\n",
    "\n",
    "    output = jnp.einsum('bqhc,hco->bqo', weighted_avg, o_weights) + o_bias\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n",
    "\n",
    "# This will prompt you to get an API key from huggingface hub, make one with\n",
    "# \"Read\" or \"Write\" permission and copy it back here.\n",
    "login()\n",
    "\n",
    "# This will download the model weights and instantiate the model on your machine.\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(model_name=\"esm3_sm_open_v1\").to(\"cuda\") # or \"cpu\"\n",
    "\n",
    "# Generate a completion for a partial Carbonic Anhydrase (2vvb)\n",
    "prompt = \"___________________________________________________DQATSLRILNNGHAFNVEFDDSQDKAVLKGGPLDGTYRLIQFHFHWGSLDGQGSEHTVDKKKYAAELHLVHWNTKYGDFGKAVQQPDGLAVLGIFLKVGSAKPGLQKVVDVLDSIKTKGKSADFTNFDPRGLLPESLDYWTYPGSLTTPP___________________________________________________________\"\n",
    "protein = ESMProtein(sequence=prompt)\n",
    "# Generate the sequence, then the structure. This will iteratively unmask the sequence track.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8, temperature=0.7))\n",
    "# We can show the predicted structure for the generated sequence.\n",
    "protein = model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "protein.to_pdb(\"./generation.pdb\")\n",
    "# Then we can do a round trip design by inverse folding the sequence and recomputing the structure\n",
    "protein.sequence = None\n",
    "protein = model.generate(protein, GenerationConfig(track=\"sequence\", num_steps=8))\n",
    "protein.structure = None\n",
    "protein = model.generate(protein, GenerationConfig(track=\"structure\", num_steps=8))\n",
    "protein.to_pdb(\"./round_tripped.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "from typing import Set\n",
    "\n",
    "def _keep_line(line: str, seqnames: Set[str]) -> bool:\n",
    "  \"\"\"Function to decide which lines to keep.\"\"\"\n",
    "  if not line.strip():\n",
    "    return True\n",
    "  if line.strip() == '//':  # End tag\n",
    "    return True\n",
    "  if line.startswith('# STOCKHOLM'):  # Start tag\n",
    "    return True\n",
    "  if line.startswith('#=GC RF'):  # Reference Annotation Line\n",
    "    return True\n",
    "  if line[:4] == '#=GS':  # Description lines - keep if sequence in list.\n",
    "    _, seqname, _ = line.split(maxsplit=2)\n",
    "    return seqname in seqnames\n",
    "  elif line.startswith('#'):  # Other markup - filter out\n",
    "    return False\n",
    "  else:  # Alignment data - keep if sequence in list.\n",
    "    seqname = line.partition(' ')[0]\n",
    "    return seqname in seqnames\n",
    "\n",
    "def deduplicate_stockholm_msa(stockholm_msa: str) -> str:\n",
    "  \"\"\"Remove duplicate sequences (ignoring insertions wrt query).\"\"\"\n",
    "  sequence_dict = collections.defaultdict(str)\n",
    "\n",
    "  # First we must extract all sequences from the MSA.\n",
    "  for line in stockholm_msa.splitlines():\n",
    "    # Only consider the alignments - ignore reference annotation, empty lines,\n",
    "    # descriptions or markup.\n",
    "    if line.strip() and not line.startswith(('#', '//')):\n",
    "      line = line.strip()\n",
    "      seqname, alignment = line.split()\n",
    "      sequence_dict[seqname] += alignment\n",
    "\n",
    "  seen_sequences = set()\n",
    "  seqnames = set()\n",
    "  # First alignment is the query.\n",
    "  query_align = next(iter(sequence_dict.values()))\n",
    "  mask = [c != '-' for c in query_align]  # Mask is False for insertions.\n",
    "  for seqname, alignment in sequence_dict.items():\n",
    "    # Apply mask to remove all insertions from the string.\n",
    "    masked_alignment = ''.join(itertools.compress(alignment, mask))\n",
    "    if masked_alignment in seen_sequences:\n",
    "      continue\n",
    "    else:\n",
    "      seen_sequences.add(masked_alignment)\n",
    "      seqnames.add(seqname)\n",
    "\n",
    "  filtered_lines = []\n",
    "  for line in stockholm_msa.splitlines():\n",
    "    if _keep_line(line, seqnames):\n",
    "      filtered_lines.append(line)\n",
    "\n",
    "  return '\\n'.join(filtered_lines) + '\\n'\n",
    "\n",
    "def remove_empty_columns_from_stockholm_msa(stockholm_msa: str) -> str:\n",
    "  \"\"\"Removes empty columns (dashes-only) from a Stockholm MSA.\"\"\"\n",
    "  processed_lines = {}\n",
    "  unprocessed_lines = {}\n",
    "  for i, line in enumerate(stockholm_msa.splitlines()):\n",
    "    if line.startswith('#=GC RF'):\n",
    "      reference_annotation_i = i\n",
    "      reference_annotation_line = line\n",
    "      # Reached the end of this chunk of the alignment. Process chunk.\n",
    "      _, _, first_alignment = line.rpartition(' ')\n",
    "      mask = []\n",
    "      for j in range(len(first_alignment)):\n",
    "        for _, unprocessed_line in unprocessed_lines.items():\n",
    "          prefix, _, alignment = unprocessed_line.rpartition(' ')\n",
    "          if alignment[j] != '-':\n",
    "            mask.append(True)\n",
    "            break\n",
    "        else:  # Every row contained a hyphen - empty column.\n",
    "          mask.append(False)\n",
    "      # Add reference annotation for processing with mask.\n",
    "      unprocessed_lines[reference_annotation_i] = reference_annotation_line\n",
    "\n",
    "      if not any(mask):  # All columns were empty. Output empty lines for chunk.\n",
    "        for line_index in unprocessed_lines:\n",
    "          processed_lines[line_index] = ''\n",
    "      else:\n",
    "        for line_index, unprocessed_line in unprocessed_lines.items():\n",
    "          prefix, _, alignment = unprocessed_line.rpartition(' ')\n",
    "          masked_alignment = ''.join(itertools.compress(alignment, mask))\n",
    "          processed_lines[line_index] = f'{prefix} {masked_alignment}'\n",
    "\n",
    "      # Clear raw_alignments.\n",
    "      unprocessed_lines = {}\n",
    "    elif line.strip() and not line.startswith(('#', '//')):\n",
    "      unprocessed_lines[i] = line\n",
    "    else:\n",
    "      processed_lines[i] = line\n",
    "  return '\\n'.join((processed_lines[i] for i in range(len(processed_lines))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/sirius/PhD/software/alphafold/output/7s4h/msas/D/uniref90_hits.sto\") as f:\n",
    "    stockholm_msa = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = deduplicate_stockholm_msa(stockholm_msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = remove_empty_columns_from_stockholm_msa(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Functions for parsing various file formats.\"\"\"\n",
    "import collections\n",
    "import dataclasses\n",
    "import itertools\n",
    "import re\n",
    "import string\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Set\n",
    "\n",
    "# Internal import (7716).\n",
    "\n",
    "\n",
    "DeletionMatrix = Sequence[Sequence[int]]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Msa:\n",
    "  \"\"\"Class representing a parsed MSA file.\"\"\"\n",
    "  sequences: Sequence[str]\n",
    "  deletion_matrix: DeletionMatrix\n",
    "  descriptions: Sequence[str]\n",
    "\n",
    "  def __post_init__(self):\n",
    "    if not (len(self.sequences) ==\n",
    "            len(self.deletion_matrix) ==\n",
    "            len(self.descriptions)):\n",
    "      raise ValueError(\n",
    "          'All fields for an MSA must have the same length. '\n",
    "          f'Got {len(self.sequences)} sequences, '\n",
    "          f'{len(self.deletion_matrix)} rows in the deletion matrix and '\n",
    "          f'{len(self.descriptions)} descriptions.')\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sequences)\n",
    "\n",
    "  def truncate(self, max_seqs: int):\n",
    "    return Msa(sequences=self.sequences[:max_seqs],\n",
    "               deletion_matrix=self.deletion_matrix[:max_seqs],\n",
    "               descriptions=self.descriptions[:max_seqs])\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TemplateHit:\n",
    "  \"\"\"Class representing a template hit.\"\"\"\n",
    "  index: int\n",
    "  name: str\n",
    "  aligned_cols: int\n",
    "  sum_probs: Optional[float]\n",
    "  query: str\n",
    "  hit_sequence: str\n",
    "  indices_query: List[int]\n",
    "  indices_hit: List[int]\n",
    "\n",
    "\n",
    "def parse_fasta(fasta_string: str) -> Tuple[Sequence[str], Sequence[str]]:\n",
    "  \"\"\"Parses FASTA string and returns list of strings with amino-acid sequences.\n",
    "\n",
    "  Arguments:\n",
    "    fasta_string: The string contents of a FASTA file.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of two lists:\n",
    "    * A list of sequences.\n",
    "    * A list of sequence descriptions taken from the comment lines. In the\n",
    "      same order as the sequences.\n",
    "  \"\"\"\n",
    "  sequences = []\n",
    "  descriptions = []\n",
    "  index = -1\n",
    "  for line in fasta_string.splitlines():\n",
    "    line = line.strip()\n",
    "    if line.startswith('>'):\n",
    "      index += 1\n",
    "      descriptions.append(line[1:])  # Remove the '>' at the beginning.\n",
    "      sequences.append('')\n",
    "      continue\n",
    "    elif not line:\n",
    "      continue  # Skip blank lines.\n",
    "    sequences[index] += line\n",
    "\n",
    "  return sequences, descriptions\n",
    "\n",
    "\n",
    "def parse_stockholm(stockholm_string: str) -> Msa:\n",
    "  \"\"\"Parses sequences and deletion matrix from stockholm format alignment.\n",
    "\n",
    "  Args:\n",
    "    stockholm_string: The string contents of a stockholm file. The first\n",
    "      sequence in the file should be the query sequence.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of:\n",
    "      * A list of sequences that have been aligned to the query. These\n",
    "        might contain duplicates.\n",
    "      * The deletion matrix for the alignment as a list of lists. The element\n",
    "        at `deletion_matrix[i][j]` is the number of residues deleted from\n",
    "        the aligned sequence i at residue position j.\n",
    "      * The names of the targets matched, including the jackhmmer subsequence\n",
    "        suffix.\n",
    "  \"\"\"\n",
    "  name_to_sequence = collections.OrderedDict()\n",
    "  for line in stockholm_string.splitlines():\n",
    "    line = line.strip()\n",
    "    if not line or line.startswith(('#', '//')):\n",
    "      continue\n",
    "    name, sequence = line.split()\n",
    "    if name not in name_to_sequence:\n",
    "      name_to_sequence[name] = ''\n",
    "    name_to_sequence[name] += sequence\n",
    "\n",
    "  msa = []\n",
    "  deletion_matrix = []\n",
    "\n",
    "  query = ''\n",
    "  keep_columns = []\n",
    "  for seq_index, sequence in enumerate(name_to_sequence.values()):\n",
    "    if seq_index == 0:\n",
    "      # Gather the columns with gaps from the query\n",
    "      query = sequence\n",
    "      keep_columns = [i for i, res in enumerate(query) if res != '-']\n",
    "\n",
    "    # Remove the columns with gaps in the query from all sequences.\n",
    "    aligned_sequence = ''.join([sequence[c] for c in keep_columns])\n",
    "\n",
    "    msa.append(aligned_sequence)\n",
    "\n",
    "    # Count the number of deletions w.r.t. query.\n",
    "    deletion_vec = []\n",
    "    deletion_count = 0\n",
    "    for seq_res, query_res in zip(sequence, query):\n",
    "      if seq_res != '-' or query_res != '-':\n",
    "        if query_res == '-':\n",
    "          deletion_count += 1\n",
    "        else:\n",
    "          deletion_vec.append(deletion_count)\n",
    "          deletion_count = 0\n",
    "    deletion_matrix.append(deletion_vec)\n",
    "\n",
    "  return Msa(sequences=msa,\n",
    "             deletion_matrix=deletion_matrix,\n",
    "             descriptions=list(name_to_sequence.keys()))\n",
    "\n",
    "\n",
    "def parse_a3m(a3m_string: str) -> Msa:\n",
    "  \"\"\"Parses sequences and deletion matrix from a3m format alignment.\n",
    "\n",
    "  Args:\n",
    "    a3m_string: The string contents of a a3m file. The first sequence in the\n",
    "      file should be the query sequence.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of:\n",
    "      * A list of sequences that have been aligned to the query. These\n",
    "        might contain duplicates.\n",
    "      * The deletion matrix for the alignment as a list of lists. The element\n",
    "        at `deletion_matrix[i][j]` is the number of residues deleted from\n",
    "        the aligned sequence i at residue position j.\n",
    "      * A list of descriptions, one per sequence, from the a3m file.\n",
    "  \"\"\"\n",
    "  sequences, descriptions = parse_fasta(a3m_string)\n",
    "  deletion_matrix = []\n",
    "  for msa_sequence in sequences:\n",
    "    deletion_vec = []\n",
    "    deletion_count = 0\n",
    "    for j in msa_sequence:\n",
    "      if j.islower():\n",
    "        deletion_count += 1\n",
    "      else:\n",
    "        deletion_vec.append(deletion_count)\n",
    "        deletion_count = 0\n",
    "    deletion_matrix.append(deletion_vec)\n",
    "\n",
    "  # Make the MSA matrix out of aligned (deletion-free) sequences.\n",
    "  deletion_table = str.maketrans('', '', string.ascii_lowercase)\n",
    "  aligned_sequences = [s.translate(deletion_table) for s in sequences]\n",
    "  return Msa(sequences=aligned_sequences,\n",
    "             deletion_matrix=deletion_matrix,\n",
    "             descriptions=descriptions)\n",
    "\n",
    "\n",
    "def _convert_sto_seq_to_a3m(\n",
    "    query_non_gaps: Sequence[bool], sto_seq: str) -> Iterable[str]:\n",
    "  for is_query_res_non_gap, sequence_res in zip(query_non_gaps, sto_seq):\n",
    "    if is_query_res_non_gap:\n",
    "      yield sequence_res\n",
    "    elif sequence_res != '-':\n",
    "      yield sequence_res.lower()\n",
    "\n",
    "\n",
    "def convert_stockholm_to_a3m(stockholm_format: str,\n",
    "                             max_sequences: Optional[int] = None,\n",
    "                             remove_first_row_gaps: bool = True) -> str:\n",
    "  \"\"\"Converts MSA in Stockholm format to the A3M format.\"\"\"\n",
    "  descriptions = {}\n",
    "  sequences = {}\n",
    "  reached_max_sequences = False\n",
    "\n",
    "  for line in stockholm_format.splitlines():\n",
    "    reached_max_sequences = max_sequences and len(sequences) >= max_sequences\n",
    "    if line.strip() and not line.startswith(('#', '//')):\n",
    "      # Ignore blank lines, markup and end symbols - remainder are alignment\n",
    "      # sequence parts.\n",
    "      seqname, aligned_seq = line.split(maxsplit=1)\n",
    "      if seqname not in sequences:\n",
    "        if reached_max_sequences:\n",
    "          continue\n",
    "        sequences[seqname] = ''\n",
    "      sequences[seqname] += aligned_seq\n",
    "\n",
    "  for line in stockholm_format.splitlines():\n",
    "    if line[:4] == '#=GS':\n",
    "      # Description row - example format is:\n",
    "      # #=GS UniRef90_Q9H5Z4/4-78            DE [subseq from] cDNA: FLJ22755 ...\n",
    "      columns = line.split(maxsplit=3)\n",
    "      seqname, feature = columns[1:3]\n",
    "      value = columns[3] if len(columns) == 4 else ''\n",
    "      if feature != 'DE':\n",
    "        continue\n",
    "      if reached_max_sequences and seqname not in sequences:\n",
    "        continue\n",
    "      descriptions[seqname] = value\n",
    "      if len(descriptions) == len(sequences):\n",
    "        break\n",
    "\n",
    "  # Convert sto format to a3m line by line\n",
    "  a3m_sequences = {}\n",
    "  if remove_first_row_gaps:\n",
    "    # query_sequence is assumed to be the first sequence\n",
    "    query_sequence = next(iter(sequences.values()))\n",
    "    query_non_gaps = [res != '-' for res in query_sequence]\n",
    "  for seqname, sto_sequence in sequences.items():\n",
    "    # Dots are optional in a3m format and are commonly removed.\n",
    "    out_sequence = sto_sequence.replace('.', '')\n",
    "    if remove_first_row_gaps:\n",
    "      out_sequence = ''.join(\n",
    "          _convert_sto_seq_to_a3m(query_non_gaps, out_sequence))\n",
    "    a3m_sequences[seqname] = out_sequence\n",
    "\n",
    "  fasta_chunks = (f\">{k} {descriptions.get(k, '')}\\n{a3m_sequences[k]}\"\n",
    "                  for k in a3m_sequences)\n",
    "  return '\\n'.join(fasta_chunks) + '\\n'  # Include terminating newline.\n",
    "\n",
    "\n",
    "def _keep_line(line: str, seqnames: Set[str]) -> bool:\n",
    "  \"\"\"Function to decide which lines to keep.\"\"\"\n",
    "  if not line.strip():\n",
    "    return True\n",
    "  if line.strip() == '//':  # End tag\n",
    "    return True\n",
    "  if line.startswith('# STOCKHOLM'):  # Start tag\n",
    "    return True\n",
    "  if line.startswith('#=GC RF'):  # Reference Annotation Line\n",
    "    return True\n",
    "  if line[:4] == '#=GS':  # Description lines - keep if sequence in list.\n",
    "    _, seqname, _ = line.split(maxsplit=2)\n",
    "    return seqname in seqnames\n",
    "  elif line.startswith('#'):  # Other markup - filter out\n",
    "    return False\n",
    "  else:  # Alignment data - keep if sequence in list.\n",
    "    seqname = line.partition(' ')[0]\n",
    "    return seqname in seqnames\n",
    "\n",
    "\n",
    "def truncate_stockholm_msa(stockholm_msa_path: str, max_sequences: int) -> str:\n",
    "  \"\"\"Reads + truncates a Stockholm file while preventing excessive RAM usage.\"\"\"\n",
    "  seqnames = set()\n",
    "  filtered_lines = []\n",
    "\n",
    "  with open(stockholm_msa_path) as f:\n",
    "    for line in f:\n",
    "      if line.strip() and not line.startswith(('#', '//')):\n",
    "        # Ignore blank lines, markup and end symbols - remainder are alignment\n",
    "        # sequence parts.\n",
    "        seqname = line.partition(' ')[0]\n",
    "        seqnames.add(seqname)\n",
    "        if len(seqnames) >= max_sequences:\n",
    "          break\n",
    "\n",
    "    f.seek(0)\n",
    "    for line in f:\n",
    "      if _keep_line(line, seqnames):\n",
    "        filtered_lines.append(line)\n",
    "\n",
    "  return ''.join(filtered_lines)\n",
    "\n",
    "\n",
    "def remove_empty_columns_from_stockholm_msa(stockholm_msa: str) -> str:\n",
    "  \"\"\"Removes empty columns (dashes-only) from a Stockholm MSA.\"\"\"\n",
    "  processed_lines = {}\n",
    "  unprocessed_lines = {}\n",
    "  for i, line in enumerate(stockholm_msa.splitlines()):\n",
    "    if line.startswith('#=GC RF'):\n",
    "      reference_annotation_i = i\n",
    "      reference_annotation_line = line\n",
    "      # Reached the end of this chunk of the alignment. Process chunk.\n",
    "      _, _, first_alignment = line.rpartition(' ')\n",
    "      mask = []\n",
    "      for j in range(len(first_alignment)):\n",
    "        for _, unprocessed_line in unprocessed_lines.items():\n",
    "          prefix, _, alignment = unprocessed_line.rpartition(' ')\n",
    "          if alignment[j] != '-':\n",
    "            mask.append(True)\n",
    "            break\n",
    "        else:  # Every row contained a hyphen - empty column.\n",
    "          mask.append(False)\n",
    "      # Add reference annotation for processing with mask.\n",
    "      unprocessed_lines[reference_annotation_i] = reference_annotation_line\n",
    "\n",
    "      if not any(mask):  # All columns were empty. Output empty lines for chunk.\n",
    "        for line_index in unprocessed_lines:\n",
    "          processed_lines[line_index] = ''\n",
    "      else:\n",
    "        for line_index, unprocessed_line in unprocessed_lines.items():\n",
    "          prefix, _, alignment = unprocessed_line.rpartition(' ')\n",
    "          masked_alignment = ''.join(itertools.compress(alignment, mask))\n",
    "          processed_lines[line_index] = f'{prefix} {masked_alignment}'\n",
    "\n",
    "      # Clear raw_alignments.\n",
    "      unprocessed_lines = {}\n",
    "    elif line.strip() and not line.startswith(('#', '//')):\n",
    "      unprocessed_lines[i] = line\n",
    "    else:\n",
    "      processed_lines[i] = line\n",
    "  return '\\n'.join((processed_lines[i] for i in range(len(processed_lines))))\n",
    "\n",
    "\n",
    "def deduplicate_stockholm_msa(stockholm_msa: str) -> str:\n",
    "  \"\"\"Remove duplicate sequences (ignoring insertions wrt query).\"\"\"\n",
    "  sequence_dict = collections.defaultdict(str)\n",
    "\n",
    "  # First we must extract all sequences from the MSA.\n",
    "  for line in stockholm_msa.splitlines():\n",
    "    # Only consider the alignments - ignore reference annotation, empty lines,\n",
    "    # descriptions or markup.\n",
    "    if line.strip() and not line.startswith(('#', '//')):\n",
    "      line = line.strip()\n",
    "      seqname, alignment = line.split()\n",
    "      sequence_dict[seqname] += alignment\n",
    "\n",
    "  seen_sequences = set()\n",
    "  seqnames = set()\n",
    "  # First alignment is the query.\n",
    "  query_align = next(iter(sequence_dict.values()))\n",
    "  mask = [c != '-' for c in query_align]  # Mask is False for insertions.\n",
    "  for seqname, alignment in sequence_dict.items():\n",
    "    # Apply mask to remove all insertions from the string.\n",
    "    masked_alignment = ''.join(itertools.compress(alignment, mask))\n",
    "    if masked_alignment in seen_sequences:\n",
    "      continue\n",
    "    else:\n",
    "      seen_sequences.add(masked_alignment)\n",
    "      seqnames.add(seqname)\n",
    "\n",
    "  filtered_lines = []\n",
    "  for line in stockholm_msa.splitlines():\n",
    "    if _keep_line(line, seqnames):\n",
    "      filtered_lines.append(line)\n",
    "\n",
    "  return '\\n'.join(filtered_lines) + '\\n'\n",
    "\n",
    "\n",
    "def _get_hhr_line_regex_groups(\n",
    "    regex_pattern: str, line: str) -> Sequence[Optional[str]]:\n",
    "  match = re.match(regex_pattern, line)\n",
    "  if match is None:\n",
    "    raise RuntimeError(f'Could not parse query line {line}')\n",
    "  return match.groups()\n",
    "\n",
    "\n",
    "def _update_hhr_residue_indices_list(\n",
    "    sequence: str, start_index: int, indices_list: List[int]):\n",
    "  \"\"\"Computes the relative indices for each residue with respect to the original sequence.\"\"\"\n",
    "  counter = start_index\n",
    "  for symbol in sequence:\n",
    "    if symbol == '-':\n",
    "      indices_list.append(-1)\n",
    "    else:\n",
    "      indices_list.append(counter)\n",
    "      counter += 1\n",
    "\n",
    "\n",
    "def _parse_hhr_hit(detailed_lines: Sequence[str]) -> TemplateHit:\n",
    "  \"\"\"Parses the detailed HMM HMM comparison section for a single Hit.\n",
    "\n",
    "  This works on .hhr files generated from both HHBlits and HHSearch.\n",
    "\n",
    "  Args:\n",
    "    detailed_lines: A list of lines from a single comparison section between 2\n",
    "      sequences (which each have their own HMM's)\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with the information from that detailed comparison section\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError: If a certain line cannot be processed\n",
    "  \"\"\"\n",
    "  # Parse first 2 lines.\n",
    "  number_of_hit = int(detailed_lines[0].split()[-1])\n",
    "  name_hit = detailed_lines[1][1:]\n",
    "\n",
    "  # Parse the summary line.\n",
    "  pattern = (\n",
    "      'Probab=(.*)[\\t ]*E-value=(.*)[\\t ]*Score=(.*)[\\t ]*Aligned_cols=(.*)[\\t'\n",
    "      ' ]*Identities=(.*)%[\\t ]*Similarity=(.*)[\\t ]*Sum_probs=(.*)[\\t '\n",
    "      ']*Template_Neff=(.*)')\n",
    "  match = re.match(pattern, detailed_lines[2])\n",
    "  if match is None:\n",
    "    raise RuntimeError(\n",
    "        'Could not parse section: %s. Expected this: \\n%s to contain summary.' %\n",
    "        (detailed_lines, detailed_lines[2]))\n",
    "  (_, _, _, aligned_cols, _, _, sum_probs, _) = [float(x)\n",
    "                                                 for x in match.groups()]\n",
    "\n",
    "  # The next section reads the detailed comparisons. These are in a 'human\n",
    "  # readable' format which has a fixed length. The strategy employed is to\n",
    "  # assume that each block starts with the query sequence line, and to parse\n",
    "  # that with a regexp in order to deduce the fixed length used for that block.\n",
    "  query = ''\n",
    "  hit_sequence = ''\n",
    "  indices_query = []\n",
    "  indices_hit = []\n",
    "  length_block = None\n",
    "\n",
    "  for line in detailed_lines[3:]:\n",
    "    # Parse the query sequence line\n",
    "    if (line.startswith('Q ') and not line.startswith('Q ss_dssp') and\n",
    "        not line.startswith('Q ss_pred') and\n",
    "        not line.startswith('Q Consensus')):\n",
    "      # Thus the first 17 characters must be 'Q <query_name> ', and we can parse\n",
    "      # everything after that.\n",
    "      #              start    sequence       end       total_sequence_length\n",
    "      patt = r'[\\t ]*([0-9]*) ([A-Z-]*)[\\t ]*([0-9]*) \\([0-9]*\\)'\n",
    "      groups = _get_hhr_line_regex_groups(patt, line[17:])\n",
    "\n",
    "      # Get the length of the parsed block using the start and finish indices,\n",
    "      # and ensure it is the same as the actual block length.\n",
    "      start = int(groups[0]) - 1  # Make index zero based.\n",
    "      delta_query = groups[1]\n",
    "      end = int(groups[2])\n",
    "      num_insertions = len([x for x in delta_query if x == '-'])\n",
    "      length_block = end - start + num_insertions\n",
    "      assert length_block == len(delta_query)\n",
    "\n",
    "      # Update the query sequence and indices list.\n",
    "      query += delta_query\n",
    "      _update_hhr_residue_indices_list(delta_query, start, indices_query)\n",
    "\n",
    "    elif line.startswith('T '):\n",
    "      # Parse the hit sequence.\n",
    "      if (not line.startswith('T ss_dssp') and\n",
    "          not line.startswith('T ss_pred') and\n",
    "          not line.startswith('T Consensus')):\n",
    "        # Thus the first 17 characters must be 'T <hit_name> ', and we can\n",
    "        # parse everything after that.\n",
    "        #              start    sequence       end     total_sequence_length\n",
    "        patt = r'[\\t ]*([0-9]*) ([A-Z-]*)[\\t ]*[0-9]* \\([0-9]*\\)'\n",
    "        groups = _get_hhr_line_regex_groups(patt, line[17:])\n",
    "        start = int(groups[0]) - 1  # Make index zero based.\n",
    "        delta_hit_sequence = groups[1]\n",
    "        assert length_block == len(delta_hit_sequence)\n",
    "\n",
    "        # Update the hit sequence and indices list.\n",
    "        hit_sequence += delta_hit_sequence\n",
    "        _update_hhr_residue_indices_list(delta_hit_sequence, start, indices_hit)\n",
    "\n",
    "  return TemplateHit(\n",
    "      index=number_of_hit,\n",
    "      name=name_hit,\n",
    "      aligned_cols=int(aligned_cols),\n",
    "      sum_probs=sum_probs,\n",
    "      query=query,\n",
    "      hit_sequence=hit_sequence,\n",
    "      indices_query=indices_query,\n",
    "      indices_hit=indices_hit,\n",
    "  )\n",
    "\n",
    "\n",
    "def parse_hhr(hhr_string: str) -> Sequence[TemplateHit]:\n",
    "  \"\"\"Parses the content of an entire HHR file.\"\"\"\n",
    "  lines = hhr_string.splitlines()\n",
    "\n",
    "  # Each .hhr file starts with a results table, then has a sequence of hit\n",
    "  # \"paragraphs\", each paragraph starting with a line 'No <hit number>'. We\n",
    "  # iterate through each paragraph to parse each hit.\n",
    "\n",
    "  block_starts = [i for i, line in enumerate(lines) if line.startswith('No ')]\n",
    "\n",
    "  hits = []\n",
    "  if block_starts:\n",
    "    block_starts.append(len(lines))  # Add the end of the final block.\n",
    "    for i in range(len(block_starts) - 1):\n",
    "      hits.append(_parse_hhr_hit(lines[block_starts[i]:block_starts[i + 1]]))\n",
    "  return hits\n",
    "\n",
    "\n",
    "def parse_e_values_from_tblout(tblout: str) -> Dict[str, float]:\n",
    "  \"\"\"Parse target to e-value mapping parsed from Jackhmmer tblout string.\"\"\"\n",
    "  e_values = {'query': 0}\n",
    "  lines = [line for line in tblout.splitlines() if line[0] != '#']\n",
    "  # As per http://eddylab.org/software/hmmer/Userguide.pdf fields are\n",
    "  # space-delimited. Relevant fields are (1) target name:  and\n",
    "  # (5) E-value (full sequence) (numbering from 1).\n",
    "  for line in lines:\n",
    "    fields = line.split()\n",
    "    e_value = fields[4]\n",
    "    target_name = fields[0]\n",
    "    e_values[target_name] = float(e_value)\n",
    "  return e_values\n",
    "\n",
    "\n",
    "def _get_indices(sequence: str, start: int) -> List[int]:\n",
    "  \"\"\"Returns indices for non-gap/insert residues starting at the given index.\"\"\"\n",
    "  indices = []\n",
    "  counter = start\n",
    "  for symbol in sequence:\n",
    "    # Skip gaps but add a placeholder so that the alignment is preserved.\n",
    "    if symbol == '-':\n",
    "      indices.append(-1)\n",
    "    # Skip deleted residues, but increase the counter.\n",
    "    elif symbol.islower():\n",
    "      counter += 1\n",
    "    # Normal aligned residue. Increase the counter and append to indices.\n",
    "    else:\n",
    "      indices.append(counter)\n",
    "      counter += 1\n",
    "  return indices\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class HitMetadata:\n",
    "  pdb_id: str\n",
    "  chain: str\n",
    "  start: int\n",
    "  end: int\n",
    "  length: int\n",
    "  text: str\n",
    "\n",
    "\n",
    "def _parse_hmmsearch_description(description: str) -> HitMetadata:\n",
    "  \"\"\"Parses the hmmsearch A3M sequence description line.\"\"\"\n",
    "  # Example 1: >4pqx_A/2-217 [subseq from] mol:protein length:217  Free text\n",
    "  # Example 2: >5g3r_A/1-55 [subseq from] mol:protein length:352\n",
    "  match = re.match(\n",
    "      r'^>?([a-z0-9]+)_(\\w+)/([0-9]+)-([0-9]+).*protein length:([0-9]+) *(.*)$',\n",
    "      description.strip())\n",
    "\n",
    "  if not match:\n",
    "    raise ValueError(f'Could not parse description: \"{description}\".')\n",
    "\n",
    "  return HitMetadata(\n",
    "      pdb_id=match[1],\n",
    "      chain=match[2],\n",
    "      start=int(match[3]),\n",
    "      end=int(match[4]),\n",
    "      length=int(match[5]),\n",
    "      text=match[6])\n",
    "\n",
    "\n",
    "def parse_hmmsearch_a3m(query_sequence: str,\n",
    "                        a3m_string: str,\n",
    "                        skip_first: bool = True) -> Sequence[TemplateHit]:\n",
    "  \"\"\"Parses an a3m string produced by hmmsearch.\n",
    "\n",
    "  Args:\n",
    "    query_sequence: The query sequence.\n",
    "    a3m_string: The a3m string produced by hmmsearch.\n",
    "    skip_first: Whether to skip the first sequence in the a3m string.\n",
    "\n",
    "  Returns:\n",
    "    A sequence of `TemplateHit` results.\n",
    "  \"\"\"\n",
    "  # Zip the descriptions and MSAs together, skip the first query sequence.\n",
    "  parsed_a3m = list(zip(*parse_fasta(a3m_string)))\n",
    "  if skip_first:\n",
    "    parsed_a3m = parsed_a3m[1:]\n",
    "\n",
    "  indices_query = _get_indices(query_sequence, start=0)\n",
    "\n",
    "  hits = []\n",
    "  for i, (hit_sequence, hit_description) in enumerate(parsed_a3m, start=1):\n",
    "    if 'mol:protein' not in hit_description:\n",
    "      continue  # Skip non-protein chains.\n",
    "    metadata = _parse_hmmsearch_description(hit_description)\n",
    "    # Aligned columns are only the match states.\n",
    "    aligned_cols = sum([r.isupper() and r != '-' for r in hit_sequence])\n",
    "    indices_hit = _get_indices(hit_sequence, start=metadata.start - 1)\n",
    "\n",
    "    hit = TemplateHit(\n",
    "        index=i,\n",
    "        name=f'{metadata.pdb_id}_{metadata.chain}',\n",
    "        aligned_cols=aligned_cols,\n",
    "        sum_probs=None,\n",
    "        query=query_sequence,\n",
    "        hit_sequence=hit_sequence.upper(),\n",
    "        indices_query=indices_query,\n",
    "        indices_hit=indices_hit,\n",
    "    )\n",
    "    hits.append(hit)\n",
    "\n",
    "  return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/sirius/PhD/software/alphafold/output/MpGDH/msas/pdb_hits.hhr\",\"r\") as f:\n",
    "    hhr_string = f.read()\n",
    "y = parse_hhr(hhr_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemplateHit(index=500, name='2ZXI_D tRNA uridine 5-carboxymethylaminomethyl modification enzyme; modification, tRNA, 5-carboxymethylaminomethyl uridine, wobble; HET: FAD; 2.3A {Aquifex aeolicus}', aligned_cols=39, sum_probs=34.8, query='SSSTDTYDYVIVGGGVAGLALASRISENKDVTVAVLESGP', hit_sequence='AWVVDEFDVVVIGGGHAGIEAALAAAR-MGAKTAMFVLNA', indices_query=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], indices_hit=[21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, -1, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1].aligned_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SSSTDTYDYVIVGGGVAGLALASRISENKDVTVAVLESGP'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1].query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AWVVDEFDVVVIGGGHAGIEAALAAAR-MGAKTAMFVLNA'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1].hit_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"AWVVDEFDVVVIGGGHAGIEAALAAAR-MGAKTAMFVLNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1].indices_query.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-1].indices_hit.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2zxi D\n"
     ]
    }
   ],
   "source": [
    "y[-1].name\n",
    "\n",
    "id_match = re.match(r'[a-zA-Z\\d]{4}_[a-zA-Z0-9.]+', y[-1].name)\n",
    "if not id_match:\n",
    "    raise ValueError(f'hit.name did not start with PDBID_chain: {y[-1].name}')\n",
    "pdb_id, chain_id = id_match.group(0).split('_')\n",
    "print(pdb_id.lower(), chain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature = np.load(\"/home/sirius/PhD/software/alphafold/output/MpGDH/features.pkl\",allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(621,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature['residue_index'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template_aatype (20, 621, 22)\n",
      "template_all_atom_masks (20, 621, 37)\n",
      "template_all_atom_positions (20, 621, 37, 3)\n",
      "template_domain_names (20,)\n",
      "template_sequence (20,)\n",
      "template_sum_probs (20, 1)\n"
     ]
    }
   ],
   "source": [
    "for key in feature:\n",
    "    if key.startswith(\"template\"):\n",
    "        print(key,feature[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'-------DNLTGDIVIIGAGAAGSLLAHYLARFSNMKIILLEAGHSHFNDPVVTDPMGFFSMQNPSYSWQGAQEPNTGAYGNRPIIAHGMGFGGSTMINRLNLVVGGRTVFDDWP----VGWKYDDVKNYFRRVLVDINPVRD--------NTKASITSVALDASGEDFLLNKATGNVPNVEKTTPDAVPDYEGVSVVAFSSFYNQLSDGNYIRKYAGNTYLNRNYKFSGLRVVSDAVVDRIIFK-G-----NRAVGVNYIDRE--GIMHYVKVNKEVVVTSGAFYTPTILQRSGIGDFTYLSSIGVKLVYNNPLVGTGLKNHYSPVTITRVHGEPSEVSRF------L----SNMAA-NPTNMGFKGLAELGFHR-----LD----PN-----KPA----NAN---T----------------------VTYRKYQLMMTAGVGILSGLSPSSNNLFTLIADDIRFAPEGYIKIGTPIPRDVPKIFFNTFVTPADQQWLISGYDIIYQTLMSMNARNDLIWHYFVPTLVGDTLSKLSYYPRVGAHLDSHQGCSCSIGR-----TVDSNLKVIGTQNVRVADLSAAAFPPGGNTWATASMIGARAVDLILGFPYLRDL---'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature['template_sequence'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_obsolete(obsolete_file_path: str) :\n",
    "  \"\"\"Parses the data file from PDB that lists which pdb_ids are obsolete.\"\"\"\n",
    "  with open(obsolete_file_path) as f:\n",
    "    result = {}\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      # Format:    Date      From     To\n",
    "      # 'OBSLTE    06-NOV-19 6G9Y'                - Removed, rare\n",
    "      # 'OBSLTE    31-JUL-94 116L     216L'       - Replaced, common\n",
    "      # 'OBSLTE    26-SEP-06 2H33     2JM5 2OWI'  - Replaced by multiple, rare\n",
    "      if line.startswith('OBSLTE'):\n",
    "        if len(line) > 30:\n",
    "          # Replaced by at least one structure.\n",
    "          from_id = line[20:24].lower()\n",
    "          to_id = line[29:33].lower()\n",
    "          result[from_id] = to_id\n",
    "        elif len(line) == 24:\n",
    "          # Removed.\n",
    "          from_id = line[20:24].lower()\n",
    "          result[from_id] = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsolete_pdbs_path = \"/home/sirius/PhD/database/pdb_mmcif/obsolete.dat\"\n",
    "_obsolete_pdbs = _parse_obsolete(obsolete_pdbs_path)\n",
    "# if this pdb was removed, don't use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'116l': '216l',\n",
       " '125d': '1aw6',\n",
       " '14ps': '1qjb',\n",
       " '151c': '251c',\n",
       " '156b': '256b',\n",
       " '179l': None,\n",
       " '1a0v': '1y46',\n",
       " '1a0w': '1y4f',\n",
       " '1a0x': '1y4g',\n",
       " '1a0y': '1y4p',\n",
       " '1a10': None,\n",
       " '1a1y': None,\n",
       " '1a2k': '5bxq',\n",
       " '1a6o': '1lr4',\n",
       " '1a9d': '1bz9',\n",
       " '1a9k': '1b0g',\n",
       " '1aa8': '1ve9',\n",
       " '1aak': '2aak',\n",
       " '1abh': '2abh',\n",
       " '1abk': '2abk',\n",
       " '1abm': '1n0j',\n",
       " '1abp': '1abe',\n",
       " '1abx': '2abx',\n",
       " '1ace': '2ace',\n",
       " '1ack': '2ack',\n",
       " '1act': '2act',\n",
       " '1ada': '2ada',\n",
       " '1adh': None,\n",
       " '1adk': '2adk',\n",
       " '1adm': '2adm',\n",
       " '1adt': '2wb0',\n",
       " '1afg': '2afg',\n",
       " '1afn': '2afn',\n",
       " '1ak3': '2ak3',\n",
       " '1alo': '1hlr',\n",
       " '1alp': '2alp',\n",
       " '1alr': '2alr',\n",
       " '1am3': '1a8o',\n",
       " '1amg': '2amg',\n",
       " '1amv': '2amv',\n",
       " '1anh': '2anh',\n",
       " '1ape': '2ape',\n",
       " '1apg': '3rtj',\n",
       " '1app': '2app',\n",
       " '1apr': '2apr',\n",
       " '1ar3': '1b0j',\n",
       " '1ara': '1am0',\n",
       " '1arn': '1dyz',\n",
       " '1as9': '1b0k',\n",
       " '1asi': '2asi',\n",
       " '1at7': '1bax',\n",
       " '1atc': '3atc',\n",
       " '1atq': '1b0m',\n",
       " '1aub': '1e0e',\n",
       " '1axf': '1y85',\n",
       " '1ayh': '2ayh',\n",
       " '1ayq': '2ayq',\n",
       " '1az9': '1wl9',\n",
       " '1aza': '2aza',\n",
       " '1b1w': '1cmi',\n",
       " '1b2n': '1g72',\n",
       " '1b3m': '1l9f',\n",
       " '1b5c': '2b5c',\n",
       " '1b6n': '1z1h',\n",
       " '1b6o': '1z1r',\n",
       " '1b7c': '1c75',\n",
       " '1b8b': '1h7a',\n",
       " '1b91': '1n72',\n",
       " '1baa': '2baa',\n",
       " '1baq': None,\n",
       " '1bcl': '2bcl',\n",
       " '1bdp': '1xwl',\n",
       " '1bef': None,\n",
       " '1ber': '1j59',\n",
       " '1bfl': '1cnq',\n",
       " '1bgh': '1vqb',\n",
       " '1bgl': '4v40',\n",
       " '1bgm': '4v40',\n",
       " '1bgr': '1b1x',\n",
       " '1bj0': '4v2f',\n",
       " '1bjl': '3bjl',\n",
       " '1bjy': '4v2g',\n",
       " '1bkq': '1f3c',\n",
       " '1bl2': '1qrk',\n",
       " '1blm': '3blm',\n",
       " '1blw': '1cn4',\n",
       " '1bme': '1bvt',\n",
       " '1bmi': '2bmi',\n",
       " '1bmy': '1ig6',\n",
       " '1bn2': '2bn2',\n",
       " '1bnh': '2bnh',\n",
       " '1bov': '2xsc',\n",
       " '1bqv': '2jv3',\n",
       " '1br7': '2e2c',\n",
       " '1buk': '2dik',\n",
       " '1bur': '1ir1',\n",
       " '1bv0': '1b27',\n",
       " '1bv5': '1drq',\n",
       " '1bv6': '1drm',\n",
       " '1bvs': '7oa5',\n",
       " '1bxj': '2v1v',\n",
       " '1byk': '4xxh',\n",
       " '1byt': '1no3',\n",
       " '1c00': '1c7i',\n",
       " '1c0d': '1vrx',\n",
       " '1c0h': '1c40',\n",
       " '1c13': '1iav',\n",
       " '1c2c': '2c2c',\n",
       " '1c4j': '1exy',\n",
       " '1c59': '1dv4',\n",
       " '1c9r': '1j5o',\n",
       " '1cab': '2cab',\n",
       " '1cac': '1ca2',\n",
       " '1cae': '2cae',\n",
       " '1caf': '2cah',\n",
       " '1cat': '3cat',\n",
       " '1cbp': '2cbp',\n",
       " '1cbz': '1eyy',\n",
       " '1cco': '2cco',\n",
       " '1ccy': '2ccy',\n",
       " '1cd4': '1cdh',\n",
       " '1cdv': '2cdv',\n",
       " '1cf6': '1huo',\n",
       " '1cfo': '1new',\n",
       " '1ch6': '1hwx',\n",
       " '1chb': '2chb',\n",
       " '1chr': None,\n",
       " '1ci2': '2ci2',\n",
       " '1cjn': '2cjn',\n",
       " '1cjo': '2cjo',\n",
       " '1ck5': '1t0k',\n",
       " '1ck8': '1t0k',\n",
       " '1ck9': '1cn7',\n",
       " '1cln': '3cln',\n",
       " '1cmw': None,\n",
       " '1cn8': '1t0k',\n",
       " '1cn9': '1t0k',\n",
       " '1cnd': '2cnd',\n",
       " '1co3': '1ez0',\n",
       " '1coq': None,\n",
       " '1cox': '3cox',\n",
       " '1cpa': '3cpa',\n",
       " '1cpk': '2cpk',\n",
       " '1cpp': '2cpp',\n",
       " '1cpv': '5cpv',\n",
       " '1cqb': '1ekh',\n",
       " '1cro': '5cro',\n",
       " '1ct3': '4caa',\n",
       " '1ct7': '1e8r',\n",
       " '1cth': '2cth',\n",
       " '1cum': '1bkj',\n",
       " '1cx0': '4pr6',\n",
       " '1cxg': '2cxg',\n",
       " '1cxs': '1eu1',\n",
       " '1cxt': '1eu1',\n",
       " '1cy3': '2cy3',\n",
       " '1cyh': '2cyh',\n",
       " '1cyp': '2cyp',\n",
       " '1cyt': '3cyt',\n",
       " '1d4g': '1k0u',\n",
       " '1d4q': '1c8p',\n",
       " '1d55': '2d55',\n",
       " '1d5k': '1gh8',\n",
       " '1d8r': '1vj3',\n",
       " '1d9b': '1hn9',\n",
       " '1d9t': '1ne7',\n",
       " '1db0': '1h1v',\n",
       " '1db7': '1o3q',\n",
       " '1db8': '1o3r',\n",
       " '1db9': '1o3s',\n",
       " '1dbc': '1o3t',\n",
       " '1ddc': '1h21',\n",
       " '1ddq': '1hqm',\n",
       " '1der': '1kp8',\n",
       " '1df9': None,\n",
       " '1dfr': '3dfr',\n",
       " '1dga': '1nm1',\n",
       " '1dgt': '1v9p',\n",
       " '1dgy': '1lii',\n",
       " '1dh0': '1lik',\n",
       " '1dh1': '1lij',\n",
       " '1dh2': '1lio',\n",
       " '1dhb': '2dhb',\n",
       " '1dhx': '1p2z',\n",
       " '1dij': '2dij',\n",
       " '1dj4': None,\n",
       " '1dkv': '1kfu',\n",
       " '1dlx': '1ly7',\n",
       " '1dn1': '3c98',\n",
       " '1doo': '1e94',\n",
       " '1dri': '2dri',\n",
       " '1drq': '1lt0',\n",
       " '1dsh': '1y0d',\n",
       " '1dt8': None,\n",
       " '1dtr': '2dtr',\n",
       " '1du7': '2x9d',\n",
       " '1dw7': '1gh9',\n",
       " '1dx4': '6xyy',\n",
       " '1dyv': '1un2',\n",
       " '1dzs': '2iz9',\n",
       " '1e01': '1e0g',\n",
       " '1e0i': '1gkb',\n",
       " '1e13': None,\n",
       " '1e1b': '1e5u',\n",
       " '1e2c': '2e2c',\n",
       " '1e3n': '1ogx',\n",
       " '1e53': '1z60',\n",
       " '1e6t': '2bu1',\n",
       " '1e7t': '1gk6',\n",
       " '1e7x': '2c4q',\n",
       " '1eck': '2eck',\n",
       " '1eee': '1flg',\n",
       " '1eh0': '1eup',\n",
       " '1eim': '1kw9',\n",
       " '1eip': '2eip',\n",
       " '1ejk': None,\n",
       " '1ekc': '1g1x',\n",
       " '1ekt': '1z0r',\n",
       " '1en0': '1i3q',\n",
       " '1end': '2end',\n",
       " '1eng': '2eng',\n",
       " '1enl': '2enl',\n",
       " '1eor': '1f9l',\n",
       " '1eqo': '1q0n',\n",
       " '1eqs': '1j9m',\n",
       " '1erk': '5umo',\n",
       " '1erl': '2erl',\n",
       " '1etc': '1r36',\n",
       " '1etd': '1r36',\n",
       " '1eul': '1su4',\n",
       " '1ewg': '3b8d',\n",
       " '1f0a': '1ghh',\n",
       " '1f19': '2f19',\n",
       " '1f1y': '1q0o',\n",
       " '1f3i': '1muh',\n",
       " '1f49': '4v41',\n",
       " '1f5d': '1j4x',\n",
       " '1f5i': '1k4z',\n",
       " '1f83': '3g94',\n",
       " '1f9y': '3h4a',\n",
       " '1fab': '2fab',\n",
       " '1fb3': '1sm4',\n",
       " '1fb4': '2fb4',\n",
       " '1fbj': '2fbj',\n",
       " '1fd1': '2fd1',\n",
       " '1fdc': '6fdr',\n",
       " '1fdx': '1dur',\n",
       " '1fe7': '1kpm',\n",
       " '1fh6': '1ohg',\n",
       " '1fhp': '1qqp',\n",
       " '1fjf': '1j5e',\n",
       " '1fku': '1jfw',\n",
       " '1fmp': '3rti',\n",
       " '1fnr': '1fnb',\n",
       " '1fpx': '6cig',\n",
       " '1fqh': None,\n",
       " '1frc': '2frc',\n",
       " '1fru': '3fru',\n",
       " '1fum': '1l0v',\n",
       " '1fvz': '1t27',\n",
       " '1fxb': '2fxb',\n",
       " '1fxc': '3fxc',\n",
       " '1fxm': '1k6a',\n",
       " '1fxn': '3fxn',\n",
       " '1fzn': None,\n",
       " '1fzp': None,\n",
       " '1g34': '1oyq',\n",
       " '1g40': None,\n",
       " '1g44': None,\n",
       " '1g56': '1vm1',\n",
       " '1g5o': '1gjh',\n",
       " '1gap': '3gap',\n",
       " '1gbp': '3gbp',\n",
       " '1gch': '2gch',\n",
       " '1gcr': '4gcr',\n",
       " '1gcx': '1wns',\n",
       " '1gdg': '1kw8',\n",
       " '1gdm': '2gdm',\n",
       " '1gdo': '1xff',\n",
       " '1gds': '1gwp',\n",
       " '1gdy': '1gwp',\n",
       " '1gdz': '1gwp',\n",
       " '1geo': '1aop',\n",
       " '1gep': '2gep',\n",
       " '1gga': '2x0n',\n",
       " '1ggs': '1fi5',\n",
       " '1gho': '4v41',\n",
       " '1gix': '4v42',\n",
       " '1giy': '4v42',\n",
       " '1gkv': '2iz8',\n",
       " '1gkw': '2izn',\n",
       " '1glt': '2glt',\n",
       " '1gly': '3gly',\n",
       " '1gm3': '1h3p',\n",
       " '1gma': '1alz',\n",
       " '1gmf': '2gmf',\n",
       " '1gms': '1xfg',\n",
       " '1gn5': '2gn5',\n",
       " '1gpg': '1aio',\n",
       " '1gq3': '5vmq',\n",
       " '1gr4': None,\n",
       " '1gr6': '2c7e',\n",
       " '1gr8': '1ock',\n",
       " '1gr9': '1ocl',\n",
       " '1grk': '1ocm',\n",
       " '1grs': '2grs',\n",
       " '1gry': '1gzu',\n",
       " '1gsr': '2gsr',\n",
       " '1gst': '6gst',\n",
       " '1gt2': '2x0r',\n",
       " '1gtx': '1ohv',\n",
       " '1gva': '2gva',\n",
       " '1gvb': '2gvb',\n",
       " '1gw5': '2vgl',\n",
       " '1gyi': '2gyi',\n",
       " '1h06': '1v1k',\n",
       " '1h0e': '1un4',\n",
       " '1h0f': '1un3',\n",
       " '1h0u': None,\n",
       " '1h1g': None,\n",
       " '1h40': '1uzc',\n",
       " '1h6w': '5lye',\n",
       " '1h6z': '2x0s',\n",
       " '1h77': '1hk8',\n",
       " '1h8j': '2c51',\n",
       " '1hbl': '1lh1',\n",
       " '1hc2': '1hc1',\n",
       " '1hc3': '1hc1',\n",
       " '1hc4': '1hc1',\n",
       " '1hc5': '1hc1',\n",
       " '1hc6': '1hc1',\n",
       " '1hdw': '2c4z',\n",
       " '1he0': '2c4y',\n",
       " '1he6': '2c50',\n",
       " '1hf5': '1h11',\n",
       " '1hf7': '1h2j',\n",
       " '1hft': '2hft',\n",
       " '1hfv': '2j5x',\n",
       " '1hg6': None,\n",
       " '1hhb': '2hhb',\n",
       " '1hid': '2hid',\n",
       " '1hiu': '2hiu',\n",
       " '1hke': '1uuz',\n",
       " '1hkp': None,\n",
       " '1hkz': None,\n",
       " '1hl0': None,\n",
       " '1hl1': None,\n",
       " '1hlr': '1vlb',\n",
       " '1hmg': '2hmg',\n",
       " '1hmi': '2hmi',\n",
       " '1hmm': '1hmq',\n",
       " '1hmn': '1hmm',\n",
       " '1hmq': '2hmq',\n",
       " '1hmx': '2hmx',\n",
       " '1hmz': '2hmz',\n",
       " '1hn7': '1jbd',\n",
       " '1hp6': '1m5k',\n",
       " '1hr5': '1jm0',\n",
       " '1hrw': '1le0',\n",
       " '1hrx': '1le1',\n",
       " '1hs0': '1le3',\n",
       " '1hsc': '2hsc',\n",
       " '1hsd': '2hsd',\n",
       " '1hsp': '2hsp',\n",
       " '1hu4': '1n5d',\n",
       " '1hvm': '2hvm',\n",
       " '1hvt': '2hvt',\n",
       " '1hw0': '1ih5',\n",
       " '1hwa': '1e8l',\n",
       " '1hwx': '3mw9',\n",
       " '1hwz': '6dhd',\n",
       " '1hyx': '2dqt',\n",
       " '1hyy': '2dqu',\n",
       " '1hzq': '1j5l',\n",
       " '1hzr': '1j5m',\n",
       " '1i03': '1jgm',\n",
       " '1i0w': '1j5c',\n",
       " '1i0y': '1j5d',\n",
       " '1i99': '1k9o',\n",
       " '1ib3': '1m8z',\n",
       " '1icb': '2icb',\n",
       " '1ife': '2ife',\n",
       " '1ig2': '2ig2',\n",
       " '1iis': '1t83',\n",
       " '1iix': '1t89',\n",
       " '1ij7': '1m9u',\n",
       " '1ikb': '1ai2',\n",
       " '1ikh': '1koi',\n",
       " '1ikz': '1m3g',\n",
       " '1imy': '1gxh',\n",
       " '1imz': '1gxg',\n",
       " '1inm': '1e1b',\n",
       " '1ins': '4ins',\n",
       " '1iq2': '1l3x',\n",
       " '1isd': '2isd',\n",
       " '1iw5': '1v9j',\n",
       " '1iya': '1j3g',\n",
       " '1j0l': '1wot',\n",
       " '1j2h': '1o3x',\n",
       " '1j2i': '1o3y',\n",
       " '1j2s': '1j3g',\n",
       " '1j3v': '2cvz',\n",
       " '1j4z': '4v43',\n",
       " '1j5q': '5tiq',\n",
       " '1j5r': '1o2d',\n",
       " '1j5v': '2afb',\n",
       " '1j6n': '1o58',\n",
       " '1j8w': '1nf5',\n",
       " '1j8x': '1nkh',\n",
       " '1j92': '1nqi',\n",
       " '1j94': '1nhe',\n",
       " '1jag': '2ocp',\n",
       " '1jcw': '2jcw',\n",
       " '1jel': '2jel',\n",
       " '1jfe': '1m1j',\n",
       " '1jfo': '1b1u',\n",
       " '1jfy': '1jr7',\n",
       " '1jgf': '1ls5',\n",
       " '1jh2': '1p3h',\n",
       " '1jjm': '2b8r',\n",
       " '1jjn': '2b8s',\n",
       " '1jjq': None,\n",
       " '1jjy': '1su8',\n",
       " '1jmr': '1mkm',\n",
       " '1jn8': '1nmm',\n",
       " '1jna': '1nwg',\n",
       " '1jnc': '1nt7',\n",
       " '1joz': '1llh',\n",
       " '1jsb': '1ryj',\n",
       " '1jsk': '4pp8',\n",
       " '1jsq': None,\n",
       " '1jtr': '1mwa',\n",
       " '1jvf': '2pd3',\n",
       " '1jw7': '2pd4',\n",
       " '1jxr': '2jxr',\n",
       " '1jyg': '1ryk',\n",
       " '1jyp': '2pjf',\n",
       " '1jyy': '4v44',\n",
       " '1jyz': '4v44',\n",
       " '1jz0': '4v45',\n",
       " '1jz1': '4v45',\n",
       " '1k00': '1j5a',\n",
       " '1k0q': '1kti',\n",
       " '1k16': '1j5b',\n",
       " '1k31': '1ksm',\n",
       " '1k3p': '4g6b',\n",
       " '1k71': '3l1q',\n",
       " '1k87': '4o8a',\n",
       " '1k8e': '1o17',\n",
       " '1ka3': '1tkv',\n",
       " '1kau': '1fwj',\n",
       " '1kc0': '1n2s',\n",
       " '1kc9': '1kpj',\n",
       " '1kdv': '6m8w',\n",
       " '1kdy': '6m8y',\n",
       " '1kdz': '6m9f',\n",
       " '1ke1': '6m9c',\n",
       " '1ke2': '6m9d',\n",
       " '1ked': '1z7j',\n",
       " '1ki5': '2ki5',\n",
       " '1kin': '1kim',\n",
       " '1knn': '1mc9',\n",
       " '1kom': '1t23',\n",
       " '1kpj': '1lnr',\n",
       " '1kpo': '4v43',\n",
       " '1krk': '1n2y',\n",
       " '1ksa': '3eni',\n",
       " '1kst': '1n4y',\n",
       " '1kuo': '2izm',\n",
       " '1kus': '1lkc',\n",
       " '1kwn': '6coa',\n",
       " '1kyb': '1o0r',\n",
       " '1kye': '3liw',\n",
       " '1kyg': '1yep',\n",
       " '1l1a': '1mur',\n",
       " '1l1b': '1vl3',\n",
       " '1l2r': '1nth',\n",
       " '1l6k': '2ou1',\n",
       " '1l6l': None,\n",
       " '1l7s': '1vpo',\n",
       " '1l7u': '1m40',\n",
       " '1l7w': '1oqm',\n",
       " '1l8u': '2b0q',\n",
       " '1l9f': '2gb0',\n",
       " '1lbd': '6hn6',\n",
       " '1ldx': '2ldx',\n",
       " '1lef': '2lef',\n",
       " '1lff': '1sz8',\n",
       " '1lfj': '1s6b',\n",
       " '1lgg': '1t0u',\n",
       " '1lh4': '1gdj',\n",
       " '1lhb': '2lhb',\n",
       " '1liu': '2vgb',\n",
       " '1liw': '2vgf',\n",
       " '1lix': '2vgi',\n",
       " '1liy': '2vgg',\n",
       " '1liz': '1sr3',\n",
       " '1lku': '1o1w',\n",
       " '1lnp': '2jmn',\n",
       " '1lnr': '1nkw',\n",
       " '1lpr': '1gbk',\n",
       " '1lpt': '1gh1',\n",
       " '1lqr': '1tyk',\n",
       " '1lrd': '1lmb',\n",
       " '1ls0': '1o0v',\n",
       " '1lus': '1n0n',\n",
       " '1lvd': '1lve',\n",
       " '1lw8': '1pc1',\n",
       " '1lym': '5lym',\n",
       " '1lz3': '135l',\n",
       " '1lzm': '2lzm',\n",
       " '1m2y': '1rwd',\n",
       " '1m3y': '5tip',\n",
       " '1m4o': '1zrr',\n",
       " '1m50': '3eni',\n",
       " '1m5j': '3gxy',\n",
       " '1m5m': '3gxz',\n",
       " '1m80': '3m10',\n",
       " '1m98': '5ui2',\n",
       " '1mad': '2mad',\n",
       " '1mb5': '2mb5',\n",
       " '1mbp': '2mbp',\n",
       " '1mbr': '2mbr',\n",
       " '1mbw': '2mbw',\n",
       " '1mc6': '1mr0',\n",
       " '1mcg': '2mcg',\n",
       " '1mcu': '1f2s',\n",
       " '1md1': '1n5b',\n",
       " '1mdh': '2mdh',\n",
       " '1mef': '3mef',\n",
       " '1mev': '2mev',\n",
       " '1mfh': '2akq',\n",
       " '1mfo': '2cc1',\n",
       " '1mhb': '2mhb',\n",
       " '1mhr': '2mhr',\n",
       " '1min': '2min',\n",
       " '1mle': '1muc',\n",
       " '1mlt': '2mlt',\n",
       " '1mon': '3mon',\n",
       " '1ms2': '2ms2',\n",
       " '1msl': '2oar',\n",
       " '1mt2': '2mt2',\n",
       " '1mur': '4dm0',\n",
       " '1mv7': '1sm5',\n",
       " '1mwf': '1pvn',\n",
       " '1mwv': '5l05',\n",
       " '1mwx': '1vqq',\n",
       " '1mxm': '2oau',\n",
       " '1mys': '2mys',\n",
       " '1n16': '1vrp',\n",
       " '1n1w': '1o1h',\n",
       " '1n2u': '1q54',\n",
       " '1n3v': '1nxn',\n",
       " '1n4t': '2ilx',\n",
       " '1n6s': '1vro',\n",
       " '1n6w': '1pxh',\n",
       " '1n8l': '2png',\n",
       " '1n98': '2q33',\n",
       " '1n9d': None,\n",
       " '1nbg': '1p4m',\n",
       " '1nbt': '2nbt',\n",
       " '1ncm': '2ncm',\n",
       " '1nef': '2nef',\n",
       " '1neo': '2neo',\n",
       " '1ngv': '1yov',\n",
       " '1nhd': '1vrw',\n",
       " '1nj7': '1t50',\n",
       " '1nle': '1o3z',\n",
       " '1nng': '1yli',\n",
       " '1nnm': '1pg4',\n",
       " '1nnn': '1pg3',\n",
       " '1no0': '1os0',\n",
       " '1nrc': '1oia',\n",
       " '1nrd': '2nrd',\n",
       " '1nrh': '1u8x',\n",
       " '1nt7': '1o23',\n",
       " '1num': '1aud',\n",
       " '1nvh': '1qxf',\n",
       " '1nwj': '1q53',\n",
       " '1nyz': '1sy4',\n",
       " '1nzh': '1y21',\n",
       " '1nzt': '1xxe',\n",
       " '1o09': '1p9k',\n",
       " '1o0g': '1qwq',\n",
       " '1o0i': '1sc0',\n",
       " '1o0u': '2b8n',\n",
       " '1o0y': '3r12',\n",
       " '1o0z': '1o53',\n",
       " '1o14': '2ajr',\n",
       " '1o2c': '1o51',\n",
       " '1o5n': '1vj2',\n",
       " '1o5y': '1vjl',\n",
       " '1o6n': '2bp4',\n",
       " '1o7r': '1vzt',\n",
       " '1oam': '1oke',\n",
       " '1oeq': '2byw',\n",
       " '1oer': '2byx',\n",
       " '1of7': '2vle',\n",
       " '1og8': '2buh',\n",
       " '1og9': '2bui',\n",
       " '1ogr': None,\n",
       " '1ojb': '2xfu',\n",
       " '1ojs': '2x0i',\n",
       " '1oju': '2x0j',\n",
       " '1oku': '2c5v',\n",
       " '1olb': '2olb',\n",
       " '1om3': '6n32',\n",
       " '1omf': '2omf',\n",
       " '1op3': '6n35',\n",
       " '1op5': '6n2x',\n",
       " '1oqz': '3s8r',\n",
       " '1ora': '2ora',\n",
       " '1otz': '4v46',\n",
       " '1oym': '1q9m',\n",
       " '1oz4': '3cf1',\n",
       " '1p07': '2p07',\n",
       " '1p08': '1gbm',\n",
       " '1p0t': '4v46',\n",
       " '1p38': '5uoj',\n",
       " '1p41': '1o57',\n",
       " '1p67': '1rgj',\n",
       " '1p6g': '4v47',\n",
       " '1p6z': '3ssw',\n",
       " '1p85': '4v47',\n",
       " '1p86': '4v48',\n",
       " '1p87': '4v48',\n",
       " '1p9v': '1sdj',\n",
       " '1pab': '2pab',\n",
       " '1pap': '8pap',\n",
       " '1paw': '2paw',\n",
       " '1pb6': '3loc',\n",
       " '1pc1': '1q4v',\n",
       " '1pcd': '2pcd',\n",
       " '1pcy': '1plc',\n",
       " '1pds': '1ti7',\n",
       " '1pec': '2pec',\n",
       " '1pel': '2pel',\n",
       " '1pep': '4pep',\n",
       " '1pf0': '1vp6',\n",
       " '1pf4': None,\n",
       " '1pgd': '2pgd',\n",
       " '1pgh': '2pgh',\n",
       " '1pgk': '3pgk',\n",
       " '1pgm': '3pgm',\n",
       " '1phy': '2phy',\n",
       " '1plz': '1x82',\n",
       " '1pm0': '1ryr',\n",
       " '1pm8': '1rys',\n",
       " '1pmq': '4z9l',\n",
       " '1pmz': '2opo',\n",
       " '1pn1': '1s0p',\n",
       " '1pns': '4v49',\n",
       " '1pnu': '4v49',\n",
       " '1pnx': '4v4a',\n",
       " '1pny': '4v4a',\n",
       " '1pps': '2pps',\n",
       " '1pri': '2pri',\n",
       " '1prj': '2prj',\n",
       " '1ps4': '1q2u',\n",
       " '1psg': '3psg',\n",
       " '1ptb': '2ptb',\n",
       " '1ptc': '2ptc',\n",
       " '1pte': '3pte',\n",
       " '1ptn': '2ptn',\n",
       " '1ptp': '2ptp',\n",
       " '1pxf': '3ers',\n",
       " '1q1d': '1qzr',\n",
       " '1q26': None,\n",
       " '1q28': None,\n",
       " '1q4f': '1vrz',\n",
       " '1q4m': '2f2g',\n",
       " '1q9q': '3sy0',\n",
       " '1q9r': '3t65',\n",
       " '1q9t': '3t77',\n",
       " '1q9v': '3t4y',\n",
       " '1qa8': '1gfw',\n",
       " '1qdx': '1es1',\n",
       " '1qe8': '1c7j',\n",
       " '1qj2': '1n5w',\n",
       " '1qla': '2bs2',\n",
       " '1qmk': '1qnv',\n",
       " '1qmm': '1e8x',\n",
       " '1qo9': '6xys',\n",
       " '1qoc': '1j7a',\n",
       " '1qod': '1j7b',\n",
       " '1qoe': '1j7c',\n",
       " '1qon': '6xyu',\n",
       " '1qq8': '1n45',\n",
       " '1qtl': '1c5f',\n",
       " '1qu8': '1hyk',\n",
       " '1qvh': '2a45',\n",
       " '1qy5': '6d28',\n",
       " '1qyh': '1u2o',\n",
       " '1qzo': '2dsz',\n",
       " '1r04': '2r04',\n",
       " '1r06': '2r06',\n",
       " '1r07': '2r07',\n",
       " '1r0t': '1z7k',\n",
       " '1r1e': '1eri',\n",
       " '1r2v': '2dpe',\n",
       " '1r3p': '1vjm',\n",
       " '1r5r': '3bjh',\n",
       " '1r72': '1xcb',\n",
       " '1r7k': '1z9w',\n",
       " '1r83': '2cvr',\n",
       " '1r8r': '1s9d',\n",
       " '1r9e': '4mtj',\n",
       " '1r9r': '1sfo',\n",
       " '1rb1': '3k7z',\n",
       " '1rel': '2rel',\n",
       " '1rfm': '2x06',\n",
       " '1rfw': '1szx',\n",
       " '1rhe': '2rhe',\n",
       " '1rhv': '2rhv',\n",
       " '1rid': None,\n",
       " '1rig': '2rig',\n",
       " '1rko': '1zkn',\n",
       " '1rle': None,\n",
       " '1rll': '1vjj',\n",
       " '1rm2': '2rm2',\n",
       " '1rm7': '2b8j',\n",
       " '1rmi': '1wu3',\n",
       " '1rmw': '2b82',\n",
       " '1rn3': '3rn3',\n",
       " '1rn5': '1y6h',\n",
       " '1rns': '2rns',\n",
       " '1rox': '2rox',\n",
       " '1roy': '2roy',\n",
       " '1rr1': '2rr1',\n",
       " '1rr5': '1u68',\n",
       " '1rrt': '1vrl',\n",
       " '1rs1': '2rs1',\n",
       " '1rs3': '2rs3',\n",
       " '1rs5': '2rs5',\n",
       " '1rsl': '2rsl',\n",
       " '1ru8': '3rjz',\n",
       " '1rux': '1p30',\n",
       " '1rv2': '2p5o',\n",
       " '1rw3': '4mh8',\n",
       " '1rw6': '3nyl',\n",
       " '1rw7': '4qyx',\n",
       " '1s0k': '2ah2',\n",
       " '1s1h': '4v4b',\n",
       " '1s1i': '4v4b',\n",
       " '1s65': '2idw',\n",
       " '1s6g': '2ien',\n",
       " '1s6s': '2ieo',\n",
       " '1s6t': '1vkj',\n",
       " '1s71': '1sgv',\n",
       " '1s7b': None,\n",
       " '1sag': '1sae',\n",
       " '1sah': '1saf',\n",
       " '1sai': '1sae',\n",
       " '1saj': '1saf',\n",
       " '1sba': '2sba',\n",
       " '1sbv': '2sbv',\n",
       " '1scp': '2scp',\n",
       " '1sct': '4hrr',\n",
       " '1sdh': '3sdh',\n",
       " '1se1': '1vrs',\n",
       " '1sec': '2sec',\n",
       " '1seo': '2zal',\n",
       " '1sga': '2sga',\n",
       " '1sgb': '2sgb',\n",
       " '1sgx': None,\n",
       " '1she': '2pk8',\n",
       " '1sia': '1kyj',\n",
       " '1sic': '2sic',\n",
       " '1sil': '2sil',\n",
       " '1sim': '2sim',\n",
       " '1skc': '2skc',\n",
       " '1skd': '2skd',\n",
       " '1ske': '2ske',\n",
       " '1sn3': '2sn3',\n",
       " '1sni': '2sni',\n",
       " '1sns': '2sns',\n",
       " '1snw': '2snw',\n",
       " '1sob': '2sob',\n",
       " '1sod': '2sod',\n",
       " '1spz': '2spz',\n",
       " '1srt': '2srt',\n",
       " '1ssi': '2ssi',\n",
       " '1sss': '1wb8',\n",
       " '1stt': '2stt',\n",
       " '1stw': '2stw',\n",
       " '1sv7': '1t0g',\n",
       " '1sv8': '2o9o',\n",
       " '1t2g': '2a4j',\n",
       " '1t42': '1u6b',\n",
       " '1t4h': '3fpq',\n",
       " '1taa': '2taa',\n",
       " '1tax': '1gok',\n",
       " '1tbm': '2hd1',\n",
       " '1tbs': '2tbs',\n",
       " '1tci': '2tci',\n",
       " '1tct': '2tct',\n",
       " '1tdm': '2tdm',\n",
       " '1tdx': '2tdx',\n",
       " '1te8': '1xr6',\n",
       " '1te9': '1xr7',\n",
       " '1tea': '1vpd',\n",
       " '1teb': '1xr5',\n",
       " '1teo': '1vq2',\n",
       " '1tep': '2tep',\n",
       " '1tga': '2tga',\n",
       " '1tgp': '2tgp',\n",
       " '1tgq': '2b95',\n",
       " '1ti0': '2dv8',\n",
       " '1ti2': '4v4c',\n",
       " '1ti4': '4v4d',\n",
       " '1ti6': '4v4e',\n",
       " '1ti9': '1xs9',\n",
       " '1tix': '1gom',\n",
       " '1tjq': '3g8f',\n",
       " '1tl0': '2nzg',\n",
       " '1tln': '3tln',\n",
       " '1tm8': '1ymg',\n",
       " '1tna': '5tna',\n",
       " '1to7': '1u8e',\n",
       " '1tpi': '3tpi',\n",
       " '1tqk': '1vqx',\n",
       " '1trc': '1fw4',\n",
       " '1trt': '2trt',\n",
       " '1trx': '1xob',\n",
       " '1ts1': '2ts1',\n",
       " '1tsa': '2tsa',\n",
       " '1tsb': '2tsb',\n",
       " '1tsg': '1o7b',\n",
       " '1tss': '2tss',\n",
       " '1txv': '2vdl',\n",
       " '1txw': '1vrn',\n",
       " '1ty3': '2vdk',\n",
       " '1ty5': '2vdm',\n",
       " '1ty6': '2vdn',\n",
       " '1ty7': '2vc2',\n",
       " '1u0y': '6d1x',\n",
       " '1u4i': '2pjg',\n",
       " '1u7e': '3fhi',\n",
       " '1u8d': '4fe5',\n",
       " '1uah': '1ug6',\n",
       " '1uba': '1dv0',\n",
       " '1ucz': '2ucz',\n",
       " '1udp': '2udp',\n",
       " '1uf6': '1wub',\n",
       " '1uiq': '1v7c',\n",
       " '1uja': '1vcj',\n",
       " '1ukd': '1uke',\n",
       " '1um3': '1vf5',\n",
       " '1umm': '2c8s',\n",
       " '1un7': '2vhl',\n",
       " '1uox': '1r51',\n",
       " '1upo': '1vyn',\n",
       " '1usj': '2bi7',\n",
       " '1utf': '4v4f',\n",
       " '1utv': '4v4f',\n",
       " '1utw': '1uyp',\n",
       " '1uul': '4llr',\n",
       " '1uvd': '2buf',\n",
       " '1uvv': '2bty',\n",
       " '1uvw': '3zdw',\n",
       " '1uw2': '2vrd',\n",
       " '1uzt': '1wco',\n",
       " '1v52': '2djk',\n",
       " '1v56': None,\n",
       " '1v78': '2e1w',\n",
       " '1v7b': '2zoy',\n",
       " '1v8a': '3hpd',\n",
       " '1v99': '4lu7',\n",
       " '1v9b': '4lu8',\n",
       " '1vaa': '2vaa',\n",
       " '1vab': '2vab',\n",
       " '1vbv': '5ycq',\n",
       " '1vc7': '4prf',\n",
       " '1vdu': '1wy6',\n",
       " '1vfk': '3a6o',\n",
       " '1vgs': '2cv4',\n",
       " '1vil': '2vil',\n",
       " '1vir': '2vir',\n",
       " '1vjj': None,\n",
       " '1vjp': '3cin',\n",
       " '1vks': '1lnp',\n",
       " '1vkv': '1zwj',\n",
       " '1vld': '4v4c',\n",
       " '1vle': '4v4d',\n",
       " '1vlf': '4v4e',\n",
       " '1voq': '4v4g',\n",
       " '1vor': '4v4g',\n",
       " '1vos': '4v4g',\n",
       " '1vou': '4v4g',\n",
       " '1vov': '4v4g',\n",
       " '1vow': '4v4g',\n",
       " '1vox': '4v4g',\n",
       " '1voy': '4v4g',\n",
       " '1voz': '4v4g',\n",
       " '1vp0': '4v4g',\n",
       " '1vpg': '2pwn',\n",
       " '1vpj': '2isb',\n",
       " '1vs5': '4v4h',\n",
       " '1vs6': '4v4h',\n",
       " '1vs7': '4v4h',\n",
       " '1vs8': '4v4h',\n",
       " '1vs9': '1vsa',\n",
       " '1vsa': '4v4i',\n",
       " '1vsp': '4v4j',\n",
       " '1vsw': '4v7g',\n",
       " '1vsx': '4v7g',\n",
       " '1vsy': '4v7o',\n",
       " '1vsz': '4cwu',\n",
       " '1vt0': '4v4k',\n",
       " '1vt2': '4v80',\n",
       " '1vt4': '4v4l',\n",
       " '1vts': None,\n",
       " '1vtz': '4v4m',\n",
       " '1vu0': '4v86',\n",
       " '1vu1': '4v86',\n",
       " '1vu2': '4v98',\n",
       " '1vu3': '4v98',\n",
       " '1vu4': '3j3q',\n",
       " '1vu5': '3j3q',\n",
       " '1vu6': '3j3q',\n",
       " '1vu7': '3j3q',\n",
       " '1vu8': '3j3q',\n",
       " '1vu9': '3j3q',\n",
       " '1vua': '3j3q',\n",
       " '1vuc': '3j3q',\n",
       " '1vud': '3j3q',\n",
       " '1vue': '3j3q',\n",
       " '1vuf': '3j3q',\n",
       " '1vug': '3j3q',\n",
       " '1vuh': '3j3q',\n",
       " '1vui': '3j3q',\n",
       " '1vuj': '3j3q',\n",
       " '1vuk': '3j3q',\n",
       " '1vul': '3j3q',\n",
       " '1vum': '3j3q',\n",
       " '1vun': '3j3q',\n",
       " '1vuo': '3j3q',\n",
       " '1vup': '3j3q',\n",
       " '1vuq': '3j3q',\n",
       " '1vur': '3j3q',\n",
       " '1vus': '3j3q',\n",
       " '1vut': '3j3q',\n",
       " '1vuu': '3j3y',\n",
       " '1vuv': '3j3y',\n",
       " '1vuw': '3j3y',\n",
       " '1vux': '3j3y',\n",
       " '1vuy': '3j3y',\n",
       " '1vuz': '3j3y',\n",
       " '1vv0': '3j3y',\n",
       " '1vv1': '3j3y',\n",
       " '1vv2': '3j3y',\n",
       " '1vv3': '3j3y',\n",
       " '1vv4': '3j3y',\n",
       " '1vv5': '3j3y',\n",
       " '1vv6': '3j3y',\n",
       " '1vv7': '3j3y',\n",
       " '1vv8': '3j3y',\n",
       " '1vv9': '3j3y',\n",
       " '1vva': '3j3y',\n",
       " '1vvb': '3j3y',\n",
       " '1vvf': '3j3y',\n",
       " '1vvg': '3j3y',\n",
       " '1vvh': '3j3y',\n",
       " '1vvi': '3j3y',\n",
       " '1vvk': '4v4n',\n",
       " '1vvl': '4l47',\n",
       " '1vvm': '4l47',\n",
       " '1vvn': '4l47',\n",
       " '1vvo': '4l47',\n",
       " '1vvp': '4l71',\n",
       " '1vvq': '4l71',\n",
       " '1vvr': '4l71',\n",
       " '1vvs': '4l71',\n",
       " '1vvt': '4lel',\n",
       " '1vvu': '4lel',\n",
       " '1vvv': '4lel',\n",
       " '1vvw': '4lel',\n",
       " '1vvx': '4lfz',\n",
       " '1vvy': '4lfz',\n",
       " '1vvz': '4lfz',\n",
       " '1vw0': '4lfz',\n",
       " '1vw1': '4o9y',\n",
       " '1vw2': '4o9y',\n",
       " '1vw3': '3j6b',\n",
       " '1vw4': '3j6b',\n",
       " '1vw5': '4nwr',\n",
       " '1vw6': '4nwr',\n",
       " '1vw7': '3j6x',\n",
       " '1vw8': '3j6x',\n",
       " '1vw9': '3j6x',\n",
       " '1vws': '3j6y',\n",
       " '1vwu': '3j6y',\n",
       " '1vwv': '3j6y',\n",
       " '1vww': '4w1z',\n",
       " '1vwx': '4w20',\n",
       " '1vwy': '4w24',\n",
       " '1vwz': '4w25',\n",
       " '1vx0': '4w26',\n",
       " '1vx1': '4w27',\n",
       " '1vx2': '4w28',\n",
       " ...}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Mapping\n",
    "def _parse_release_dates(path: str) -> Mapping[str, datetime.datetime]:\n",
    "  \"\"\"Parses release dates file, returns a mapping from PDBs to release dates.\"\"\"\n",
    "  if path.endswith('txt'):\n",
    "    release_dates = {}\n",
    "    with open(path, 'r') as f:\n",
    "      for line in f:\n",
    "        pdb_id, date = line.split(':')\n",
    "        date = date.strip()\n",
    "        # Python 3.6 doesn't have datetime.date.fromisoformat() which is about\n",
    "        # 90x faster than strptime. However, splitting the string manually is\n",
    "        # about 10x faster than strptime.\n",
    "        release_dates[pdb_id.strip()] = datetime.datetime(\n",
    "            year=int(date[:4]), month=int(date[5:7]), day=int(date[8:10]))\n",
    "    return release_dates\n",
    "  else:\n",
    "    raise ValueError('Invalid format of the release date file %s.' % path)\n",
    "\n",
    "\n",
    "_release_dates = _parse_release_dates(release_dates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 DeepMind Technologies Limited\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Parses the mmCIF file format.\"\"\"\n",
    "import collections\n",
    "import dataclasses\n",
    "import functools\n",
    "import io\n",
    "from typing import Any, Mapping, Optional, Sequence, Tuple\n",
    "\n",
    "from absl import logging\n",
    "from Bio import PDB\n",
    "import Bio\n",
    "\n",
    "# Type aliases:\n",
    "ChainId = str\n",
    "PdbHeader = Mapping[str, Any]\n",
    "PdbStructure = PDB.Structure.Structure\n",
    "SeqRes = str\n",
    "MmCIFDict = Mapping[str, Sequence[str]]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Monomer:\n",
    "  id: str\n",
    "  num: int\n",
    "\n",
    "\n",
    "# Note - mmCIF format provides no guarantees on the type of author-assigned\n",
    "# sequence numbers. They need not be integers.\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class AtomSite:\n",
    "  residue_name: str\n",
    "  author_chain_id: str\n",
    "  mmcif_chain_id: str\n",
    "  author_seq_num: str\n",
    "  mmcif_seq_num: int\n",
    "  insertion_code: str\n",
    "  hetatm_atom: str\n",
    "  model_num: int\n",
    "\n",
    "\n",
    "# Used to map SEQRES index to a residue in the structure.\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ResiduePosition:\n",
    "  chain_id: str\n",
    "  residue_number: int\n",
    "  insertion_code: str\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ResidueAtPosition:\n",
    "  position: Optional[ResiduePosition]\n",
    "  name: str\n",
    "  is_missing: bool\n",
    "  hetflag: str\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class MmcifObject:\n",
    "  \"\"\"Representation of a parsed mmCIF file.\n",
    "\n",
    "  Contains:\n",
    "    file_id: A meaningful name, e.g. a pdb_id. Should be unique amongst all\n",
    "      files being processed.\n",
    "    header: Biopython header.\n",
    "    structure: Biopython structure.\n",
    "    chain_to_seqres: Dict mapping chain_id to 1 letter amino acid sequence. E.g.\n",
    "      {'A': 'ABCDEFG'}\n",
    "    seqres_to_structure: Dict; for each chain_id contains a mapping between\n",
    "      SEQRES index and a ResidueAtPosition. e.g. {'A': {0: ResidueAtPosition,\n",
    "                                                        1: ResidueAtPosition,\n",
    "                                                        ...}}\n",
    "    raw_string: The raw string used to construct the MmcifObject.\n",
    "  \"\"\"\n",
    "  file_id: str\n",
    "  header: PdbHeader\n",
    "  structure: PdbStructure\n",
    "  chain_to_seqres: Mapping[ChainId, SeqRes]\n",
    "  seqres_to_structure: Mapping[ChainId, Mapping[int, ResidueAtPosition]]\n",
    "  raw_string: Any\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ParsingResult:\n",
    "  \"\"\"Returned by the parse function.\n",
    "\n",
    "  Contains:\n",
    "    mmcif_object: A MmcifObject, may be None if no chain could be successfully\n",
    "      parsed.\n",
    "    errors: A dict mapping (file_id, chain_id) to any exception generated.\n",
    "  \"\"\"\n",
    "  mmcif_object: Optional[MmcifObject]\n",
    "  errors: Mapping[Tuple[str, str], Any]\n",
    "\n",
    "\n",
    "class ParseError(Exception):\n",
    "  \"\"\"An error indicating that an mmCIF file could not be parsed.\"\"\"\n",
    "\n",
    "\n",
    "def mmcif_loop_to_list(prefix: str,\n",
    "                       parsed_info: MmCIFDict) -> Sequence[Mapping[str, str]]:\n",
    "  \"\"\"Extracts loop associated with a prefix from mmCIF data as a list.\n",
    "\n",
    "  Reference for loop_ in mmCIF:\n",
    "    http://mmcif.wwpdb.org/docs/tutorials/mechanics/pdbx-mmcif-syntax.html\n",
    "\n",
    "  Args:\n",
    "    prefix: Prefix shared by each of the data items in the loop.\n",
    "      e.g. '_entity_poly_seq.', where the data items are _entity_poly_seq.num,\n",
    "      _entity_poly_seq.mon_id. Should include the trailing period.\n",
    "    parsed_info: A dict of parsed mmCIF data, e.g. _mmcif_dict from a Biopython\n",
    "      parser.\n",
    "\n",
    "  Returns:\n",
    "    Returns a list of dicts; each dict represents 1 entry from an mmCIF loop.\n",
    "  \"\"\"\n",
    "  cols = []\n",
    "  data = []\n",
    "  for key, value in parsed_info.items():\n",
    "    if key.startswith(prefix):\n",
    "      cols.append(key)\n",
    "      data.append(value)\n",
    "\n",
    "  assert all([len(xs) == len(data[0]) for xs in data]), (\n",
    "      'mmCIF error: Not all loops are the same length: %s' % cols)\n",
    "\n",
    "  return [dict(zip(cols, xs)) for xs in zip(*data)]\n",
    "\n",
    "\n",
    "def mmcif_loop_to_dict(prefix: str,\n",
    "                       index: str,\n",
    "                       parsed_info: MmCIFDict,\n",
    "                       ) -> Mapping[str, Mapping[str, str]]:\n",
    "  \"\"\"Extracts loop associated with a prefix from mmCIF data as a dictionary.\n",
    "\n",
    "  Args:\n",
    "    prefix: Prefix shared by each of the data items in the loop.\n",
    "      e.g. '_entity_poly_seq.', where the data items are _entity_poly_seq.num,\n",
    "      _entity_poly_seq.mon_id. Should include the trailing period.\n",
    "    index: Which item of loop data should serve as the key.\n",
    "    parsed_info: A dict of parsed mmCIF data, e.g. _mmcif_dict from a Biopython\n",
    "      parser.\n",
    "\n",
    "  Returns:\n",
    "    Returns a dict of dicts; each dict represents 1 entry from an mmCIF loop,\n",
    "    indexed by the index column.\n",
    "  \"\"\"\n",
    "  entries = mmcif_loop_to_list(prefix, parsed_info)\n",
    "  return {entry[index]: entry for entry in entries}\n",
    "\n",
    "\n",
    "@functools.lru_cache(16, typed=False)\n",
    "def parse(*,\n",
    "          file_id: str,\n",
    "          mmcif_string: str,\n",
    "          catch_all_errors: bool = True) -> ParsingResult:\n",
    "  \"\"\"Entry point, parses an mmcif_string.\n",
    "\n",
    "  Args:\n",
    "    file_id: A string identifier for this file. Should be unique within the\n",
    "      collection of files being processed.\n",
    "    mmcif_string: Contents of an mmCIF file.\n",
    "    catch_all_errors: If True, all exceptions are caught and error messages are\n",
    "      returned as part of the ParsingResult. If False exceptions will be allowed\n",
    "      to propagate.\n",
    "\n",
    "  Returns:\n",
    "    A ParsingResult.\n",
    "  \"\"\"\n",
    "  errors = {}\n",
    "  try:\n",
    "    parser = PDB.MMCIFParser(QUIET=True)\n",
    "    handle = io.StringIO(mmcif_string)\n",
    "    full_structure = parser.get_structure('', handle)\n",
    "    first_model_structure = _get_first_model(full_structure)\n",
    "    # Extract the _mmcif_dict from the parser, which contains useful fields not\n",
    "    # reflected in the Biopython structure.\n",
    "    parsed_info = parser._mmcif_dict  # pylint:disable=protected-access\n",
    "\n",
    "    # Ensure all values are lists, even if singletons.\n",
    "    for key, value in parsed_info.items():\n",
    "      if not isinstance(value, list):\n",
    "        parsed_info[key] = [value]\n",
    "\n",
    "    header = _get_header(parsed_info)\n",
    "\n",
    "    # Determine the protein chains, and their start numbers according to the\n",
    "    # internal mmCIF numbering scheme (likely but not guaranteed to be 1).\n",
    "    valid_chains = _get_protein_chains(parsed_info=parsed_info)\n",
    "    if not valid_chains:\n",
    "      return ParsingResult(\n",
    "          None, {(file_id, ''): 'No protein chains found in this file.'})\n",
    "    seq_start_num = {chain_id: min([monomer.num for monomer in seq])\n",
    "                     for chain_id, seq in valid_chains.items()}\n",
    "\n",
    "    # Loop over the atoms for which we have coordinates. Populate two mappings:\n",
    "    # -mmcif_to_author_chain_id (maps internal mmCIF chain ids to chain ids used\n",
    "    # the authors / Biopython).\n",
    "    # -seq_to_structure_mappings (maps idx into sequence to ResidueAtPosition).\n",
    "    mmcif_to_author_chain_id = {}\n",
    "    seq_to_structure_mappings = {}\n",
    "    for atom in _get_atom_site_list(parsed_info):\n",
    "      if atom.model_num != '1':\n",
    "        # We only process the first model at the moment.\n",
    "        continue\n",
    "\n",
    "      mmcif_to_author_chain_id[atom.mmcif_chain_id] = atom.author_chain_id\n",
    "\n",
    "      if atom.mmcif_chain_id in valid_chains:\n",
    "        hetflag = ' '\n",
    "        if atom.hetatm_atom == 'HETATM':\n",
    "          # Water atoms are assigned a special hetflag of W in Biopython. We\n",
    "          # need to do the same, so that this hetflag can be used to fetch\n",
    "          # a residue from the Biopython structure by id.\n",
    "          if atom.residue_name in ('HOH', 'WAT'):\n",
    "            hetflag = 'W'\n",
    "          else:\n",
    "            hetflag = 'H_' + atom.residue_name\n",
    "        insertion_code = atom.insertion_code\n",
    "        if not _is_set(atom.insertion_code):\n",
    "          insertion_code = ' '\n",
    "        position = ResiduePosition(chain_id=atom.author_chain_id,\n",
    "                                   residue_number=int(atom.author_seq_num),\n",
    "                                   insertion_code=insertion_code)\n",
    "        seq_idx = int(atom.mmcif_seq_num) - seq_start_num[atom.mmcif_chain_id]\n",
    "        current = seq_to_structure_mappings.get(atom.author_chain_id, {})\n",
    "        current[seq_idx] = ResidueAtPosition(position=position,\n",
    "                                             name=atom.residue_name,\n",
    "                                             is_missing=False,\n",
    "                                             hetflag=hetflag)\n",
    "        seq_to_structure_mappings[atom.author_chain_id] = current\n",
    "\n",
    "    # Add missing residue information to seq_to_structure_mappings.\n",
    "    for chain_id, seq_info in valid_chains.items():\n",
    "      author_chain = mmcif_to_author_chain_id[chain_id]\n",
    "      current_mapping = seq_to_structure_mappings[author_chain]\n",
    "      for idx, monomer in enumerate(seq_info):\n",
    "        if idx not in current_mapping:\n",
    "          current_mapping[idx] = ResidueAtPosition(position=None,\n",
    "                                                   name=monomer.id,\n",
    "                                                   is_missing=True,\n",
    "                                                   hetflag=' ')\n",
    "\n",
    "    author_chain_to_sequence = {}\n",
    "    for chain_id, seq_info in valid_chains.items():\n",
    "      author_chain = mmcif_to_author_chain_id[chain_id]\n",
    "      seq = []\n",
    "      for monomer in seq_info: \n",
    "        code = Bio.Data.IUPACData.protein_letters_3to1.get(monomer.id, 'X')\n",
    "        seq.append(code if len(code) == 1 else 'X')\n",
    "      seq = ''.join(seq)\n",
    "      author_chain_to_sequence[author_chain] = seq\n",
    "\n",
    "    mmcif_object = MmcifObject(\n",
    "        file_id=file_id,\n",
    "        header=header,\n",
    "        structure=first_model_structure,\n",
    "        chain_to_seqres=author_chain_to_sequence,\n",
    "        seqres_to_structure=seq_to_structure_mappings,\n",
    "        raw_string=parsed_info)\n",
    "\n",
    "    return ParsingResult(mmcif_object=mmcif_object, errors=errors)\n",
    "  except Exception as e:  # pylint:disable=broad-except\n",
    "    errors[(file_id, '')] = e\n",
    "    if not catch_all_errors:\n",
    "      raise\n",
    "    return ParsingResult(mmcif_object=None, errors=errors)\n",
    "\n",
    "\n",
    "def _get_first_model(structure: PdbStructure) -> PdbStructure:\n",
    "  \"\"\"Returns the first model in a Biopython structure.\"\"\"\n",
    "  return next(structure.get_models())\n",
    "\n",
    "_MIN_LENGTH_OF_CHAIN_TO_BE_COUNTED_AS_PEPTIDE = 21\n",
    "\n",
    "\n",
    "def get_release_date(parsed_info: MmCIFDict) -> str:\n",
    "  \"\"\"Returns the oldest revision date.\"\"\"\n",
    "  revision_dates = parsed_info['_pdbx_audit_revision_history.revision_date']\n",
    "  return min(revision_dates)\n",
    "\n",
    "\n",
    "def _get_header(parsed_info: MmCIFDict) -> PdbHeader:\n",
    "  \"\"\"Returns a basic header containing method, release date and resolution.\"\"\"\n",
    "  header = {}\n",
    "\n",
    "  experiments = mmcif_loop_to_list('_exptl.', parsed_info)\n",
    "  header['structure_method'] = ','.join([\n",
    "      experiment['_exptl.method'].lower() for experiment in experiments])\n",
    "\n",
    "  # Note: The release_date here corresponds to the oldest revision. We prefer to\n",
    "  # use this for dataset filtering over the deposition_date.\n",
    "  if '_pdbx_audit_revision_history.revision_date' in parsed_info:\n",
    "    header['release_date'] = get_release_date(parsed_info)\n",
    "  else:\n",
    "    logging.warning('Could not determine release_date: %s',\n",
    "                    parsed_info['_entry.id'])\n",
    "\n",
    "  header['resolution'] = 0.00\n",
    "  for res_key in ('_refine.ls_d_res_high', '_em_3d_reconstruction.resolution',\n",
    "                  '_reflns.d_resolution_high'):\n",
    "    if res_key in parsed_info:\n",
    "      try:\n",
    "        raw_resolution = parsed_info[res_key][0]\n",
    "        header['resolution'] = float(raw_resolution)\n",
    "      except ValueError:\n",
    "        logging.debug('Invalid resolution format: %s', parsed_info[res_key])\n",
    "\n",
    "  return header\n",
    "\n",
    "\n",
    "def _get_atom_site_list(parsed_info: MmCIFDict) -> Sequence[AtomSite]:\n",
    "  \"\"\"Returns list of atom sites; contains data not present in the structure.\"\"\"\n",
    "  return [AtomSite(*site) for site in zip(  # pylint:disable=g-complex-comprehension\n",
    "      parsed_info['_atom_site.label_comp_id'],\n",
    "      parsed_info['_atom_site.auth_asym_id'],\n",
    "      parsed_info['_atom_site.label_asym_id'],\n",
    "      parsed_info['_atom_site.auth_seq_id'],\n",
    "      parsed_info['_atom_site.label_seq_id'],\n",
    "      parsed_info['_atom_site.pdbx_PDB_ins_code'],\n",
    "      parsed_info['_atom_site.group_PDB'],\n",
    "      parsed_info['_atom_site.pdbx_PDB_model_num'],\n",
    "      )]\n",
    "\n",
    "\n",
    "def _get_protein_chains(\n",
    "    *, parsed_info: Mapping[str, Any]) -> Mapping[ChainId, Sequence[Monomer]]:\n",
    "  \"\"\"Extracts polymer information for protein chains only.\n",
    "\n",
    "  Args:\n",
    "    parsed_info: _mmcif_dict produced by the Biopython parser.\n",
    "\n",
    "  Returns:\n",
    "    A dict mapping mmcif chain id to a list of Monomers.\n",
    "  \"\"\"\n",
    "  # Get polymer information for each entity in the structure.\n",
    "  entity_poly_seqs = mmcif_loop_to_list('_entity_poly_seq.', parsed_info)\n",
    "\n",
    "  polymers = collections.defaultdict(list)\n",
    "  for entity_poly_seq in entity_poly_seqs:\n",
    "    polymers[entity_poly_seq['_entity_poly_seq.entity_id']].append(\n",
    "        Monomer(id=entity_poly_seq['_entity_poly_seq.mon_id'],\n",
    "                num=int(entity_poly_seq['_entity_poly_seq.num'])))\n",
    "\n",
    "  # Get chemical compositions. Will allow us to identify which of these polymers\n",
    "  # are proteins.\n",
    "  chem_comps = mmcif_loop_to_dict('_chem_comp.', '_chem_comp.id', parsed_info)\n",
    "\n",
    "  # Get chains information for each entity. Necessary so that we can return a\n",
    "  # dict keyed on chain id rather than entity.\n",
    "  struct_asyms = mmcif_loop_to_list('_struct_asym.', parsed_info)\n",
    "\n",
    "  entity_to_mmcif_chains = collections.defaultdict(list)\n",
    "  for struct_asym in struct_asyms:\n",
    "    chain_id = struct_asym['_struct_asym.id']\n",
    "    entity_id = struct_asym['_struct_asym.entity_id']\n",
    "    entity_to_mmcif_chains[entity_id].append(chain_id)\n",
    "\n",
    "  # Identify and return the valid protein chains.\n",
    "  valid_chains = {}\n",
    "  for entity_id, seq_info in polymers.items():\n",
    "    chain_ids = entity_to_mmcif_chains[entity_id]\n",
    "\n",
    "    # Reject polymers without any peptide-like components, such as DNA/RNA.\n",
    "    if any(['peptide' in chem_comps[monomer.id]['_chem_comp.type'].lower()\n",
    "            for monomer in seq_info]):\n",
    "      for chain_id in chain_ids:\n",
    "        valid_chains[chain_id] = seq_info\n",
    "  return valid_chains\n",
    "\n",
    "\n",
    "def _is_set(data: str) -> bool:\n",
    "  \"\"\"Returns False if data is a special mmCIF character indicating 'unset'.\"\"\"\n",
    "  return data not in ('.', '?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_pdb_code = \"4xov\"\n",
    "cif_string = open(\"/home/sirius/PhD/database/pdb_mmcif/mmcif_files/4xov.cif\").read()\n",
    "parsing_result = parse(file_id=hit_pdb_code, mmcif_string=cif_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'structure_method': 'x-ray diffraction',\n",
       " 'release_date': '2015-09-09',\n",
       " 'resolution': 1.2}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing_result.mmcif_object.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4xov'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing_result.mmcif_object.file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Model id=0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing_result.mmcif_object.structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2024 10. 15\n",
    "alphafold differences in three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model_1_ptm_feature = {'aatype': (4, 468), 'residue_index': (4, 468), 'seq_length': (4,), 'template_aatype': (4, 4, 468), 'template_all_atom_masks': (4, 4, 468, 37), 'template_all_atom_positions': (4, 4, 468, 37, 3), 'template_sum_probs': (4, 4, 1), 'is_distillation': (4,), 'seq_mask': (4, 468), 'msa_mask': (4, 508, 468), 'msa_row_mask': (4, 508), 'random_crop_to_size_seed': (4, 2), 'template_mask': (4, 4), 'template_pseudo_beta': (4, 4, 468, 3), 'template_pseudo_beta_mask': (4, 4, 468), 'atom14_atom_exists': (4, 468, 14), 'residx_atom14_to_atom37': (4, 468, 14), 'residx_atom37_to_atom14': (4, 468, 37), 'atom37_atom_exists': (4, 468, 37), 'extra_msa': (4, 5120, 468), 'extra_msa_mask': (4, 5120, 468), 'extra_msa_row_mask': (4, 5120), 'bert_mask': (4, 508, 468), 'true_msa': (4, 508, 468), 'extra_has_deletion': (4, 5120, 468), 'extra_deletion_value': (4, 5120, 468), 'msa_feat': (4, 508, 468, 49), 'target_feat': (4, 468, 22)}\n",
    "model_2_ptm_feature = {'aatype': (4, 468), 'residue_index': (4, 468), 'seq_length': (4,), 'template_aatype': (4, 4, 468), 'template_all_atom_masks': (4, 4, 468, 37), 'template_all_atom_positions': (4, 4, 468, 37, 3), 'template_sum_probs': (4, 4, 1), 'is_distillation': (4,), 'seq_mask': (4, 468), 'msa_mask': (4, 508, 468), 'msa_row_mask': (4, 508), 'random_crop_to_size_seed': (4, 2), 'template_mask': (4, 4), 'template_pseudo_beta': (4, 4, 468, 3), 'template_pseudo_beta_mask': (4, 4, 468), 'atom14_atom_exists': (4, 468, 14), 'residx_atom14_to_atom37': (4, 468, 14), 'residx_atom37_to_atom14': (4, 468, 37), 'atom37_atom_exists': (4, 468, 37), 'extra_msa': (4, 1024, 468), 'extra_msa_mask': (4, 1024, 468), 'extra_msa_row_mask': (4, 1024), 'bert_mask': (4, 508, 468), 'true_msa': (4, 508, 468), 'extra_has_deletion': (4, 1024, 468), 'extra_deletion_value': (4, 1024, 468), 'msa_feat': (4, 508, 468, 49), 'target_feat': (4, 468, 22)}\n",
    "model_3_ptm_feature = {'aatype': (4, 468), 'residue_index': (4, 468), 'seq_length': (4,), 'is_distillation': (4,), 'seq_mask': (4, 468), 'msa_mask': (4, 512, 468), 'msa_row_mask': (4, 512), 'random_crop_to_size_seed': (4, 2), 'atom14_atom_exists': (4, 468, 14), 'residx_atom14_to_atom37': (4, 468, 14), 'residx_atom37_to_atom14': (4, 468, 37), 'atom37_atom_exists': (4, 468, 37), 'extra_msa': (4, 5120, 468), 'extra_msa_mask': (4, 5120, 468), 'extra_msa_row_mask': (4, 5120), 'bert_mask': (4, 512, 468), 'true_msa': (4, 512, 468), 'extra_has_deletion': (4, 5120, 468), 'extra_deletion_value': (4, 5120, 468), 'msa_feat': (4, 512, 468, 49), 'target_feat': (4, 468, 22)}\n",
    "model_4_ptm_feature = {'aatype': (4, 468), 'residue_index': (4, 468), 'seq_length': (4,), 'is_distillation': (4,), 'seq_mask': (4, 468), 'msa_mask': (4, 512, 468), 'msa_row_mask': (4, 512), 'random_crop_to_size_seed': (4, 2), 'atom14_atom_exists': (4, 468, 14), 'residx_atom14_to_atom37': (4, 468, 14), 'residx_atom37_to_atom14': (4, 468, 37), 'atom37_atom_exists': (4, 468, 37), 'extra_msa': (4, 5120, 468), 'extra_msa_mask': (4, 5120, 468), 'extra_msa_row_mask': (4, 5120), 'bert_mask': (4, 512, 468), 'true_msa': (4, 512, 468), 'extra_has_deletion': (4, 5120, 468), 'extra_deletion_value': (4, 5120, 468), 'msa_feat': (4, 512, 468, 49), 'target_feat': (4, 468, 22)}\n",
    "model_5_ptm_feature = {'aatype': (4, 468), 'residue_index': (4, 468), 'seq_length': (4,), 'is_distillation': (4,), 'seq_mask': (4, 468), 'msa_mask': (4, 512, 468), 'msa_row_mask': (4, 512), 'random_crop_to_size_seed': (4, 2), 'atom14_atom_exists': (4, 468, 14), 'residx_atom14_to_atom37': (4, 468, 14), 'residx_atom37_to_atom14': (4, 468, 37), 'atom37_atom_exists': (4, 468, 37), 'extra_msa': (4, 1024, 468), 'extra_msa_mask': (4, 1024, 468), 'extra_msa_row_mask': (4, 1024), 'bert_mask': (4, 512, 468), 'true_msa': (4, 512, 468), 'extra_has_deletion': (4, 1024, 468), 'extra_deletion_value': (4, 1024, 468), 'msa_feat': (4, 512, 468, 49), 'target_feat': (4, 468, 22)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aatype (4, 468)\n",
      "residue_index (4, 468)\n",
      "seq_length (4,)\n",
      "is_distillation (4,)\n",
      "seq_mask (4, 468)\n",
      "msa_mask (4, 512, 468)\n",
      "msa_row_mask (4, 512)\n",
      "random_crop_to_size_seed (4, 2)\n",
      "atom14_atom_exists (4, 468, 14)\n",
      "residx_atom14_to_atom37 (4, 468, 14)\n",
      "residx_atom37_to_atom14 (4, 468, 37)\n",
      "atom37_atom_exists (4, 468, 37)\n",
      "extra_msa (4, 1024, 468)\n",
      "extra_msa_mask (4, 1024, 468)\n",
      "extra_msa_row_mask (4, 1024)\n",
      "bert_mask (4, 512, 468)\n",
      "true_msa (4, 512, 468)\n",
      "extra_has_deletion (4, 1024, 468)\n",
      "extra_deletion_value (4, 1024, 468)\n",
      "msa_feat (4, 512, 468, 49)\n",
      "target_feat (4, 468, 22)\n"
     ]
    }
   ],
   "source": [
    "for key in model_5_ptm_feature.keys():\n",
    "    print(key,model_5_ptm_feature[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aatype (4, 468)\n",
      "residue_index (4, 468)\n",
      "seq_length (4,)\n",
      "template_aatype (4, 4, 468)\n",
      "template_all_atom_masks (4, 4, 468, 37)\n",
      "template_all_atom_positions (4, 4, 468, 37, 3)\n",
      "template_sum_probs (4, 4, 1)\n",
      "is_distillation (4,)\n",
      "seq_mask (4, 468)\n",
      "msa_mask (4, 508, 468)\n",
      "msa_row_mask (4, 508)\n",
      "random_crop_to_size_seed (4, 2)\n",
      "template_mask (4, 4)\n",
      "template_pseudo_beta (4, 4, 468, 3)\n",
      "template_pseudo_beta_mask (4, 4, 468)\n",
      "atom14_atom_exists (4, 468, 14)\n",
      "residx_atom14_to_atom37 (4, 468, 14)\n",
      "residx_atom37_to_atom14 (4, 468, 37)\n",
      "atom37_atom_exists (4, 468, 37)\n",
      "extra_msa (4, 5120, 468)\n",
      "extra_msa_mask (4, 5120, 468)\n",
      "extra_msa_row_mask (4, 5120)\n",
      "bert_mask (4, 508, 468)\n",
      "true_msa (4, 508, 468)\n",
      "extra_has_deletion (4, 5120, 468)\n",
      "extra_deletion_value (4, 5120, 468)\n",
      "msa_feat (4, 508, 468, 49)\n",
      "target_feat (4, 468, 22)\n"
     ]
    }
   ],
   "source": [
    "for key in model_1_ptm_feature.keys():\n",
    "    print(key,model_1_ptm_feature[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.load(\"/home/sirius/PhD/software/alphafold/output_without_MSA/xcf_mc9bp81/features.pkl\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aatype', 'between_segment_residues', 'domain_name', 'residue_index', 'seq_length', 'sequence', 'deletion_matrix_int', 'msa', 'num_alignments', 'msa_species_identifiers', 'template_aatype', 'template_all_atom_masks', 'template_all_atom_positions', 'template_domain_names', 'template_sequence', 'template_sum_probs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aatype (468, 21)\n",
      "between_segment_residues (468,)\n",
      "domain_name (1,)\n",
      "residue_index (468,)\n",
      "seq_length (468,)\n",
      "sequence (1,)\n",
      "deletion_matrix_int (1, 468)\n",
      "msa (1, 468)\n",
      "num_alignments (468,)\n",
      "msa_species_identifiers (1,)\n",
      "template_aatype (0,)\n",
      "template_all_atom_masks (0,)\n",
      "template_all_atom_positions (0,)\n",
      "template_domain_names (0,)\n",
      "template_sequence (0,)\n",
      "template_sum_probs (0,)\n"
     ]
    }
   ],
   "source": [
    "for i in feature.keys():\n",
    "    print(i,feature[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
