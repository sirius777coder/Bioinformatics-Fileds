{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024.0509 \n",
    "研究按照概率进行采样，设计的方法有: dataset, sampler(返回一个indices,兼容dataloader层面的batch_size和drop_last), batch_sampler(自定义返回一组) 和collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphafold mimic residue index\n",
    "copy_num = 1\n",
    "input_seqs = ['AABCAA']\n",
    "input_sequence = input_seqs[0] * copy_num\n",
    "sequence_features = {'residue_index': np.array(range(len(input_sequence)))}\n",
    "\n",
    "idx_res = sequence_features['residue_index']\n",
    "L_prev = 0\n",
    "Ls = [len(input_seqs[0])]*copy_num\n",
    "# Ls: number of residues in each chain\n",
    "for L_i in Ls[:-1]:\n",
    "    idx_res[L_prev+L_i:] += 200\n",
    "    L_prev += L_i\n",
    "sequence_features['residue_index'] = idx_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(5, 10)\n",
    "        self.labels = torch.randint(0, 2, (5,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "dataset = toy_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.9,0.01,0.01,0.01,0.01]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(dataset), replacement=False)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler,drop_last=False)\n",
    "# 注意, weightedRandomSampler是一个sampler，每次返回一个indices,这样可以指定batch_size和drop_last；如果是自己写batch_sampler，则需要手动返回一个batch的indices,不支持在Dataloader层面输入batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "class WeightedRandomSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements from ``[0,..,len(weights)-1]`` with given probabilities (weights).\n",
    "\n",
    "    Args:\n",
    "        weights (sequence)   : a sequence of weights, not necessary summing up to one\n",
    "        num_samples (int): number of samples to draw\n",
    "        replacement (bool): if ``True``, samples are drawn with replacement.\n",
    "            If not, they are drawn without replacement, which means that when a\n",
    "            sample index is drawn for a row, it cannot be drawn again for that row.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "\n",
    "    Example:\n",
    "        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
    "        >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n",
    "        [4, 4, 1, 4, 5]\n",
    "        >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n",
    "        [0, 1, 4, 3, 2]\n",
    "    \"\"\"\n",
    "\n",
    "    weights: Tensor\n",
    "    num_samples: int\n",
    "    replacement: bool\n",
    "\n",
    "    def __init__(self, weights, num_samples: int,\n",
    "                 replacement: bool = True, generator=None) -> None:\n",
    "        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n",
    "                num_samples <= 0:\n",
    "            raise ValueError(f\"num_samples should be a positive integer value, but got num_samples={num_samples}\")\n",
    "        if not isinstance(replacement, bool):\n",
    "            raise ValueError(f\"replacement should be a boolean value, but got replacement={replacement}\")\n",
    "\n",
    "        weights_tensor = torch.as_tensor(weights, dtype=torch.double)\n",
    "        if len(weights_tensor.shape) != 1:\n",
    "            raise ValueError(\"weights should be a 1d sequence but given \"\n",
    "                             f\"weights have shape {tuple(weights_tensor.shape)}\")\n",
    "\n",
    "        self.weights = weights_tensor\n",
    "        self.num_samples = num_samples\n",
    "        self.replacement = replacement\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self) :\n",
    "        # weights 需要为全部的数据集长度, num_samples表示从数据集中按照weight的概率采样N个样本\n",
    "        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)\n",
    "        for indices in rand_tensor:\n",
    "            yield [indices]\n",
    "        # yield from iter(rand_tensor.tolist())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.9,0.01,0.01,0.01,0.01]\n",
    "sampler = WeightedRandomSampler(weights, len(dataset), replacement=False) # sampler定义好之后，在for 循环的时候已经定义好一个dataloader的顺序\n",
    "dataloader = DataLoader(dataset,batch_sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.9735,  0.0124, -1.3516,  0.7601, -0.6293,  0.5954, -0.2301, -0.2935,\n",
      "         -0.6909, -0.0957]]), tensor([0])]\n",
      "[tensor([[-0.6128, -1.4545, -0.0640,  1.0201, -0.9541, -0.0118,  0.3479,  0.7788,\n",
      "         -2.0531,  0.1543]]), tensor([0])]\n",
      "[tensor([[ 8.8880e-04,  8.8777e-01,  1.7839e+00, -1.1767e+00, -1.9333e-01,\n",
      "          2.0674e+00,  1.4307e-01,  7.7601e-01, -1.1393e+00, -6.4407e-01]]), tensor([0])]\n",
      "[tensor([[ 0.2018, -1.4598, -0.2240, -1.3986, -0.6045, -0.1464,  2.2327,  0.8451,\n",
      "         -0.6582, -0.3910]]), tensor([0])]\n",
      "[tensor([[ 0.2854, -0.3968, -0.0665,  1.6820, -0.3343, -1.2104, -0.3105,  1.1500,\n",
      "          0.0115, -0.1329]]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccedingSequenceLengthSampler(Sampler[int]): # sampler=sampler\n",
    "    def __init__(self, data: List[str]) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        sizes = torch.tensor([len(x) for x in self.data])\n",
    "        yield from torch.argsort(sizes).tolist()\n",
    "\n",
    "class AccedingSequenceLengthBatchSampler(Sampler[List[int]]): # batch_sampler=sampler\n",
    "    # 按照长度进行chunk_size的batch sample\n",
    "    def __init__(self, data: List[str], batch_size: int) -> None:\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        sizes = torch.tensor([len(x) for x in self.data])\n",
    "        for batch in torch.chunk(torch.argsort(sizes), len(self)):\n",
    "            yield batch.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mypy: allow-untyped-defs\n",
    "from typing import (\n",
    "    Generic,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Sized,\n",
    "    TypeVar,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"BatchSampler\",\n",
    "    \"RandomSampler\",\n",
    "    \"Sampler\",\n",
    "    \"SequentialSampler\",\n",
    "    \"SubsetRandomSampler\",\n",
    "    \"WeightedRandomSampler\",\n",
    "]\n",
    "\n",
    "\n",
    "_T_co = TypeVar(\"_T_co\", covariant=True)\n",
    "\n",
    "\n",
    "class Sampler(Generic[_T_co]):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "\n",
    "    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a\n",
    "    way to iterate over indices or lists of indices (batches) of dataset elements,\n",
    "    and may provide a :meth:`__len__` method that returns the length of the returned iterators.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): This argument is not used and will be removed in 2.2.0.\n",
    "            You may still have custom implementation that utilizes it.\n",
    "\n",
    "    Example:\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> class AccedingSequenceLengthSampler(Sampler[int]):\n",
    "        >>>     def __init__(self, data: List[str]) -> None:\n",
    "        >>>         self.data = data\n",
    "        >>>\n",
    "        >>>     def __len__(self) -> int:\n",
    "        >>>         return len(self.data)\n",
    "        >>>\n",
    "        >>>     def __iter__(self) -> Iterator[int]:\n",
    "        >>>         sizes = torch.tensor([len(x) for x in self.data])\n",
    "        >>>         yield from torch.argsort(sizes).tolist()\n",
    "        >>>\n",
    "        >>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):\n",
    "        >>>     def __init__(self, data: List[str], batch_size: int) -> None:\n",
    "        >>>         self.data = data\n",
    "        >>>         self.batch_size = batch_size\n",
    "        >>>\n",
    "        >>>     def __len__(self) -> int:\n",
    "        >>>         return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "        >>>\n",
    "        >>>     def __iter__(self) -> Iterator[List[int]]:\n",
    "        >>>         sizes = torch.tensor([len(x) for x in self.data])\n",
    "        >>>         for batch in torch.chunk(torch.argsort(sizes), len(self)):\n",
    "        >>>             yield batch.tolist()\n",
    "\n",
    "    .. note:: The :meth:`__len__` method isn't strictly required by\n",
    "              :class:`~torch.utils.data.DataLoader`, but is expected in any\n",
    "              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source: Optional[Sized] = None) -> None:\n",
    "        if data_source is not None:\n",
    "            import warnings\n",
    "\n",
    "            warnings.warn(\n",
    "                \"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
    "                \"You may still have custom implementation that utilizes it.\"\n",
    "            )\n",
    "\n",
    "    def __iter__(self) -> Iterator[_T_co]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "    #\n",
    "    # Many times we have an abstract class representing a collection/iterable of\n",
    "    # data, e.g., `torch.utils.data.Sampler`, with its subclasses optionally\n",
    "    # implementing a `__len__` method. In such cases, we must make sure to not\n",
    "    # provide a default implementation, because both straightforward default\n",
    "    # implementations have their issues:\n",
    "    #\n",
    "    #   + `return NotImplemented`:\n",
    "    #     Calling `len(subclass_instance)` raises:\n",
    "    #       TypeError: 'NotImplementedType' object cannot be interpreted as an integer\n",
    "    #\n",
    "    #   + `raise NotImplementedError`:\n",
    "    #     This prevents triggering some fallback behavior. E.g., the built-in\n",
    "    #     `list(X)` tries to call `len(X)` first, and executes a different code\n",
    "    #     path if the method is not found or `NotImplemented` is returned, while\n",
    "    #     raising a `NotImplementedError` will propagate and make the call fail\n",
    "    #     where it could have used `__iter__` to complete the call.\n",
    "    #\n",
    "    # Thus, the only two sensible things to do are\n",
    "    #\n",
    "    #   + **not** provide a default `__len__`.\n",
    "    #\n",
    "    #   + raise a `TypeError` instead, which is what Python uses when users call\n",
    "    #     a method that is not defined on an object.\n",
    "    #     (@ssnl verifies that this works on at least Python 3.7.)\n",
    "\n",
    "\n",
    "class SequentialSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements sequentially, always in the same order.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: Sized\n",
    "\n",
    "    def __init__(self, data_source: Sized) -> None:\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(range(len(self.data_source)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "# mypy: allow-untyped-defs\n",
    "from typing import (\n",
    "    Generic,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Sized,\n",
    "    TypeVar,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"BatchSampler\",\n",
    "    \"RandomSampler\",\n",
    "    \"Sampler\",\n",
    "    \"SequentialSampler\",\n",
    "    \"SubsetRandomSampler\",\n",
    "    \"WeightedRandomSampler\",\n",
    "]\n",
    "\n",
    "\n",
    "_T_co = TypeVar(\"_T_co\", covariant=True)\n",
    "\n",
    "\n",
    "class Sampler(Generic[_T_co]):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "\n",
    "    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a\n",
    "    way to iterate over indices or lists of indices (batches) of dataset elements,\n",
    "    and may provide a :meth:`__len__` method that returns the length of the returned iterators.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): This argument is not used and will be removed in 2.2.0.\n",
    "            You may still have custom implementation that utilizes it.\n",
    "\n",
    "    Example:\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> class AccedingSequenceLengthSampler(Sampler[int]):\n",
    "        >>>     def __init__(self, data: List[str]) -> None:\n",
    "        >>>         self.data = data\n",
    "        >>>\n",
    "        >>>     def __len__(self) -> int:\n",
    "        >>>         return len(self.data)\n",
    "        >>>\n",
    "        >>>     def __iter__(self) -> Iterator[int]:\n",
    "        >>>         sizes = torch.tensor([len(x) for x in self.data])\n",
    "        >>>         yield from torch.argsort(sizes).tolist()\n",
    "        >>>\n",
    "        >>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):\n",
    "        >>>     def __init__(self, data: List[str], batch_size: int) -> None:\n",
    "        >>>         self.data = data\n",
    "        >>>         self.batch_size = batch_size\n",
    "        >>>\n",
    "        >>>     def __len__(self) -> int:\n",
    "        >>>         return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "        >>>\n",
    "        >>>     def __iter__(self) -> Iterator[List[int]]:\n",
    "        >>>         sizes = torch.tensor([len(x) for x in self.data])\n",
    "        >>>         for batch in torch.chunk(torch.argsort(sizes), len(self)):\n",
    "        >>>             yield batch.tolist()\n",
    "\n",
    "    .. note:: The :meth:`__len__` method isn't strictly required by\n",
    "              :class:`~torch.utils.data.DataLoader`, but is expected in any\n",
    "              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source: Optional[Sized] = None) -> None:\n",
    "        if data_source is not None:\n",
    "            import warnings\n",
    "\n",
    "            warnings.warn(\n",
    "                \"`data_source` argument is not used and will be removed in 2.2.0.\"\n",
    "                \"You may still have custom implementation that utilizes it.\"\n",
    "            )\n",
    "\n",
    "    def __iter__(self) -> Iterator[_T_co]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "    #\n",
    "    # Many times we have an abstract class representing a collection/iterable of\n",
    "    # data, e.g., `torch.utils.data.Sampler`, with its subclasses optionally\n",
    "    # implementing a `__len__` method. In such cases, we must make sure to not\n",
    "    # provide a default implementation, because both straightforward default\n",
    "    # implementations have their issues:\n",
    "    #\n",
    "    #   + `return NotImplemented`:\n",
    "    #     Calling `len(subclass_instance)` raises:\n",
    "    #       TypeError: 'NotImplementedType' object cannot be interpreted as an integer\n",
    "    #\n",
    "    #   + `raise NotImplementedError`:\n",
    "    #     This prevents triggering some fallback behavior. E.g., the built-in\n",
    "    #     `list(X)` tries to call `len(X)` first, and executes a different code\n",
    "    #     path if the method is not found or `NotImplemented` is returned, while\n",
    "    #     raising a `NotImplementedError` will propagate and make the call fail\n",
    "    #     where it could have used `__iter__` to complete the call.\n",
    "    #\n",
    "    # Thus, the only two sensible things to do are\n",
    "    #\n",
    "    #   + **not** provide a default `__len__`.\n",
    "    #\n",
    "    #   + raise a `TypeError` instead, which is what Python uses when users call\n",
    "    #     a method that is not defined on an object.\n",
    "    #     (@ssnl verifies that this works on at least Python 3.7.)\n",
    "\n",
    "\n",
    "# If no shuffile\n",
    "class SequentialSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements sequentially, always in the same order.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: Sized\n",
    "\n",
    "    def __init__(self, data_source: Sized) -> None:\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(range(len(self.data_source)))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "\n",
    "\n",
    "# If shuffle\n",
    "class RandomSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
    "\n",
    "    If with replacement, then user can specify :attr:`num_samples` to draw.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n",
    "        num_samples (int): number of samples to draw, default=`len(dataset)`.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: Sized\n",
    "    replacement: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_source: Sized,\n",
    "        replacement: bool = False,\n",
    "        num_samples: Optional[int] = None,\n",
    "        generator=None,\n",
    "    ) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.replacement = replacement\n",
    "        self._num_samples = num_samples\n",
    "        self.generator = generator\n",
    "\n",
    "        if not isinstance(self.replacement, bool):\n",
    "            raise TypeError(\n",
    "                f\"replacement should be a boolean value, but got replacement={self.replacement}\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
    "            raise ValueError(\n",
    "                f\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        n = len(self.data_source)\n",
    "        if self.generator is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            generator = self.generator\n",
    "\n",
    "        if self.replacement:\n",
    "            for _ in range(self.num_samples // 32):\n",
    "                yield from torch.randint(\n",
    "                    high=n, size=(32,), dtype=torch.int64, generator=generator\n",
    "                ).tolist()\n",
    "            yield from torch.randint(\n",
    "                high=n,\n",
    "                size=(self.num_samples % 32,),\n",
    "                dtype=torch.int64,\n",
    "                generator=generator,\n",
    "            ).tolist()\n",
    "        else:\n",
    "            for _ in range(self.num_samples // n):\n",
    "                yield from torch.randperm(n, generator=generator).tolist()\n",
    "            yield from torch.randperm(n, generator=generator).tolist()[\n",
    "                : self.num_samples % n\n",
    "            ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.randn((100,1000,1000))\n",
    "sequential_sampler = SequentialSampler(dataset)\n",
    "random_sampler = RandomSampler(dataset,replacement=False,num_samples=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python native cache function, makes input datatype hashable to cache\n",
    "\n",
    "hashable (immutable) : number, string, tuple\n",
    "\n",
    "unhashable (mutable) : list, set ,dict, set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_squared_res = 1e6\n",
    "max_len = 455\n",
    "max_batch_examples = int(max_squared_res // max_len**2)\n",
    "print(max_batch_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((100,))\n",
    "pad_idx = 0\n",
    "max_len = 512\n",
    "\n",
    "seq_len = x.shape[pad_idx]\n",
    "pad_amt = max_len - seq_len\n",
    "pad_widths = [(0, 0)] * x.ndim\n",
    "pad_widths[pad_idx] = (0, pad_amt)\n",
    "np.pad(x, pad_widths).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batch_examples = int(max_squared_res // max_len**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing square of (2, 3)\n",
      "[4, 9]\n",
      "Computing square of (3, 4)\n",
      "[9, 16]\n",
      "[4, 9]\n",
      "Computing square of (4, 5)\n",
      "[16, 25]\n",
      "Computing square of (5, 6)\n",
      "[25, 36]\n",
      "[16, 25]\n"
     ]
    }
   ],
   "source": [
    "import functools as fn\n",
    "import numpy as np\n",
    "\n",
    "# Correctly defining the compute_square function\n",
    "@fn.lru_cache(maxsize=3)\n",
    "def compute_square(n):\n",
    "    \"\"\"\n",
    "    Computes the square of each element in the input sequence.\n",
    "\n",
    "    Args:\n",
    "        n (tuple): A tuple of integers.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the squares of the input integers.\n",
    "    \"\"\"\n",
    "    print(f\"Computing square of {n}\")\n",
    "    return np.square(n).tolist()\n",
    "\n",
    "# Function calls with tuples instead of lists\n",
    "print(compute_square((2, 3)))  # Computes and caches result\n",
    "print(compute_square((3, 4)))  # Computes and caches result\n",
    "print(compute_square((2, 3)))  # Returns cached result\n",
    "print(compute_square((4, 5)))  # Computes and caches result\n",
    "print(compute_square((5, 6)))  # Computes and caches result, evicts least recently used ((2, 3))\n",
    "print(compute_square((4, 5)))  # Returns cached result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_iter = iter(sequential_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "batch = [next(sampler_iter) for _ in range(10)]\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(Sampler[List[int]]):\n",
    "    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n",
    "\n",
    "    Args:\n",
    "        sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "\n",
    "    Example:\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampler: Union[Sampler[int], Iterable[int]],\n",
    "        batch_size: int,\n",
    "        drop_last: bool,\n",
    "    ) -> None:\n",
    "        # Since collections.abc.Iterable does not check for `__getitem__`, which\n",
    "        # is one way for an object to be an iterable, we don't do an `isinstance`\n",
    "        # check here.\n",
    "        if (\n",
    "            not isinstance(batch_size, int)\n",
    "            or isinstance(batch_size, bool)\n",
    "            or batch_size <= 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"batch_size should be a positive integer value, but got batch_size={batch_size}\"\n",
    "            )\n",
    "        if not isinstance(drop_last, bool):\n",
    "            raise ValueError(\n",
    "                f\"drop_last should be a boolean value, but got drop_last={drop_last}\"\n",
    "            )\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        # Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951\n",
    "        if self.drop_last:\n",
    "            sampler_iter = iter(self.sampler) # important! only generates iter once\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = [next(sampler_iter) for _ in range(self.batch_size)]\n",
    "                    yield batch\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        else:\n",
    "            batch = [0] * self.batch_size\n",
    "            idx_in_batch = 0\n",
    "            for idx in self.sampler: # for loop still generates iter once and then iterates <-> for idx in iter(self.sampler)\n",
    "                batch[idx_in_batch] = idx\n",
    "                idx_in_batch += 1\n",
    "                if idx_in_batch == self.batch_size:\n",
    "                    yield batch\n",
    "                    idx_in_batch = 0\n",
    "                    batch = [0] * self.batch_size\n",
    "            if idx_in_batch > 0:\n",
    "                yield batch[:idx_in_batch]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Can only be called if self.sampler has __len__ implemented\n",
    "        # We cannot enforce this condition, so we turn off typechecking for the\n",
    "        # implementation below.\n",
    "        # Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size  # type: ignore[arg-type]\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size  # type: ignore[arg-type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next() : A built-in function used to return the next item in an iterator. \n",
    "\n",
    "iter() : A built-in function used to convert an iterable to an iterator. \n",
    "\n",
    "yield() : A python keyword similar to the return keyword, except yield returns a generator object instead of a value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generator is Lazy Evaluation so produce items one at a time and don't need to load all the dadta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_naive_iterator:\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(10, 10)\n",
    "        self.labels = torch.randint(0, 2, (10,))\n",
    "    def __iter__(self):\n",
    "        return iter(range(len(self.data)))\n",
    "    def __next__(self):\n",
    "        return next(self.__iter__())\n",
    "\n",
    "class test_generator_iterator:\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(10, 10)\n",
    "        self.labels = torch.randint(0, 2, (10,))\n",
    "    def __iter__(self):\n",
    "        yield from range(len(self.data))\n",
    "    def __next__(self):\n",
    "        return next(self.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = test_naive_iterator()\n",
    "generator = test_generator_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in naive:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in generator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
