{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024.0509 研究按照概率进行采样，设计的方法有: dataset, sampler(返回一个indices,兼容dataloader层面的batch_size和drop_last), batch_sampler(自定义返回一组) 和collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphafold mimic residue index\n",
    "copy_num = 1\n",
    "input_seqs = ['AABCAA']\n",
    "input_sequence = input_seqs[0] * copy_num\n",
    "sequence_features = {'residue_index': np.array(range(len(input_sequence)))}\n",
    "\n",
    "idx_res = sequence_features['residue_index']\n",
    "L_prev = 0\n",
    "Ls = [len(input_seqs[0])]*copy_num\n",
    "# Ls: number of residues in each chain\n",
    "for L_i in Ls[:-1]:\n",
    "    idx_res[L_prev+L_i:] += 200\n",
    "    L_prev += L_i\n",
    "sequence_features['residue_index'] = idx_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load(\"/home/sirius/PhD/software/alphafold/output/tmp_142/TMP_142/features.pkl\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   32,\n",
       "         33,   34,   35,   36,   37,   38,   39,   40,   41,   42,   43,\n",
       "         44,   45,   46,   47,   48,   49,   50,   51,   52,   53,   54,\n",
       "         55,   56,   57,   58,   59,   60,   61,   62,   63,   64,   65,\n",
       "         66,   67,   68,   69,   70,   71,   72,   73,  274,  275,  276,\n",
       "        277,  278,  279,  280,  281,  282,  283,  284,  285,  286,  287,\n",
       "        288,  289,  290,  291,  292,  293,  294,  295,  296,  297,  298,\n",
       "        299,  300,  301,  302,  303,  304,  305,  306,  307,  308,  309,\n",
       "        310,  311,  312,  313,  314,  315,  316,  317,  318,  319,  320,\n",
       "        321,  322,  323,  324,  325,  326,  327,  328,  329,  330,  331,\n",
       "        332,  333,  334,  335,  336,  337,  338,  339,  340,  341,  342,\n",
       "        343,  344,  345,  346,  347,  548,  549,  550,  551,  552,  553,\n",
       "        554,  555,  556,  557,  558,  559,  560,  561,  562,  563,  564,\n",
       "        565,  566,  567,  568,  569,  570,  571,  572,  573,  574,  575,\n",
       "        576,  577,  578,  579,  580,  581,  582,  583,  584,  585,  586,\n",
       "        587,  588,  589,  590,  591,  592,  593,  594,  595,  596,  597,\n",
       "        598,  599,  600,  601,  602,  603,  604,  605,  606,  607,  608,\n",
       "        609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,\n",
       "        620,  621,  822,  823,  824,  825,  826,  827,  828,  829,  830,\n",
       "        831,  832,  833,  834,  835,  836,  837,  838,  839,  840,  841,\n",
       "        842,  843,  844,  845,  846,  847,  848,  849,  850,  851,  852,\n",
       "        853,  854,  855,  856,  857,  858,  859,  860,  861,  862,  863,\n",
       "        864,  865,  866,  867,  868,  869,  870,  871,  872,  873,  874,\n",
       "        875,  876,  877,  878,  879,  880,  881,  882,  883,  884,  885,\n",
       "        886,  887,  888,  889,  890,  891,  892,  893,  894,  895, 1096,\n",
       "       1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107,\n",
       "       1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118,\n",
       "       1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129,\n",
       "       1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140,\n",
       "       1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151,\n",
       "       1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162,\n",
       "       1163, 1164, 1165, 1166, 1167, 1168, 1169, 1370, 1371, 1372, 1373,\n",
       "       1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384,\n",
       "       1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395,\n",
       "       1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406,\n",
       "       1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417,\n",
       "       1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428,\n",
       "       1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439,\n",
       "       1440, 1441, 1442, 1443, 1644, 1645, 1646, 1647, 1648, 1649, 1650,\n",
       "       1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661,\n",
       "       1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672,\n",
       "       1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683,\n",
       "       1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694,\n",
       "       1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705,\n",
       "       1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716,\n",
       "       1717, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927,\n",
       "       1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938,\n",
       "       1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949,\n",
       "       1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960,\n",
       "       1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971,\n",
       "       1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982,\n",
       "       1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 2192, 2193,\n",
       "       2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204,\n",
       "       2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215,\n",
       "       2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226,\n",
       "       2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237,\n",
       "       2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248,\n",
       "       2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259,\n",
       "       2260, 2261, 2262, 2263, 2264, 2265], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['residue_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(5, 10)\n",
    "        self.labels = torch.randint(0, 2, (5,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "dataset = toy_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.9,0.01,0.01,0.01,0.01]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(dataset), replacement=False)\n",
    "dataloader = DataLoader(dataset, batch_size=2, sampler=sampler,drop_last=False)\n",
    "# 注意, weightedRandomSampler是一个sampler，每次返回一个indices,这样可以指定batch_size和drop_last；如果是自己写batch_sampler，则需要手动返回一个batch的indices,不支持在Dataloader层面输入batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "class WeightedRandomSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements from ``[0,..,len(weights)-1]`` with given probabilities (weights).\n",
    "\n",
    "    Args:\n",
    "        weights (sequence)   : a sequence of weights, not necessary summing up to one\n",
    "        num_samples (int): number of samples to draw\n",
    "        replacement (bool): if ``True``, samples are drawn with replacement.\n",
    "            If not, they are drawn without replacement, which means that when a\n",
    "            sample index is drawn for a row, it cannot be drawn again for that row.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "\n",
    "    Example:\n",
    "        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
    "        >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n",
    "        [4, 4, 1, 4, 5]\n",
    "        >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n",
    "        [0, 1, 4, 3, 2]\n",
    "    \"\"\"\n",
    "\n",
    "    weights: Tensor\n",
    "    num_samples: int\n",
    "    replacement: bool\n",
    "\n",
    "    def __init__(self, weights, num_samples: int,\n",
    "                 replacement: bool = True, generator=None) -> None:\n",
    "        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n",
    "                num_samples <= 0:\n",
    "            raise ValueError(f\"num_samples should be a positive integer value, but got num_samples={num_samples}\")\n",
    "        if not isinstance(replacement, bool):\n",
    "            raise ValueError(f\"replacement should be a boolean value, but got replacement={replacement}\")\n",
    "\n",
    "        weights_tensor = torch.as_tensor(weights, dtype=torch.double)\n",
    "        if len(weights_tensor.shape) != 1:\n",
    "            raise ValueError(\"weights should be a 1d sequence but given \"\n",
    "                             f\"weights have shape {tuple(weights_tensor.shape)}\")\n",
    "\n",
    "        self.weights = weights_tensor\n",
    "        self.num_samples = num_samples\n",
    "        self.replacement = replacement\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self) :\n",
    "        # weights 需要为全部的数据集长度, num_samples表示从数据集中按照weight的概率采样N个样本\n",
    "        rand_tensor = torch.multinomial(self.weights, self.num_samples, self.replacement, generator=self.generator)\n",
    "        for indices in rand_tensor:\n",
    "            yield [indices]\n",
    "        # yield from iter(rand_tensor.tolist())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.9,0.01,0.01,0.01,0.01]\n",
    "sampler = WeightedRandomSampler(weights, len(dataset), replacement=False) # sampler定义好之后，在for 循环的时候已经定义好一个dataloader的顺序\n",
    "dataloader = DataLoader(dataset,batch_sampler=sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.0441, -2.0333,  1.3254, -1.5246,  0.6038, -0.2783, -0.3543, -0.2539,\n",
      "         -0.5496, -0.4995]]), tensor([0])]\n",
      "[tensor([[ 0.3582, -0.6539,  2.1860,  0.1551,  1.2448, -1.4091, -0.7608,  0.0832,\n",
      "         -0.2801,  1.1191]]), tensor([0])]\n",
      "[tensor([[-0.3643, -0.4998,  0.5933,  0.4850,  1.2706,  0.2043, -0.5046, -0.1878,\n",
      "         -0.8916,  1.3904]]), tensor([0])]\n",
      "[tensor([[ 0.8679,  0.2033,  1.8755, -0.4040,  0.3463, -0.8196, -0.9209,  0.3268,\n",
      "         -1.7543, -1.4313]]), tensor([1])]\n",
      "[tensor([[ 1.3100, -0.6122,  0.6776, -0.1324, -0.5451,  0.5843, -0.0231, -0.7489,\n",
      "          0.4617, -0.6036]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccedingSequenceLengthSampler(Sampler[int]): # sampler=sampler\n",
    "    def __init__(self, data: List[str]) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        sizes = torch.tensor([len(x) for x in self.data])\n",
    "        yield from torch.argsort(sizes).tolist()\n",
    "\n",
    "class AccedingSequenceLengthBatchSampler(Sampler[List[int]]): # batch_sampler=sampler\n",
    "    # 按照长度进行chunk_size的batch sample\n",
    "    def __init__(self, data: List[str], batch_size: int) -> None:\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self) -> int:\n",
    "        return (len(self.data) + self.batch_size - 1) // self.batch_size\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        sizes = torch.tensor([len(x) for x in self.data])\n",
    "        for batch in torch.chunk(torch.argsort(sizes), len(self)):\n",
    "            yield batch.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
