{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, “hooks” are functions that automatically execute after a particular event\n",
    "\n",
    "Pytorch hook会自动给每一个torch.Tensor和torch.nn.Module注册多个hook\n",
    "在forward或者backward之前/之后自动执行的一些操作，用于检查和可视化\n",
    "\n",
    "\n",
    "1. Tensor hook会自动执行。The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  8., 12.])\n",
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "v = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
    "v.register_hook(lambda grad: print(grad))  # print the gradient\n",
    "v.register_hook(lambda grad : grad / 2)\n",
    "v.register_hook(lambda grad : print(grad))\n",
    "loss = torch.sum(v ** 2)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    "h = v.register_hook(lambda grad: grad ** 2)  # double the gradient\n",
    "v.backward(torch.tensor([1., 2., 3.])) # Jacobian vector product\n",
    "print(v.grad) \n",
    "\n",
    "h.remove()  # removes the hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 可以计算 Jacobian vector product   \n",
    "y = f(x)  \n",
    "y.backward(v) -> $v^TJ$\n",
    "\n",
    "The size of v should be the same as the size of the original tensor, with respect to which we want to compute the product ---- 由于v的形状与输入x的shape一样, 此时x的梯度已经为$v^TJ$\n",
    "\n",
    "\n",
    "\n",
    "backward()之后计算图会被销毁，但是每个张量的梯度会被保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. pytorch中为nn.Module注册了更多的hook，包括forward, backward, before forward, after forward等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1  register_module_forward_pre_hook(hook) / register_forward_pre_hook\n",
    "\n",
    "```\n",
    "hook(module, input) -> None or modified input\n",
    "```\n",
    "\n",
    "We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple\n",
    "\n",
    "2.2 register_module_forward_hook / register_forward_hook\n",
    "```\n",
    "hook(module, input, output) -> None or modified output\n",
    "```\n",
    "\n",
    "The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.\n",
    "\n",
    "\n",
    "2.3 register_module_full_backward_hook / register_full_backward_hook\n",
    "```\n",
    "hook(module, grad_input, grad_output) -> Tensor or None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "layer = nn.Linear(5, 3)\n",
    "x = torch.randn((6,5),requires_grad=True)\n",
    "layer.register_forward_pre_hook(lambda module, input: print(f'forward_pre_hook with {input[0].shape}'))\n",
    "layer.register_forward_hook(lambda module, input, output: print(f'forward_hook with {input[0].shape} and output {output.shape}'))\n",
    "layer.register_full_backward_hook(lambda module, grad_input, grad_output: print(f'backward_hook with grad_input ', grad_input, ' and grad_output ', grad_output))\n",
    "output = layer(x)\n",
    "output.sum().backward()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseExecution(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(\n",
    "                lambda layer, _, output: print(f\"{layer.__name__}: {output.shape}\")\n",
    "            )\n",
    "\n",
    "    def forward(self, x) :\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward = []\n",
    "class ForwardCheck(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        # Register a hook for each layer\n",
    "        for name, layer in self.model.named_children():\n",
    "            layer.__name__ = name\n",
    "            layer.register_forward_hook(self.forward_hook(name))\n",
    "\n",
    "    def forward_hook(self, name):\n",
    "        def hook(layer, input, output):\n",
    "            print(f\"{name},{output.mean().item()}\")\n",
    "            forward.append(output.mean().item())\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: torch.Size([10, 64, 112, 112])\n",
      "bn1: torch.Size([10, 64, 112, 112])\n",
      "relu: torch.Size([10, 64, 112, 112])\n",
      "maxpool: torch.Size([10, 64, 56, 56])\n",
      "layer1: torch.Size([10, 256, 56, 56])\n",
      "layer2: torch.Size([10, 512, 28, 28])\n",
      "layer3: torch.Size([10, 1024, 14, 14])\n",
      "layer4: torch.Size([10, 2048, 7, 7])\n",
      "avgpool: torch.Size([10, 2048, 1, 1])\n",
      "fc: torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "verbose_resnet = VerboseExecution(resnet50())\n",
    "dummy_input = torch.ones(10, 3, 224, 224)\n",
    "\n",
    "_ = verbose_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1,-0.059701159596443176\n",
      "bn1,3.8469323726531e-08\n",
      "relu,0.1515873670578003\n",
      "maxpool,0.2705785632133484\n",
      "layer1,0.6606165170669556\n",
      "layer2,0.9451380372047424\n",
      "layer3,1.3701601028442383\n",
      "layer4,0.9949647188186646\n",
      "avgpool,0.9949647784233093\n",
      "fc,-0.013356885872781277\n"
     ]
    }
   ],
   "source": [
    "forward_resnet = ForwardCheck(resnet50())\n",
    "\n",
    "_ = forward_resnet(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给tensor注册hook\n",
    "def GradientCheck(model: nn.Module) -> nn.Module:\n",
    "    for name, parameter in model.named_parameters():\n",
    "        parameter.register_hook(lambda grad, name=name: print(f\"{name},{grad.mean().item()}\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc.bias,-7.450580430390374e-11\n",
      "fc.weight,2.0116568133499158e-10\n",
      "layer4.2.bn3.weight,-4.083880776306614e-05\n",
      "layer4.2.bn3.bias,-0.00010678460239432752\n",
      "layer4.2.conv3.weight,-3.0424041597143514e-06\n",
      "layer4.2.bn2.weight,-1.161606633104384e-08\n",
      "layer4.2.bn2.bias,-1.0719409146986436e-06\n",
      "layer4.2.conv2.weight,-1.3477792890626006e-05\n",
      "layer4.2.bn1.weight,-2.9103830456733704e-11\n",
      "layer4.2.bn1.bias,-2.8743081202264875e-05\n",
      "layer4.2.conv1.weight,1.487585177528672e-05\n",
      "layer4.1.bn3.weight,-7.001651829341426e-05\n",
      "layer4.1.bn3.bias,-5.002642501494847e-05\n",
      "layer4.1.conv3.weight,6.656162895524176e-06\n",
      "layer4.1.bn2.weight,-2.3654138203710318e-08\n",
      "layer4.1.bn2.bias,3.75801682821475e-05\n",
      "layer4.1.conv2.weight,-8.846724085742608e-06\n",
      "layer4.1.bn1.weight,8.585629984736443e-10\n",
      "layer4.1.bn1.bias,0.00019211262406315655\n",
      "layer4.1.conv1.weight,-1.7508104065200314e-05\n",
      "layer4.0.downsample.1.weight,-4.5250169932842255e-05\n",
      "layer4.0.downsample.1.bias,-2.2337619157042354e-05\n",
      "layer4.0.downsample.0.weight,-2.9237884518806823e-05\n",
      "layer4.0.bn3.weight,2.020531246671453e-05\n",
      "layer4.0.bn3.bias,-2.2337619157042354e-05\n",
      "layer4.0.conv3.weight,-2.5600304070394486e-05\n",
      "layer4.0.bn2.weight,1.6196281649172306e-08\n",
      "layer4.0.bn2.bias,-3.153137367917225e-05\n",
      "layer4.0.conv2.weight,5.94900802752818e-06\n",
      "layer4.0.bn1.weight,-1.6079866327345371e-09\n",
      "layer4.0.bn1.bias,-0.00016471209528390318\n",
      "layer4.0.conv1.weight,3.183948138030246e-05\n",
      "layer3.5.bn3.weight,1.8027085388894193e-05\n",
      "layer3.5.bn3.bias,-2.1272262529237196e-05\n",
      "layer3.5.conv3.weight,-1.401085319230333e-05\n",
      "layer3.5.bn2.weight,9.080395102500916e-09\n",
      "layer3.5.bn2.bias,0.00026726993382908404\n",
      "layer3.5.conv2.weight,6.082435447751777e-06\n",
      "layer3.5.bn1.weight,-1.5279510989785194e-09\n",
      "layer3.5.bn1.bias,-0.0001536720956210047\n",
      "layer3.5.conv1.weight,5.644318298436701e-05\n",
      "layer3.4.bn3.weight,9.950260573532432e-05\n",
      "layer3.4.bn3.bias,-3.7883601180510595e-06\n",
      "layer3.4.conv3.weight,-2.197841604356654e-05\n",
      "layer3.4.bn2.weight,1.7491402104496956e-08\n",
      "layer3.4.bn2.bias,-0.0002092382055707276\n",
      "layer3.4.conv2.weight,0.00019769772188737988\n",
      "layer3.4.bn1.weight,-1.8044374883174896e-09\n",
      "layer3.4.bn1.bias,-0.0003117749292869121\n",
      "layer3.4.conv1.weight,1.507480556028895e-05\n",
      "layer3.3.bn3.weight,-2.5069719413295388e-05\n",
      "layer3.3.bn3.bias,2.080473677779082e-06\n",
      "layer3.3.conv3.weight,-5.3464224038179964e-05\n",
      "layer3.3.bn2.weight,-1.076841726899147e-08\n",
      "layer3.3.bn2.bias,0.0007212873315438628\n",
      "layer3.3.conv2.weight,-8.556281682103872e-05\n",
      "layer3.3.bn1.weight,2.371962182223797e-09\n",
      "layer3.3.bn1.bias,0.000489917816594243\n",
      "layer3.3.conv1.weight,-6.0704925999743864e-05\n",
      "layer3.2.bn3.weight,-0.00012410322960931808\n",
      "layer3.2.bn3.bias,-8.813171734800562e-05\n",
      "layer3.2.conv3.weight,-8.795219036983326e-05\n",
      "layer3.2.bn2.weight,-3.264722181484103e-08\n",
      "layer3.2.bn2.bias,0.0004576425999403\n",
      "layer3.2.conv2.weight,4.3740783439716324e-05\n",
      "layer3.2.bn1.weight,4.874891601502895e-09\n",
      "layer3.2.bn1.bias,-1.1171738151460886e-05\n",
      "layer3.2.conv1.weight,-3.045306402782444e-05\n",
      "layer3.1.bn3.weight,-0.0001267048646695912\n",
      "layer3.1.bn3.bias,-4.6638706407975405e-06\n",
      "layer3.1.conv3.weight,0.0002701041812542826\n",
      "layer3.1.bn2.weight,-4.316098056733608e-08\n",
      "layer3.1.bn2.bias,-0.0002556330291554332\n",
      "layer3.1.conv2.weight,4.4361728214425966e-05\n",
      "layer3.1.bn1.weight,-2.9103830456733704e-10\n",
      "layer3.1.bn1.bias,-0.0010532353771850467\n",
      "layer3.1.conv1.weight,0.00010579542868072167\n",
      "layer3.0.downsample.1.weight,-0.0003595854213926941\n",
      "layer3.0.downsample.1.bias,-0.0002926517918240279\n",
      "layer3.0.downsample.0.weight,-0.00016110300202853978\n",
      "layer3.0.bn3.weight,0.0005179331637918949\n",
      "layer3.0.bn3.bias,-0.0002926517918240279\n",
      "layer3.0.conv3.weight,-0.00023058181977830827\n",
      "layer3.0.bn2.weight,2.3058964870870113e-07\n",
      "layer3.0.bn2.bias,-0.0003713701735250652\n",
      "layer3.0.conv2.weight,0.0005812422605231404\n",
      "layer3.0.bn1.weight,1.8044374883174896e-09\n",
      "layer3.0.bn1.bias,-0.00430890079587698\n",
      "layer3.0.conv1.weight,0.0003900311712641269\n",
      "layer2.3.bn3.weight,-0.0014135215897113085\n",
      "layer2.3.bn3.bias,0.0007539736689068377\n",
      "layer2.3.conv3.weight,-0.000812424928881228\n",
      "layer2.3.bn2.weight,-3.9819860830903053e-07\n",
      "layer2.3.bn2.bias,0.0026195712853223085\n",
      "layer2.3.conv2.weight,-0.0006496385904029012\n",
      "layer2.3.bn1.weight,1.4435499906539917e-08\n",
      "layer2.3.bn1.bias,0.0013352588284760714\n",
      "layer2.3.conv1.weight,0.00029046981944702566\n",
      "layer2.2.bn3.weight,0.000967374537140131\n",
      "layer2.2.bn3.bias,0.0005607398343272507\n",
      "layer2.2.conv3.weight,-0.00074209418380633\n",
      "layer2.2.bn2.weight,1.435109879821539e-07\n",
      "layer2.2.bn2.bias,-0.0001909112324938178\n",
      "layer2.2.conv2.weight,-0.0011967996833845973\n",
      "layer2.2.bn1.weight,-1.3737007975578308e-08\n",
      "layer2.2.bn1.bias,0.00461721932515502\n",
      "layer2.2.conv1.weight,3.0794646590948105e-05\n",
      "layer2.1.bn3.weight,-0.0005753461737185717\n",
      "layer2.1.bn3.bias,0.0004252395883668214\n",
      "layer2.1.conv3.weight,5.6775519624352455e-05\n",
      "layer2.1.bn2.weight,-2.889428287744522e-07\n",
      "layer2.1.bn2.bias,-0.0036764515098184347\n",
      "layer2.1.conv2.weight,-0.0006622318178415298\n",
      "layer2.1.bn1.weight,1.909211277961731e-08\n",
      "layer2.1.bn1.bias,0.00646965391933918\n",
      "layer2.1.conv1.weight,0.00017984176520258188\n",
      "layer2.0.downsample.1.weight,-0.00034811467048712075\n",
      "layer2.0.downsample.1.bias,0.0002173797693103552\n",
      "layer2.0.downsample.0.weight,0.0018980109598487616\n",
      "layer2.0.bn3.weight,0.0013696022797375917\n",
      "layer2.0.bn3.bias,0.0002173797693103552\n",
      "layer2.0.conv3.weight,-0.00022773060481995344\n",
      "layer2.0.bn2.weight,-2.805609256029129e-08\n",
      "layer2.0.bn2.bias,0.00553817767649889\n",
      "layer2.0.conv2.weight,0.0037190408911556005\n",
      "layer2.0.bn1.weight,-1.6810372471809387e-07\n",
      "layer2.0.bn1.bias,0.003633964341133833\n",
      "layer2.0.conv1.weight,0.0031146002002060413\n",
      "layer1.2.bn3.weight,0.012409192509949207\n",
      "layer1.2.bn3.bias,0.0022366135381162167\n",
      "layer1.2.conv3.weight,-0.002233982551842928\n",
      "layer1.2.bn2.weight,2.6756897568702698e-06\n",
      "layer1.2.bn2.bias,-0.003804124891757965\n",
      "layer1.2.conv2.weight,-0.006442056968808174\n",
      "layer1.2.bn1.weight,-6.146728992462158e-08\n",
      "layer1.2.bn1.bias,0.010905683040618896\n",
      "layer1.2.conv1.weight,0.0029136412777006626\n",
      "layer1.1.bn3.weight,-0.0031587909907102585\n",
      "layer1.1.bn3.bias,-0.003684722352772951\n",
      "layer1.1.conv3.weight,-0.0020567486062645912\n",
      "layer1.1.bn2.weight,7.264316082000732e-08\n",
      "layer1.1.bn2.bias,0.0047957696951925755\n",
      "layer1.1.conv2.weight,0.0051598758436739445\n",
      "layer1.1.bn1.weight,-5.364418029785156e-07\n",
      "layer1.1.bn1.bias,-0.03375707566738129\n",
      "layer1.1.conv1.weight,0.005746529437601566\n",
      "layer1.0.downsample.1.weight,-0.02436944842338562\n",
      "layer1.0.downsample.1.bias,-0.013447504490613937\n",
      "layer1.0.downsample.0.weight,0.02483627013862133\n",
      "layer1.0.bn3.weight,0.015119152143597603\n",
      "layer1.0.bn3.bias,-0.013447504490613937\n",
      "layer1.0.conv3.weight,0.002065184060484171\n",
      "layer1.0.bn2.weight,6.146728992462158e-07\n",
      "layer1.0.bn2.bias,0.015142052434384823\n",
      "layer1.0.conv2.weight,0.003095954190939665\n",
      "layer1.0.bn1.weight,-3.9301812648773193e-07\n",
      "layer1.0.bn1.bias,0.008210750296711922\n",
      "layer1.0.conv1.weight,0.03793521597981453\n",
      "bn1.weight,-5.271285772323608e-06\n",
      "bn1.bias,-0.001657383982092142\n",
      "conv1.weight,0.0445576012134552\n"
     ]
    }
   ],
   "source": [
    "gradient_resnet = GradientCheck(resnet50())\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "predict = gradient_resnet(dummy_input)\n",
    "loss(predict, torch.zeros(10, dtype=torch.long)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
