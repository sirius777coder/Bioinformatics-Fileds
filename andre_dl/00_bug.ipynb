{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2024 11 26 in-place operation bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mimic nemo feature and tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn((5,100,100))\n",
    "feature = torch.randn((5,100,100))\n",
    "data.requires_grad = True\n",
    "feature.requires_grad = True\n",
    "\n",
    "mlp1 = torch.randn((100,100))\n",
    "mlp2 = torch.randn((100,100))\n",
    "\n",
    "data.requires_grad = True\n",
    "feature.requires_grad = True\n",
    "mlp1.requires_grad = True\n",
    "mlp2.requires_grad = True\n",
    "\n",
    "x1 = data @ mlp1\n",
    "x1.retain_grad()\n",
    "\n",
    "x2 = feature @ mlp2\n",
    "x2.retain_grad()\n",
    "\n",
    "x1 += x2\n",
    "\n",
    "loss = x1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all gradients to zero\n",
    "data.grad.zero_()\n",
    "feature.grad.zero_()\n",
    "mlp1.grad.zero_()\n",
    "mlp2.grad.zero_()\n",
    "# delete the computation graph\n",
    "x1 = x1.detach()\n",
    "x2 = x2.detach()\n",
    "loss = loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.requires_grad = True\n",
    "x2.requires_grad = True\n",
    "mlp1.requires_grad = True\n",
    "mlp2.requires_grad = True\n",
    "\n",
    "x1.retain_grad()\n",
    "x2.retain_grad()\n",
    "mlp1.retain_grad()\n",
    "mlp2.retain_grad()\n",
    "\n",
    "x1 = data @ mlp1\n",
    "x1.retain_grad()\n",
    "\n",
    "x2 = feature @ mlp2\n",
    "x2.retain_grad()\n",
    "\n",
    "x1 = x1 + x2\n",
    "loss = x1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40792/733781882.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380481/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  x1.grad, x2.grad, mlp1.grad, mlp2.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " tensor([[ -2.6065,  -2.6065,  -2.6065,  ...,  -2.6065,  -2.6065,  -2.6065],\n",
       "         [ -1.6994,  -1.6994,  -1.6994,  ...,  -1.6994,  -1.6994,  -1.6994],\n",
       "         [  1.6826,   1.6826,   1.6826,  ...,   1.6826,   1.6826,   1.6826],\n",
       "         ...,\n",
       "         [ 29.1005,  29.1005,  29.1005,  ...,  29.1005,  29.1005,  29.1005],\n",
       "         [-13.4131, -13.4131, -13.4131,  ..., -13.4131, -13.4131, -13.4131],\n",
       "         [-20.6864, -20.6864, -20.6864,  ..., -20.6864, -20.6864, -20.6864]]),\n",
       " tensor([[25.7726, 25.7726, 25.7726,  ..., 25.7726, 25.7726, 25.7726],\n",
       "         [-0.4677, -0.4677, -0.4677,  ..., -0.4677, -0.4677, -0.4677],\n",
       "         [10.8872, 10.8872, 10.8872,  ..., 10.8872, 10.8872, 10.8872],\n",
       "         ...,\n",
       "         [ 1.8547,  1.8547,  1.8547,  ...,  1.8547,  1.8547,  1.8547],\n",
       "         [17.8869, 17.8869, 17.8869,  ..., 17.8869, 17.8869, 17.8869],\n",
       "         [32.5849, 32.5849, 32.5849,  ..., 32.5849, 32.5849, 32.5849]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad, x2.grad, mlp1.grad, mlp2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
